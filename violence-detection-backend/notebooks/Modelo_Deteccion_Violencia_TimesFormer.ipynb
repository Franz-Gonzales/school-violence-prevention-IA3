{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KlMSrGEwj-Wh",
        "outputId": "8eb1c3d1-7bb1-4606-ea58-8f855fb5c3c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Collecting decord\n",
            "  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord) (2.0.2)\n",
            "Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m118.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: decord\n",
            "Successfully installed decord-0.6.0\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n",
            "Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n",
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n",
            "Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n",
            "Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n",
            "Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m50.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m46.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.7.1\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Collecting av\n",
            "  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m67.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: av\n",
            "Successfully installed av-14.4.0\n",
            "Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n",
            "Collecting evaluate\n",
            "  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
            "Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: evaluate\n",
            "Successfully installed evaluate-0.4.3\n"
          ]
        }
      ],
      "source": [
        "# Instalación de dependencias\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install decord\n",
        "!pip install scikit-learn\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install pandas\n",
        "!pip install tqdm\n",
        "!pip install scipy\n",
        "!pip install torchmetrics\n",
        "!pip install timm\n",
        "!pip install av\n",
        "!pip install einops\n",
        "!pip install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Afo36OBfkCKI"
      },
      "outputs": [],
      "source": [
        "# Importar bibliotecas necesarias\n",
        "import os\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from datetime import datetime\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR\n",
        "from torchvision import transforms\n",
        "from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score, BinarySpecificity\n",
        "from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "from transformers import TimesformerForVideoClassification, TimesformerConfig, AutoImageProcessor\n",
        "from transformers import get_cosine_schedule_with_warmup\n",
        "import decord\n",
        "from decord import VideoReader, cpu\n",
        "import av\n",
        "import gc\n",
        "import warnings\n",
        "import random\n",
        "import io\n",
        "import zipfile\n",
        "import logging\n",
        "import json\n",
        "from pathlib import Path\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3iG0bNlkIfx",
        "outputId": "38da38fa-ca25-4e9e-bcd6-c7cb0a207aed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Montar Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MnaOy73kHBy",
        "outputId": "b6f84090-74a5-429f-fdcc-1e866fbc9da4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando dispositivo: cuda\n"
          ]
        }
      ],
      "source": [
        "# Configurar advertencias\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "# Verificar disponibilidad de GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Z7ad0SlnuLJ"
      },
      "outputs": [],
      "source": [
        "# ============================== CONFIGURACIÓN DE HIPERPARÁMETROS ==============================\n",
        "\n",
        "# Hiperparámetros generales\n",
        "CONFIG = {\n",
        "    # Rutas y nombres\n",
        "    \"dataset_path\": \"/content/drive/MyDrive/dataset_violencia\",  # Ajustar según la ubicación real\n",
        "    \"output_dir\": \"/content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer\",\n",
        "    \"model_name\": \"timesformer_violence_detector\",\n",
        "\n",
        "    # Parámetros del modelo\n",
        "    \"pretrained_model\": \"facebook/timesformer-base-finetuned-k400\",\n",
        "    \"num_frames\": 8,              # Número de frames a procesar\n",
        "    \"image_size\": 224,             # Tamaño de los frames (224x224)\n",
        "    \"num_classes\": 2,              # Violencia / No violencia\n",
        "\n",
        "    # Parámetros de entrenamiento - Transfer Learning\n",
        "    \"tl_batch_size\": 8,            # Tamaño del batch\n",
        "    \"tl_num_epochs\": 10,           # Número de épocas\n",
        "    \"tl_learning_rate\": 5e-5,      # Learning rate inicial\n",
        "    \"tl_weight_decay\": 1e-4,       # Regularización L2\n",
        "    \"tl_dropout\": 0.2,             # Tasa de dropout\n",
        "    \"tl_warmup_ratio\": 0.1,        # Proporción de steps para warmup\n",
        "\n",
        "    # Parámetros de entrenamiento - Fine-Tuning\n",
        "    \"ft_batch_size\": 8,            # Tamaño del batch (más pequeño para fine-tuning)\n",
        "    \"ft_num_epochs\": 5,            # Número de épocas adicionales\n",
        "    \"ft_learning_rate\": 1e-5,      # Learning rate más bajo para fine-tuning\n",
        "    \"ft_weight_decay\": 5e-5,       # Regularización L2 suave\n",
        "\n",
        "    # Umbral de clasificación\n",
        "    \"threshold\": 0.70,              # Umbral de decisión para la clasificación\n",
        "\n",
        "    # Configuración de checkpoints\n",
        "    \"save_steps\": 200,             # Guardar cada X pasos\n",
        "    \"save_total_limit\": 3,         # Máximo número de checkpoints a mantener\n",
        "    \"save_best_only\": True,        # Guardar solo el mejor modelo\n",
        "\n",
        "    # Métricas y evaluación\n",
        "    \"eval_steps\": 100,              # Evaluar cada X pasos\n",
        "    \"logging_steps\": 50,           # Mostrar métricas cada X pasos\n",
        "\n",
        "    # Otros parámetros\n",
        "    \"seed\": 42,                    # Semilla para reproducibilidad\n",
        "    \"mixed_precision\": True,       # Usar precisión mixta para acelerar entrenamiento\n",
        "}\n",
        "\n",
        "# Crear directorio de salida si no existe\n",
        "os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n",
        "\n",
        "# Guardar configuración\n",
        "with open(os.path.join(CONFIG[\"output_dir\"], \"config.json\"), 'w') as f:\n",
        "    json.dump(CONFIG, f, indent=4)\n",
        "\n",
        "# Configurar reproducibilidad\n",
        "def set_seed(seed=42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(CONFIG[\"seed\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A_RQ45EhnyXZ"
      },
      "outputs": [],
      "source": [
        "# ============================== CLASES PARA EL DATASET Y PROCESAMIENTO ==============================\n",
        "\n",
        "# Clase para procesar y cargar los videos\n",
        "class ViolenceVideoDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', transform=None, num_frames=16, image_size=224, max_videos=None):\n",
        "        \"\"\"\n",
        "        Dataset para clasificación de violencia en videos\n",
        "\n",
        "        Args:\n",
        "            root_dir: Directorio raíz del dataset\n",
        "            split: 'train', 'val' o 'test'\n",
        "            transform: Transformaciones a aplicar\n",
        "            num_frames: Número de frames a extraer de cada video\n",
        "            image_size: Tamaño de los frames\n",
        "            max_videos: Limitar número de videos (para pruebas rápidas)\n",
        "        \"\"\"\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.transform = transform\n",
        "        self.num_frames = num_frames\n",
        "        self.image_size = image_size\n",
        "\n",
        "        self.processor = AutoImageProcessor.from_pretrained(CONFIG[\"pretrained_model\"])\n",
        "\n",
        "        # Obtener las rutas de videos y etiquetas\n",
        "        violence_dir = os.path.join(root_dir, split, 'violence')\n",
        "        no_violence_dir = os.path.join(root_dir, split, 'no_violence')\n",
        "\n",
        "        # Verificar que los directorios existan\n",
        "        if not os.path.exists(violence_dir) or not os.path.exists(no_violence_dir):\n",
        "            raise ValueError(f\"No se encontraron los directorios del dataset en {root_dir}/{split}\")\n",
        "\n",
        "        violence_videos = glob.glob(os.path.join(violence_dir, '*.mp4'))\n",
        "        no_violence_videos = glob.glob(os.path.join(no_violence_dir, '*.mp4'))\n",
        "\n",
        "        if len(violence_videos) == 0 or len(no_violence_videos) == 0:\n",
        "            raise ValueError(f\"No se encontraron videos en {violence_dir} o {no_violence_dir}\")\n",
        "\n",
        "        # Limitar videos si es necesario\n",
        "        if max_videos is not None:\n",
        "            max_per_class = max_videos // 2\n",
        "            violence_videos = violence_videos[:max_per_class]\n",
        "            no_violence_videos = no_violence_videos[:max_per_class]\n",
        "\n",
        "        self.video_paths = violence_videos + no_violence_videos\n",
        "        self.labels = [1] * len(violence_videos) + [0] * len(no_violence_videos)\n",
        "\n",
        "        # Mezclar los datos manteniendo correspondencia entre paths y labels\n",
        "        combined = list(zip(self.video_paths, self.labels))\n",
        "        random.shuffle(combined)\n",
        "        self.video_paths, self.labels = zip(*combined)\n",
        "\n",
        "        # Convertir a lista\n",
        "        self.video_paths = list(self.video_paths)\n",
        "        self.labels = list(self.labels)\n",
        "\n",
        "        print(f\"Cargados {len(self.video_paths)} videos para split '{split}'\")\n",
        "        print(f\"Violencia: {len(violence_videos)}, No Violencia: {len(no_violence_videos)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.video_paths)\n",
        "\n",
        "    def sample_frames_from_video(self, video_path):\n",
        "        \"\"\"Extrae frames uniformemente espaciados del video\"\"\"\n",
        "        try:\n",
        "            # Usar decord para cargar el video eficientemente\n",
        "            video_reader = VideoReader(video_path, ctx=cpu(0))\n",
        "            total_frames = len(video_reader)\n",
        "\n",
        "            if total_frames == 0:\n",
        "                raise ValueError(f\"Video vacío o corrupto: {video_path}\")\n",
        "\n",
        "            # Seleccionar frames uniformemente\n",
        "            indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n",
        "            frames = video_reader.get_batch(indices).asnumpy()  # (num_frames, H, W, C)\n",
        "\n",
        "            # Aplicar resize y normalización\n",
        "            processed_frames = []\n",
        "            for frame in frames:\n",
        "                # Redimensionar\n",
        "                frame = transforms.functional.resize(\n",
        "                    transforms.functional.to_tensor(frame),\n",
        "                    (self.image_size, self.image_size)\n",
        "                )\n",
        "                processed_frames.append(frame)\n",
        "\n",
        "            # Apilar frames\n",
        "            frames_tensor = torch.stack(processed_frames)  # (T, C, H, W)\n",
        "\n",
        "            # Mover dimensiones para coincidir con lo que espera el modelo (B, C, T, H, W)\n",
        "            frames_tensor = frames_tensor.permute(1, 0, 2, 3).unsqueeze(0)\n",
        "\n",
        "            return frames_tensor\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error al procesar video {video_path}: {str(e)}\")\n",
        "            # Retornar un tensor de ceros en caso de error\n",
        "            return torch.zeros((1, 3, self.num_frames, self.image_size, self.image_size))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Obtiene un item por su índice\"\"\"\n",
        "        video_path = self.video_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Extraer frames\n",
        "        frames = self.sample_frames_from_video(video_path)\n",
        "\n",
        "        # Preprocesar frames usando el procesador de TimeSformer\n",
        "        try:\n",
        "            frames_list = list(frames.squeeze(0).permute(1, 0, 2, 3))  # Convertir a lista de tensores (T, C, H, W)\n",
        "            # inputs = self.processor(frames_list, return_tensors=\"pt\")\n",
        "            inputs = self.processor(\n",
        "                frames_list,\n",
        "                return_tensors=\"pt\",\n",
        "                do_rescale=False  # Añadir esta línea para evitar el re-escalado\n",
        "            )\n",
        "            pixel_values = inputs['pixel_values'].squeeze(0)  # Eliminar dim de batch\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error al procesar frames del video {video_path}: {str(e)}\")\n",
        "            # Crear input vacío de tamaño correcto en caso de error\n",
        "            pixel_values = torch.zeros((3, self.num_frames, self.image_size, self.image_size))\n",
        "\n",
        "        return {\n",
        "            'pixel_values': pixel_values,\n",
        "            'labels': torch.tensor(label, dtype=torch.long),\n",
        "            'video_path': video_path\n",
        "        }\n",
        "\n",
        "# ============================== FUNCIONES DE ENTRENAMIENTO Y EVALUACIÓN ==============================\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch, config):\n",
        "    \"\"\"Entrena el modelo durante una época completa\"\"\"\n",
        "    model.train()\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    epoch_precision = 0\n",
        "    epoch_recall = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    # Métricas\n",
        "    accuracy_metric = BinaryAccuracy().to(device)\n",
        "    precision_metric = BinaryPrecision().to(device)\n",
        "    recall_metric = BinaryRecall().to(device)\n",
        "    f1_metric = BinaryF1Score().to(device)\n",
        "\n",
        "    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Época {epoch+1}\")\n",
        "\n",
        "    for step, batch in progress_bar:\n",
        "        try:\n",
        "            # Inicializar con valores predeterminados en caso de error\n",
        "            loss_value = 0.0\n",
        "            accuracy = precision = recall = f1 = 0.0\n",
        "\n",
        "            # Mover datos al dispositivo\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            loss_value = loss.item()  # Guardar valor inmediatamente\n",
        "\n",
        "            # Backward pass\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip gradient norm para estabilidad\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            # Calcular métricas\n",
        "            logits = outputs.logits\n",
        "            preds = torch.sigmoid(logits[:, 1])  # Solo necesitamos la probabilidad de 'violencia'\n",
        "\n",
        "            accuracy = accuracy_metric(preds, labels)\n",
        "            precision = precision_metric(preds, labels)\n",
        "            recall = recall_metric(preds, labels)\n",
        "            f1 = f1_metric(preds, labels)\n",
        "\n",
        "            # Acumular métricas\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += accuracy.item()\n",
        "            epoch_precision += precision.item()\n",
        "            epoch_recall += recall.item()\n",
        "            epoch_f1 += f1.item()\n",
        "\n",
        "            # Actualizar progreso\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': loss.item(),\n",
        "                'acc': accuracy.item(),\n",
        "                'prec': precision.item(),\n",
        "                'rec': recall.item(),\n",
        "                'f1': f1.item()\n",
        "            })\n",
        "\n",
        "            # Liberar memoria explícitamente\n",
        "            del pixel_values, labels, outputs, loss, logits, preds\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "            # Guardar checkpoint cada ciertos pasos\n",
        "            if (step + 1) % config[\"save_steps\"] == 0:\n",
        "                checkpoint_path = os.path.join(\n",
        "                    config[\"output_dir\"],\n",
        "                    f\"checkpoint_epoch{epoch+1}_step{step+1}.pt\"\n",
        "                )\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'step': step,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "                    'loss': loss.item(),\n",
        "                }, checkpoint_path)\n",
        "                logger.info(f\"Guardado checkpoint en {checkpoint_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error en paso {step}, época {epoch+1}: {str(e)}\")\n",
        "            # Intentar liberar memoria y continuar\n",
        "            torch.cuda.empty_cache()\n",
        "            continue\n",
        "\n",
        "    # Calcular métricas promedio\n",
        "    num_batches = len(dataloader)\n",
        "    epoch_loss /= num_batches\n",
        "    epoch_acc /= num_batches\n",
        "    epoch_precision /= num_batches\n",
        "    epoch_recall /= num_batches\n",
        "    epoch_f1 /= num_batches\n",
        "\n",
        "    return {\n",
        "        'loss': epoch_loss,\n",
        "        'accuracy': epoch_acc,\n",
        "        'precision': epoch_precision,\n",
        "        'recall': epoch_recall,\n",
        "        'f1': epoch_f1\n",
        "    }\n",
        "\n",
        "def evaluate(model, dataloader, criterion, device, config):\n",
        "    \"\"\"Evalúa el modelo en un conjunto de datos\"\"\"\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    val_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(dataloader, desc=\"Evaluando\"):\n",
        "            try:\n",
        "                # Mover datos al dispositivo\n",
        "                pixel_values = batch['pixel_values'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(pixel_values=pixel_values, labels=labels)\n",
        "                loss = outputs.loss\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Obtener predicciones\n",
        "                logits = outputs.logits\n",
        "                preds = torch.sigmoid(logits[:, 1])  # Solo la probabilidad de 'violencia'\n",
        "\n",
        "                # Guardar predicciones y etiquetas\n",
        "                all_preds.extend(preds.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Liberar memoria\n",
        "                del pixel_values, labels, outputs, loss, logits, preds\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            except Exception as e:\n",
        "                logger.error(f\"Error al evaluar batch: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "    # Convertir a arrays numpy\n",
        "    all_preds = np.array(all_preds)\n",
        "    all_labels = np.array(all_labels)\n",
        "\n",
        "    if len(all_preds) == 0 or len(all_labels) == 0:\n",
        "        logger.error(\"No se pudieron obtener predicciones o etiquetas durante la evaluación\")\n",
        "        return {\n",
        "            'loss': float('inf'),\n",
        "            'accuracy': 0,\n",
        "            'precision': 0,\n",
        "            'recall': 0,\n",
        "            'specificity': 0,\n",
        "            'f1': 0,\n",
        "            'roc_auc': 0,\n",
        "            'confusion_matrix': np.zeros((2, 2)),\n",
        "            'fpr': np.array([0, 1]),\n",
        "            'tpr': np.array([0, 0]),\n",
        "            'predictions': np.array([]),\n",
        "            'labels': np.array([])\n",
        "        }\n",
        "\n",
        "    # Calcular métricas\n",
        "    binary_preds = (all_preds >= config[\"threshold\"]).astype(int)\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, binary_preds)\n",
        "    precision = precision_score(all_labels, binary_preds, zero_division=0)\n",
        "    recall = recall_score(all_labels, binary_preds, zero_division=0)\n",
        "    f1 = f1_score(all_labels, binary_preds, zero_division=0)\n",
        "\n",
        "    # Calcular especificidad (TN / (TN + FP))\n",
        "    tn, fp, fn, tp = confusion_matrix(all_labels, binary_preds, labels=[0, 1]).ravel()\n",
        "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
        "\n",
        "    # Calcular métricas de curva ROC\n",
        "    try:\n",
        "        fpr, tpr, _ = roc_curve(all_labels, all_preds)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al calcular curva ROC: {str(e)}\")\n",
        "        fpr, tpr = np.array([0, 1]), np.array([0, 0])\n",
        "        roc_auc = 0\n",
        "\n",
        "    # Matriz de confusión\n",
        "    cm = confusion_matrix(all_labels, binary_preds, labels=[0, 1])\n",
        "\n",
        "    # Pérdida promedio\n",
        "    val_loss /= len(dataloader)\n",
        "\n",
        "    # Crear informe de evaluación\n",
        "    eval_results = {\n",
        "        'loss': val_loss,\n",
        "        'accuracy': accuracy,\n",
        "        'precision': precision,\n",
        "        'recall': recall,  # Sensibilidad\n",
        "        'specificity': specificity,\n",
        "        'f1': f1,\n",
        "        'roc_auc': roc_auc,\n",
        "        'confusion_matrix': cm,\n",
        "        'fpr': fpr,\n",
        "        'tpr': tpr,\n",
        "        'predictions': all_preds,\n",
        "        'labels': all_labels\n",
        "    }\n",
        "\n",
        "    return eval_results\n",
        "\n",
        "def plot_metrics(train_metrics, val_metrics, config):\n",
        "    \"\"\"Genera gráficos de métricas de entrenamiento\"\"\"\n",
        "    metrics_to_plot = ['loss', 'accuracy', 'precision', 'recall', 'f1']\n",
        "    epochs = range(1, len(train_metrics['loss']) + 1)\n",
        "\n",
        "    plt.figure(figsize=(20, 15))\n",
        "\n",
        "    for i, metric in enumerate(metrics_to_plot):\n",
        "        plt.subplot(3, 2, i+1)\n",
        "        plt.plot(epochs, train_metrics[metric], 'b-', label=f'Training {metric}')\n",
        "        plt.plot(epochs, val_metrics[metric], 'r-', label=f'Validation {metric}')\n",
        "        plt.title(f'{metric.capitalize()} vs. Epochs')\n",
        "        plt.xlabel('Epochs')\n",
        "        plt.ylabel(metric.capitalize())\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "    # Guardar figura\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], \"training_metrics.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def plot_confusion_matrix(cm, config, phase='transfer_learning'):\n",
        "    \"\"\"Visualiza la matriz de confusión\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['No Violencia', 'Violencia'],\n",
        "                yticklabels=['No Violencia', 'Violencia'])\n",
        "    plt.xlabel('Predicción')\n",
        "    plt.ylabel('Real')\n",
        "    plt.title('Matriz de Confusión')\n",
        "\n",
        "    # Guardar figura\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], f\"confusion_matrix_{phase}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def plot_roc_curve(fpr, tpr, roc_auc, config, phase='transfer_learning'):\n",
        "    \"\"\"Visualiza la curva ROC\"\"\"\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "\n",
        "    # Guardar figura\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], f\"roc_curve_{phase}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "def save_evaluation_report(eval_results, config, phase='transfer_learning'):\n",
        "    \"\"\"Guarda un informe detallado de la evaluación\"\"\"\n",
        "    report = {\n",
        "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        'phase': phase,\n",
        "        'metrics': {\n",
        "            'loss': float(eval_results['loss']),\n",
        "            'accuracy': float(eval_results['accuracy']),\n",
        "            'precision': float(eval_results['precision']),\n",
        "            'recall': float(eval_results['recall']),\n",
        "            'specificity': float(eval_results['specificity']),\n",
        "            'f1_score': float(eval_results['f1']),\n",
        "            'roc_auc': float(eval_results['roc_auc']),\n",
        "        },\n",
        "        'confusion_matrix': eval_results['confusion_matrix'].tolist(),\n",
        "    }\n",
        "\n",
        "    # Guardar informe en formato JSON\n",
        "    with open(os.path.join(config[\"output_dir\"], f\"evaluation_report_{phase}.json\"), 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "\n",
        "    # También guardar en formato de texto para mejor legibilidad\n",
        "    with open(os.path.join(config[\"output_dir\"], f\"evaluation_report_{phase}.txt\"), 'w') as f:\n",
        "        f.write(f\"Evaluación del Modelo - Fase: {phase}\\n\")\n",
        "        f.write(f\"Fecha: {report['timestamp']}\\n\")\n",
        "        f.write(\"\\n=== Métricas ===\\n\")\n",
        "        f.write(f\"Loss: {report['metrics']['loss']:.4f}\\n\")\n",
        "        f.write(f\"Accuracy: {report['metrics']['accuracy']:.4f}\\n\")\n",
        "        f.write(f\"Precision: {report['metrics']['precision']:.4f}\\n\")\n",
        "        f.write(f\"Recall (Sensibilidad): {report['metrics']['recall']:.4f}\\n\")\n",
        "        f.write(f\"Specificity: {report['metrics']['specificity']:.4f}\\n\")\n",
        "        f.write(f\"F1-Score: {report['metrics']['f1_score']:.4f}\\n\")\n",
        "        f.write(f\"ROC AUC: {report['metrics']['roc_auc']:.4f}\\n\")\n",
        "        f.write(\"\\n=== Matriz de Confusión ===\\n\")\n",
        "        f.write(\"                Pred: No Violencia  Pred: Violencia\\n\")\n",
        "        f.write(f\"Real: No Violencia    {eval_results['confusion_matrix'][0][0]}               {eval_results['confusion_matrix'][0][1]}\\n\")\n",
        "        f.write(f\"Real: Violencia       {eval_results['confusion_matrix'][1][0]}               {eval_results['confusion_matrix'][1][1]}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3DXJcCLCgqG"
      },
      "source": [
        "# **ENTRENAMIENTO CON TRANSFER LEARNING**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "c70dc8fa9c75469a8c69b04b7b345fd9",
            "0ab87c4962c24854977b1720b6e397e5",
            "a4794311dbca483ab835bcb8fcdc8b65",
            "78be3b1743e74d8f9c46ad9686ea4d1f",
            "9b20c26e11fe4ddeb978790d5dbda4df",
            "e5a3d9f8c76540d0845eda6990761822",
            "f696fd99ca4c45fb8072d9ccf4ef18b3",
            "f6d4ecffbf66449a837f623669b4f90b",
            "4c920dd08c514eeb9a0a89e8784a85d0",
            "48ba9105fa0c4a84abddb5c2188974f8",
            "304defd4210344dea1df926daca3b354",
            "86fab07cbc724a3c921ac54a20dc184f",
            "3015681206ea445c8a441d38f86abd65",
            "430de3d2c7e64d89b931da4f4f4c015d",
            "d6ecf9ce6b514df89a07ce6e12f0c9c1",
            "9347ea6584014eb99612fd52683d3b91",
            "31a0ce2d230b42978c6455e06367ee9a",
            "88c3145659364b82a7500247fe48fc5e",
            "8f8d3646a0ae4a8da2ca2a47baa46c1a",
            "43c20c9b6cff452d97ff206c646a8203",
            "86494bab4a8448f090bf2ac7221b02c6",
            "05a7492588824d1183fbc5f2057c7097",
            "02c3a9e0ab7b41f1b730034c6825a120",
            "993c467478ac462893cdee1ada619cae",
            "0141526f8d0141b6a812b0a6941272bb",
            "70084cb51d8d4c769b8bba3924790cc0",
            "f3b7cd4fde9441599d0f88d16d8d50ea",
            "abb316a8a9c445f2bb087c34de97759a",
            "0036e20bbb244e398638463dc411bb00",
            "5e507e90e67e40e18ebd1dfcaf570fe0",
            "c0c68530852a4c03822a5eddb7d167ab",
            "7fe7a616069a405899b51b5e0443e383",
            "163caaa69e744e359df1b98f551e936b",
            "cbe04538156045e3aa920af8473d7be0",
            "94ab69d17cc74403816d9ef3ee847ee2",
            "5442d68d0851435c83c97d3b87b97673",
            "9ede8fbc4df74511ad73a7cd5faa4ef0",
            "bba839d147104ce3800c7bcd6a8af8a3",
            "ad153ad30f8f485798e898d835d43f6d",
            "55fdfca0729742b5a9606b88cbb2f58c",
            "3fb063b921f54cdcb828d47e570e0f31",
            "3cbcffb047f94bee9efb6a256821b4cc",
            "b625666bee8e43c789a6c0adae62706f",
            "c341479863d24a299bd5f91f7505924f"
          ]
        },
        "id": "cs5N5PgCn7vg",
        "outputId": "e6302b25-e8b3-400d-b6a7-95a99d6e4810"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando fase de Transfer Learning\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c70dc8fa9c75469a8c69b04b7b345fd9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/22.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "86fab07cbc724a3c921ac54a20dc184f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/486M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parámetros entrenables: 1,538 / 121,260,290 (0.00%)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "02c3a9e0ab7b41f1b730034c6825a120",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbe04538156045e3aa920af8473d7be0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/486M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargados 8000 videos para split 'train'\n",
            "Violencia: 4000, No Violencia: 4000\n",
            "Cargados 1500 videos para split 'val'\n",
            "Violencia: 750, No Violencia: 750\n",
            "Iniciando época 1/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 1:  20%|█▉        | 199/1000 [09:04<33:59,  2.55s/it, loss=0.712, acc=0.375, prec=0.2, rec=0.5, f1=0.286]ERROR:__main__:Error en paso 199, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1:  40%|███▉      | 399/1000 [17:14<31:30,  3.14s/it, loss=0.926, acc=0.5, prec=0.667, rec=0.4, f1=0.5] ERROR:__main__:Error en paso 399, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1:  60%|█████▉    | 599/1000 [25:49<15:07,  2.26s/it, loss=0.621, acc=0.5, prec=0.667, rec=0.4, f1=0.5]     ERROR:__main__:Error en paso 599, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1:  80%|███████▉  | 799/1000 [33:44<06:02,  1.80s/it, loss=0.431, acc=0.5, prec=1, rec=0.333, f1=0.5]  ERROR:__main__:Error en paso 799, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1: 100%|█████████▉| 999/1000 [41:38<00:02,  2.07s/it, loss=0.249, acc=0.875, prec=1, rec=0.75, f1=0.857] ERROR:__main__:Error en paso 999, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1: 100%|██████████| 1000/1000 [41:39<00:00,  2.50s/it, loss=0.249, acc=0.875, prec=1, rec=0.75, f1=0.857]\n",
            "Evaluando: 100%|██████████| 188/188 [13:03<00:00,  4.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 - Train Loss: 0.6173, Val Loss: 0.3113, Train Acc: 0.5199, Val Acc: 0.7760, Val F1: 0.7214\n",
            "Guardado mejor modelo con F1: 0.7214 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n",
            "Guardado checkpoint de época 1 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_epoch1.pt\n",
            "Iniciando época 2/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 2:  20%|█▉        | 199/1000 [01:50<07:19,  1.82it/s, loss=0.37, acc=0.75, prec=1, rec=0.667, f1=0.8]ERROR:__main__:Error en paso 199, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2:  40%|███▉      | 399/1000 [03:41<05:31,  1.82it/s, loss=0.15, acc=0.875, prec=1, rec=0.667, f1=0.8]     ERROR:__main__:Error en paso 399, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2:  60%|█████▉    | 599/1000 [05:30<03:40,  1.82it/s, loss=0.368, acc=0.75, prec=0.75, rec=0.75, f1=0.75]ERROR:__main__:Error en paso 599, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2:  80%|███████▉  | 799/1000 [07:20<01:50,  1.82it/s, loss=0.261, acc=0.875, prec=0.8, rec=1, f1=0.889]ERROR:__main__:Error en paso 799, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2: 100%|█████████▉| 999/1000 [09:10<00:00,  1.82it/s, loss=0.121, acc=0.875, prec=0.857, rec=1, f1=0.923]ERROR:__main__:Error en paso 999, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2: 100%|██████████| 1000/1000 [09:11<00:00,  1.81it/s, loss=0.121, acc=0.875, prec=0.857, rec=1, f1=0.923]\n",
            "Evaluando: 100%|██████████| 188/188 [01:43<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 - Train Loss: 0.2418, Val Loss: 0.1893, Train Acc: 0.8856, Val Acc: 0.8973, Val F1: 0.8882\n",
            "Guardado mejor modelo con F1: 0.8882 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n",
            "Guardado checkpoint de época 2 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_epoch2.pt\n",
            "Iniciando época 3/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 3:  20%|█▉        | 199/1000 [01:52<07:19,  1.82it/s, loss=0.255, acc=0.875, prec=0.857, rec=1, f1=0.923]ERROR:__main__:Error en paso 199, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3:  40%|███▉      | 399/1000 [03:42<05:29,  1.82it/s, loss=0.13, acc=0.875, prec=1, rec=0.833, f1=0.909]ERROR:__main__:Error en paso 399, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3:  60%|█████▉    | 599/1000 [05:32<03:40,  1.82it/s, loss=0.142, acc=0.875, prec=1, rec=0.8, f1=0.889] ERROR:__main__:Error en paso 599, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3:  80%|███████▉  | 799/1000 [07:22<01:50,  1.82it/s, loss=0.241, acc=0.875, prec=0.667, rec=1, f1=0.8]ERROR:__main__:Error en paso 799, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3: 100%|█████████▉| 999/1000 [09:12<00:00,  1.82it/s, loss=0.238, acc=1, prec=1, rec=1, f1=1]           ERROR:__main__:Error en paso 999, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3: 100%|██████████| 1000/1000 [09:12<00:00,  1.81it/s, loss=0.238, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:43<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 - Train Loss: 0.1828, Val Loss: 0.1600, Train Acc: 0.9175, Val Acc: 0.9167, Val F1: 0.9113\n",
            "Guardado mejor modelo con F1: 0.9113 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n",
            "Guardado checkpoint de época 3 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_epoch3.pt\n",
            "Iniciando época 4/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 4:  20%|█▉        | 199/1000 [01:56<07:18,  1.83it/s, loss=0.504, acc=0.875, prec=0.833, rec=1, f1=0.909]  ERROR:__main__:Error en paso 199, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4:  40%|███▉      | 399/1000 [03:46<05:30,  1.82it/s, loss=0.00767, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4:  60%|█████▉    | 599/1000 [05:36<03:40,  1.82it/s, loss=0.0319, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4:  80%|███████▉  | 799/1000 [07:26<01:50,  1.82it/s, loss=0.0214, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 799, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4: 100%|█████████▉| 999/1000 [09:16<00:00,  1.82it/s, loss=0.0139, acc=1, prec=1, rec=1, f1=1]         ERROR:__main__:Error en paso 999, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4: 100%|██████████| 1000/1000 [09:16<00:00,  1.80it/s, loss=0.0139, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:43<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 - Train Loss: 0.1619, Val Loss: 0.1470, Train Acc: 0.9274, Val Acc: 0.9227, Val F1: 0.9180\n",
            "Guardado mejor modelo con F1: 0.9180 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n",
            "Guardado checkpoint de época 4 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_epoch4.pt\n",
            "Iniciando época 5/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 5:  20%|█▉        | 199/1000 [01:54<07:18,  1.83it/s, loss=0.0414, acc=1, prec=1, rec=1, f1=1]         ERROR:__main__:Error en paso 199, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5:  40%|███▉      | 399/1000 [03:44<05:30,  1.82it/s, loss=0.0261, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5:  60%|█████▉    | 599/1000 [05:34<03:40,  1.82it/s, loss=0.497, acc=0.875, prec=1, rec=0.8, f1=0.889]ERROR:__main__:Error en paso 599, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5:  80%|███████▉  | 799/1000 [07:24<01:50,  1.81it/s, loss=0.665, acc=0.875, prec=0.75, rec=1, f1=0.857]ERROR:__main__:Error en paso 799, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5: 100%|█████████▉| 999/1000 [09:14<00:00,  1.82it/s, loss=0.0667, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 999, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5: 100%|██████████| 1000/1000 [09:14<00:00,  1.80it/s, loss=0.0667, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:43<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 - Train Loss: 0.1506, Val Loss: 0.1389, Train Acc: 0.9346, Val Acc: 0.9253, Val F1: 0.9211\n",
            "Guardado mejor modelo con F1: 0.9211 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n",
            "Guardado checkpoint de época 5 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_epoch5.pt\n",
            "Iniciando época 6/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 6:  20%|█▉        | 199/1000 [01:54<07:20,  1.82it/s, loss=0.0418, acc=1, prec=1, rec=1, f1=1]          ERROR:__main__:Error en paso 199, época 6: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 6:  40%|███▉      | 399/1000 [03:44<05:29,  1.82it/s, loss=0.101, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 399, época 6: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 6:  60%|█████▉    | 599/1000 [05:34<03:40,  1.81it/s, loss=0.109, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, época 6: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 6:  80%|███████▉  | 799/1000 [07:24<01:50,  1.82it/s, loss=0.247, acc=0.75, prec=1, rec=0.714, f1=0.833]ERROR:__main__:Error en paso 799, época 6: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 6: 100%|█████████▉| 999/1000 [09:14<00:00,  1.82it/s, loss=0.0894, acc=0.875, prec=1, rec=0.833, f1=0.909]ERROR:__main__:Error en paso 999, época 6: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 6: 100%|██████████| 1000/1000 [09:14<00:00,  1.80it/s, loss=0.0894, acc=0.875, prec=1, rec=0.833, f1=0.909]\n",
            "Evaluando: 100%|██████████| 188/188 [01:43<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 - Train Loss: 0.1434, Val Loss: 0.1352, Train Acc: 0.9380, Val Acc: 0.9287, Val F1: 0.9248\n",
            "Guardado mejor modelo con F1: 0.9248 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n",
            "Guardado checkpoint de época 6 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_epoch6.pt\n",
            "Iniciando época 7/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 7:  20%|█▉        | 199/1000 [01:52<07:20,  1.82it/s, loss=0.248, acc=0.875, prec=0.75, rec=1, f1=0.857]ERROR:__main__:Error en paso 199, época 7: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 7:  40%|███▉      | 399/1000 [03:42<05:30,  1.82it/s, loss=0.0938, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 7: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 7:  60%|█████▉    | 599/1000 [05:32<03:40,  1.82it/s, loss=0.0319, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, época 7: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 7:  80%|███████▉  | 799/1000 [07:22<01:50,  1.82it/s, loss=0.0173, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 799, época 7: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 7: 100%|█████████▉| 999/1000 [09:12<00:00,  1.82it/s, loss=0.0144, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 999, época 7: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 7: 100%|██████████| 1000/1000 [09:12<00:00,  1.81it/s, loss=0.0144, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:43<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/10 - Train Loss: 0.1391, Val Loss: 0.1322, Train Acc: 0.9409, Val Acc: 0.9287, Val F1: 0.9249\n",
            "Guardado mejor modelo con F1: 0.9249 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n",
            "Guardado checkpoint de época 7 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_epoch7.pt\n",
            "Iniciando época 8/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 8:  20%|█▉        | 199/1000 [01:55<07:19,  1.82it/s, loss=0.0282, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 199, época 8: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 8:  40%|███▉      | 399/1000 [03:45<05:30,  1.82it/s, loss=0.0452, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 8: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 8:  60%|█████▉    | 599/1000 [05:35<03:41,  1.81it/s, loss=0.0355, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, época 8: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 8:  80%|███████▉  | 799/1000 [07:25<01:50,  1.82it/s, loss=0.174, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 799, época 8: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 8: 100%|█████████▉| 999/1000 [09:15<00:00,  1.82it/s, loss=0.234, acc=0.875, prec=1, rec=0.875, f1=0.933]ERROR:__main__:Error en paso 999, época 8: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 8: 100%|██████████| 1000/1000 [09:15<00:00,  1.80it/s, loss=0.234, acc=0.875, prec=1, rec=0.875, f1=0.933]\n",
            "Evaluando: 100%|██████████| 188/188 [01:43<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/10 - Train Loss: 0.1365, Val Loss: 0.1309, Train Acc: 0.9416, Val Acc: 0.9293, Val F1: 0.9257\n",
            "Guardado mejor modelo con F1: 0.9257 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n",
            "Guardado checkpoint de época 8 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_epoch8.pt\n",
            "Iniciando época 9/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 9:  20%|█▉        | 199/1000 [01:53<07:19,  1.82it/s, loss=0.0313, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 199, época 9: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 9:  40%|███▉      | 399/1000 [03:43<05:31,  1.81it/s, loss=0.0185, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 9: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 9:  60%|█████▉    | 599/1000 [05:33<03:41,  1.81it/s, loss=0.365, acc=0.875, prec=1, rec=0.833, f1=0.909]ERROR:__main__:Error en paso 599, época 9: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 9:  80%|███████▉  | 799/1000 [07:23<01:50,  1.82it/s, loss=0.044, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 799, época 9: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 9: 100%|█████████▉| 999/1000 [09:13<00:00,  1.82it/s, loss=0.0322, acc=1, prec=1, rec=1, f1=1]         ERROR:__main__:Error en paso 999, época 9: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 9: 100%|██████████| 1000/1000 [09:13<00:00,  1.81it/s, loss=0.0322, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:43<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/10 - Train Loss: 0.1353, Val Loss: 0.1304, Train Acc: 0.9425, Val Acc: 0.9313, Val F1: 0.9279\n",
            "Guardado mejor modelo con F1: 0.9279 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n",
            "Guardado checkpoint de época 9 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_epoch9.pt\n",
            "Iniciando época 10/10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 10:  20%|█▉        | 199/1000 [01:53<07:20,  1.82it/s, loss=0.298, acc=0.875, prec=0.8, rec=1, f1=0.889] ERROR:__main__:Error en paso 199, época 10: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 10:  40%|███▉      | 399/1000 [03:43<05:30,  1.82it/s, loss=0.275, acc=0.875, prec=1, rec=0.8, f1=0.889]ERROR:__main__:Error en paso 399, época 10: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 10:  60%|█████▉    | 599/1000 [05:33<03:40,  1.82it/s, loss=0.238, acc=0.875, prec=1, rec=0.667, f1=0.8]ERROR:__main__:Error en paso 599, época 10: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 10:  80%|███████▉  | 799/1000 [07:23<01:50,  1.82it/s, loss=0.183, acc=0.75, prec=1, rec=0.333, f1=0.5]  ERROR:__main__:Error en paso 799, época 10: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 10: 100%|█████████▉| 999/1000 [09:13<00:00,  1.82it/s, loss=0.117, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 999, época 10: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 10: 100%|██████████| 1000/1000 [09:13<00:00,  1.81it/s, loss=0.117, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:43<00:00,  1.82it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 10/10 - Train Loss: 0.1348, Val Loss: 0.1304, Train Acc: 0.9430, Val Acc: 0.9313, Val F1: 0.9279\n",
            "Guardado checkpoint de época 10 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_epoch10.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================== ENTRENAMIENTO CON TRANSFER LEARNING ==============================\n",
        "\n",
        "logger.info(\"Iniciando fase de Transfer Learning\")\n",
        "print(\"Iniciando fase de Transfer Learning\")\n",
        "\n",
        "# 1. Cargar modelo pre-entrenado\n",
        "model = TimesformerForVideoClassification.from_pretrained(\n",
        "    CONFIG[\"pretrained_model\"],\n",
        "    num_frames=CONFIG[\"num_frames\"],\n",
        "    image_size=CONFIG[\"image_size\"],\n",
        "    num_labels=CONFIG[\"num_classes\"],  # Añadir esto para configurar 2 clases desde el inicio\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "\n",
        "# 2. Asegurarnos de que la clasificación final tiene el número correcto de salidas\n",
        "if hasattr(model, 'classifier'):\n",
        "    if hasattr(model.classifier, 'out_features') and model.classifier.out_features != CONFIG[\"num_classes\"]:\n",
        "        # Guardar dimensión de entrada\n",
        "        in_features = model.classifier.in_features\n",
        "\n",
        "        # Reemplazar completamente el clasificador\n",
        "        model.classifier = nn.Linear(in_features, CONFIG[\"num_classes\"])\n",
        "\n",
        "        logger.info(f\"Reemplazada capa de clasificación: {in_features} -> {CONFIG['num_classes']}\")\n",
        "    elif isinstance(model.classifier, nn.Sequential):\n",
        "        # Si ya es una secuencia, asegurarnos que la última capa tenga la salida correcta\n",
        "        last_layer = model.classifier[-1]\n",
        "        if hasattr(last_layer, 'out_features') and last_layer.out_features != CONFIG[\"num_classes\"]:\n",
        "            in_features = last_layer.in_features\n",
        "            model.classifier[-1] = nn.Linear(in_features, CONFIG[\"num_classes\"])\n",
        "            logger.info(f\"Reemplazada última capa de clasificación: {in_features} -> {CONFIG['num_classes']}\")\n",
        "\n",
        "\n",
        "# 3. Congelar los parámetros del modelo base (excepto los de la capa de clasificación)\n",
        "for name, param in model.named_parameters():\n",
        "    if 'classifier' not in name:  # Congelar todos los parámetros excepto los del clasificador\n",
        "        param.requires_grad = False\n",
        "\n",
        "# Verificar parámetros entrenables vs congelados\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "logger.info(f\"Parámetros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"Parámetros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n",
        "# Mover modelo a GPU\n",
        "model.to(device)\n",
        "\n",
        "# 4. Preparar datasets y dataloaders\n",
        "train_dataset = ViolenceVideoDataset(\n",
        "    root_dir=CONFIG[\"dataset_path\"],\n",
        "    split='train',\n",
        "    num_frames=CONFIG[\"num_frames\"],\n",
        "    image_size=CONFIG[\"image_size\"]\n",
        ")\n",
        "\n",
        "val_dataset = ViolenceVideoDataset(\n",
        "    root_dir=CONFIG[\"dataset_path\"],\n",
        "    split='val',\n",
        "    num_frames=CONFIG[\"num_frames\"],\n",
        "    image_size=CONFIG[\"image_size\"]\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG[\"tl_batch_size\"],\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG[\"tl_batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# 5. Configurar optimizador y scheduler\n",
        "optimizer = optim.AdamW(\n",
        "    filter(lambda p: p.requires_grad, model.parameters()),\n",
        "    lr=CONFIG[\"tl_learning_rate\"],\n",
        "    weight_decay=CONFIG[\"tl_weight_decay\"]\n",
        ")\n",
        "\n",
        "# Calcular pasos totales para schedulers\n",
        "num_training_steps = len(train_dataloader) * CONFIG[\"tl_num_epochs\"]\n",
        "num_warmup_steps = int(num_training_steps * CONFIG[\"tl_warmup_ratio\"])\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# 6. Criterio de pérdida (ya incluido en el modelo)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# 7. Inicializar tracking de métricas\n",
        "best_val_f1 = 0.0\n",
        "train_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "val_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n",
        "\n",
        "# 8. Entrenamiento por épocas\n",
        "for epoch in range(CONFIG[\"tl_num_epochs\"]):\n",
        "    logger.info(f\"Iniciando época {epoch+1}/{CONFIG['tl_num_epochs']}\")\n",
        "    print(f\"Iniciando época {epoch+1}/{CONFIG['tl_num_epochs']}\")\n",
        "\n",
        "    # Entrenamiento\n",
        "    train_results = train_epoch(\n",
        "        model=model,\n",
        "        dataloader=train_dataloader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        criterion=criterion,\n",
        "        device=device,\n",
        "        epoch=epoch,\n",
        "        config=CONFIG\n",
        "    )\n",
        "\n",
        "    # Evaluación\n",
        "    eval_results = evaluate(\n",
        "        model=model,\n",
        "        dataloader=val_dataloader,\n",
        "        criterion=criterion,\n",
        "        device=device,\n",
        "        config=CONFIG\n",
        "    )\n",
        "\n",
        "    # Registrar métricas\n",
        "    for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1']:\n",
        "        train_metrics[metric].append(train_results[metric])\n",
        "        val_metrics[metric].append(eval_results[metric])\n",
        "\n",
        "    # Mostrar resultados\n",
        "    logger.info(f\"Epoch {epoch+1}/{CONFIG['tl_num_epochs']} - \"\n",
        "               f\"Train Loss: {train_results['loss']:.4f}, \"\n",
        "               f\"Val Loss: {eval_results['loss']:.4f}, \"\n",
        "               f\"Train Acc: {train_results['accuracy']:.4f}, \"\n",
        "               f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n",
        "               f\"Val F1: {eval_results['f1']:.4f}\")\n",
        "    print(f\"Epoch {epoch+1}/{CONFIG['tl_num_epochs']} - \"\n",
        "          f\"Train Loss: {train_results['loss']:.4f}, \"\n",
        "          f\"Val Loss: {eval_results['loss']:.4f}, \"\n",
        "          f\"Train Acc: {train_results['accuracy']:.4f}, \"\n",
        "          f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n",
        "          f\"Val F1: {eval_results['f1']:.4f}\")\n",
        "\n",
        "    # Guardar mejor modelo\n",
        "    if eval_results['f1'] > best_val_f1:\n",
        "        best_val_f1 = eval_results['f1']\n",
        "\n",
        "        # Guardar modelo\n",
        "        model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_tl.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'val_f1': best_val_f1,\n",
        "            'config': CONFIG,\n",
        "        }, model_path)\n",
        "\n",
        "        logger.info(f\"Guardado mejor modelo con F1: {best_val_f1:.4f} en {model_path}\")\n",
        "        print(f\"Guardado mejor modelo con F1: {best_val_f1:.4f} en {model_path}\")\n",
        "\n",
        "    # Guardar checkpoint al final de cada época\n",
        "    checkpoint_path = os.path.join(CONFIG[\"output_dir\"], f\"checkpoint_epoch{epoch+1}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict(),\n",
        "        'train_metrics': train_metrics,\n",
        "        'val_metrics': val_metrics,\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    logger.info(f\"Guardado checkpoint de época {epoch+1} en {checkpoint_path}\")\n",
        "    print(f\"Guardado checkpoint de época {epoch+1} en {checkpoint_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDiaD6363f1I",
        "outputId": "f477ab0f-5a03-4d9c-af50-2cca6a3570c4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluando mejor modelo de Transfer Learning (F1: 0.9279)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluando: 100%|██████████| 188/188 [01:42<00:00,  1.83it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completada fase de Transfer Learning\n"
          ]
        }
      ],
      "source": [
        "# 9. Visualizar y guardar métricas\n",
        "plot_metrics(train_metrics, val_metrics, CONFIG)\n",
        "\n",
        "# 10. Evaluación final del mejor modelo\n",
        "# Cargar el mejor modelo\n",
        "best_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_tl.pt\")\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "logger.info(f\"Evaluando mejor modelo de Transfer Learning (F1: {checkpoint['val_f1']:.4f})\")\n",
        "print(f\"Evaluando mejor modelo de Transfer Learning (F1: {checkpoint['val_f1']:.4f})\")\n",
        "\n",
        "final_eval_results = evaluate(\n",
        "    model=model,\n",
        "    dataloader=val_dataloader,\n",
        "    criterion=criterion,\n",
        "    device=device,\n",
        "    config=CONFIG\n",
        ")\n",
        "\n",
        "# Visualizar matriz de confusión\n",
        "plot_confusion_matrix(final_eval_results['confusion_matrix'], CONFIG, phase='transfer_learning')\n",
        "\n",
        "# Visualizar curva ROC\n",
        "plot_roc_curve(\n",
        "    final_eval_results['fpr'],\n",
        "    final_eval_results['tpr'],\n",
        "    final_eval_results['roc_auc'],\n",
        "    CONFIG,\n",
        "    phase='transfer_learning'\n",
        ")\n",
        "\n",
        "# Guardar informe detallado\n",
        "save_evaluation_report(final_eval_results, CONFIG, phase='transfer_learning')\n",
        "\n",
        "logger.info(\"Completada fase de Transfer Learning\")\n",
        "print(\"Completada fase de Transfer Learning\")\n",
        "\n",
        "# Guardar resultados para usarlos en etapas posteriores\n",
        "tl_results = final_eval_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "01q4XewQs9fa"
      },
      "source": [
        "# ENTRENAMIENTO CON FINE TUNING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "a21a2e5fe08743b19089010ab6643f65",
            "6d9802e791374f19b92af905ade50637",
            "20359e031e13479185630f6f08c81deb",
            "dad3a562e2d04632b508f7a14540a6e6",
            "6b2871975c2f4da8865b9a6712ab66b2",
            "d935d37c6b2c434a8036b33cb20ebe61",
            "17843f701f674c7b836a2d232a81b1ed",
            "7039014ae1f24d6da3fb528b30122076",
            "fb869f720924426eb8cf0d86e80a5fe5",
            "e1a3a9e38df94c03aacfb1a0ab07d0f6",
            "6c621cfb7bcf4bbf93f9f0cafe354c11"
          ]
        },
        "id": "225qRrTYtZAo",
        "outputId": "c2f95dea-bce0-437f-c7ed-9d593a6bea1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando fase de Fine-Tuning\n",
            "Creando y cargando modelo desde checkpoint\n",
            "Parámetros entrenables: 121,260,290 / 121,260,290 (100.00%)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a21a2e5fe08743b19089010ab6643f65",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargados 8000 videos para split 'train'\n",
            "Violencia: 4000, No Violencia: 4000\n",
            "Cargados 1500 videos para split 'val'\n",
            "Violencia: 750, No Violencia: 750\n",
            "Iniciando época 1/10 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 1:  20%|█▉        | 199/1000 [10:52<30:51,  2.31s/it, loss=0.0106, acc=1, prec=1, rec=1, f1=1]        ERROR:__main__:Error en paso 199, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1:  40%|███▉      | 399/1000 [18:48<21:51,  2.18s/it, loss=0.0996, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1:  60%|█████▉    | 599/1000 [26:32<18:07,  2.71s/it, loss=0.127, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 599, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1:  80%|███████▉  | 799/1000 [34:36<08:51,  2.64s/it, loss=0.0321, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 799, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1: 100%|█████████▉| 999/1000 [43:19<00:02,  2.65s/it, loss=0.0394, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 999, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1: 100%|██████████| 1000/1000 [43:19<00:00,  2.60s/it, loss=0.0394, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [12:41<00:00,  4.05s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10 (FT) - Train Loss: 0.1266, Val Loss: 0.1082, Train Acc: 0.9484, Val Acc: 0.9193, Val F1: 0.9126, Val Specificity: 0.9960\n",
            "Guardado mejor modelo (FT) con F1: 0.9126 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft.pt\n",
            "Guardado checkpoint de fine-tuning época 1 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch1.pt\n",
            "Iniciando época 2/10 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 2:  20%|█▉        | 199/1000 [05:42<22:43,  1.70s/it, loss=0.39, acc=0.75, prec=0.8, rec=0.8, f1=0.8]  ERROR:__main__:Error en paso 199, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2:  40%|███▉      | 399/1000 [11:23<17:03,  1.70s/it, loss=0.0287, acc=0.875, prec=0.833, rec=1, f1=0.909]ERROR:__main__:Error en paso 399, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2:  60%|█████▉    | 599/1000 [17:05<11:25,  1.71s/it, loss=0.0894, acc=0.875, prec=0.75, rec=1, f1=0.857]ERROR:__main__:Error en paso 599, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2:  80%|███████▉  | 799/1000 [22:46<05:43,  1.71s/it, loss=0.101, acc=0.875, prec=1, rec=0.75, f1=0.857]ERROR:__main__:Error en paso 799, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2: 100%|█████████▉| 999/1000 [28:27<00:01,  1.71s/it, loss=0.00208, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 999, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2: 100%|██████████| 1000/1000 [28:27<00:00,  1.71s/it, loss=0.00208, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:44<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/10 (FT) - Train Loss: 0.0910, Val Loss: 0.0777, Train Acc: 0.9651, Val Acc: 0.9573, Val F1: 0.9558, Val Specificity: 0.9920\n",
            "Guardado mejor modelo (FT) con F1: 0.9558 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft.pt\n",
            "Guardado checkpoint de fine-tuning época 2 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch2.pt\n",
            "Iniciando época 3/10 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 3:  20%|█▉        | 199/1000 [06:04<22:47,  1.71s/it, loss=0.0137, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 199, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3:  40%|███▉      | 399/1000 [11:45<17:03,  1.70s/it, loss=0.0169, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 399, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3:  60%|█████▉    | 599/1000 [17:25<11:23,  1.70s/it, loss=0.00141, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3:  80%|███████▉  | 799/1000 [23:06<05:41,  1.70s/it, loss=0.0707, acc=0.875, prec=0.833, rec=1, f1=0.909]ERROR:__main__:Error en paso 799, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3: 100%|█████████▉| 999/1000 [28:47<00:01,  1.71s/it, loss=0.00135, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 999, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3: 100%|██████████| 1000/1000 [28:48<00:00,  1.73s/it, loss=0.00135, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:44<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/10 (FT) - Train Loss: 0.0600, Val Loss: 0.0611, Train Acc: 0.9764, Val Acc: 0.9620, Val F1: 0.9608, Val Specificity: 0.9933\n",
            "Guardado mejor modelo (FT) con F1: 0.9608 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft.pt\n",
            "Guardado checkpoint de fine-tuning época 3 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch3.pt\n",
            "Iniciando época 4/10 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 4:  20%|█▉        | 199/1000 [06:03<22:47,  1.71s/it, loss=0.00028, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 199, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4:  40%|███▉      | 399/1000 [11:44<17:06,  1.71s/it, loss=0.00421, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4:  60%|█████▉    | 599/1000 [17:25<11:21,  1.70s/it, loss=0.00444, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4:  80%|███████▉  | 799/1000 [23:06<05:43,  1.71s/it, loss=0.00286, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 799, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4: 100%|█████████▉| 999/1000 [28:47<00:01,  1.70s/it, loss=0.00583, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 999, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4: 100%|██████████| 1000/1000 [28:48<00:00,  1.73s/it, loss=0.00583, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:44<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/10 (FT) - Train Loss: 0.0396, Val Loss: 0.0510, Train Acc: 0.9859, Val Acc: 0.9700, Val F1: 0.9693, Val Specificity: 0.9933\n",
            "Guardado mejor modelo (FT) con F1: 0.9693 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft.pt\n",
            "Guardado checkpoint de fine-tuning época 4 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch4.pt\n",
            "Iniciando época 5/10 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 5:  20%|█▉        | 199/1000 [06:03<22:48,  1.71s/it, loss=0.0108, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 199, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5:  40%|███▉      | 399/1000 [11:44<17:06,  1.71s/it, loss=0.000839, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5:  60%|█████▉    | 599/1000 [17:25<11:24,  1.71s/it, loss=0.00257, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 599, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5:  80%|███████▉  | 799/1000 [23:06<05:43,  1.71s/it, loss=0.00013, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 799, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5: 100%|█████████▉| 999/1000 [28:47<00:01,  1.70s/it, loss=0.00109, acc=1, prec=1, rec=1, f1=1]        ERROR:__main__:Error en paso 999, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5: 100%|██████████| 1000/1000 [28:47<00:00,  1.73s/it, loss=0.00109, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:44<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/10 (FT) - Train Loss: 0.0274, Val Loss: 0.0457, Train Acc: 0.9899, Val Acc: 0.9727, Val F1: 0.9720, Val Specificity: 0.9960\n",
            "Guardado mejor modelo (FT) con F1: 0.9720 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft.pt\n",
            "Guardado checkpoint de fine-tuning época 5 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch5.pt\n",
            "Iniciando época 6/10 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 6:  20%|█▉        | 199/1000 [06:03<22:41,  1.70s/it, loss=0.00243, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 199, época 6: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 6:  40%|███▉      | 399/1000 [11:43<17:05,  1.71s/it, loss=0.00228, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 399, época 6: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 6:  60%|█████▉    | 599/1000 [17:24<11:21,  1.70s/it, loss=0.561, acc=0.875, prec=1, rec=0.8, f1=0.889]ERROR:__main__:Error en paso 599, época 6: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 6:  80%|███████▉  | 799/1000 [23:05<05:43,  1.71s/it, loss=0.00112, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 799, época 6: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 6: 100%|█████████▉| 999/1000 [28:46<00:01,  1.71s/it, loss=0.000837, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 999, época 6: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 6: 100%|██████████| 1000/1000 [28:46<00:00,  1.73s/it, loss=0.000837, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [01:44<00:00,  1.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/10 (FT) - Train Loss: 0.0196, Val Loss: 0.0398, Train Acc: 0.9941, Val Acc: 0.9787, Val F1: 0.9783, Val Specificity: 0.9960\n",
            "Guardado mejor modelo (FT) con F1: 0.9783 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft.pt\n",
            "Guardado checkpoint de fine-tuning época 6 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch6.pt\n",
            "Iniciando época 7/10 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 7:  20%|█▉        | 199/1000 [06:05<22:41,  1.70s/it, loss=0.000409, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 199, época 7: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 7:  40%|███▉      | 399/1000 [11:46<17:07,  1.71s/it, loss=0.0102, acc=1, prec=1, rec=1, f1=1]  ERROR:__main__:Error en paso 399, época 7: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 7:  55%|█████▌    | 550/1000 [16:02<12:45,  1.70s/it, loss=0.000943, acc=1, prec=1, rec=1, f1=1]"
          ]
        }
      ],
      "source": [
        "# ============================== ENTRENAMIENTO FINE TUNING ==============================\n",
        "\n",
        "logger.info(\"Iniciando fase de Fine-Tuning\")\n",
        "print(\"Iniciando fase de Fine-Tuning\")\n",
        "\n",
        "# Cargar el mejor modelo de Transfer Learning\n",
        "best_tl_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_tl.pt\")\n",
        "checkpoint = torch.load(best_tl_model_path)\n",
        "\n",
        "# Comprobar si ya tenemos el modelo cargado (de la celda anterior) o necesitamos cargarlo\n",
        "try:\n",
        "    # Intentar acceder al modelo, si no está definido, lanzará una excepción\n",
        "    model\n",
        "    logger.info(\"Usando modelo ya cargado de celda anterior\")\n",
        "    print(\"Usando modelo ya cargado de celda anterior\")\n",
        "    # Cargar estado del modelo desde checkpoint\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "except NameError:\n",
        "    # Si el modelo no está definido, crear uno nuevo y cargarlo\n",
        "    logger.info(\"Creando y cargando modelo desde checkpoint\")\n",
        "    print(\"Creando y cargando modelo desde checkpoint\")\n",
        "    # model = TimesformerForVideoClassification.from_pretrained(\n",
        "    #     CONFIG[\"pretrained_model\"],\n",
        "    #     num_frames=CONFIG[\"num_frames\"],\n",
        "    #     image_size=CONFIG[\"image_size\"],\n",
        "    #     num_labels=CONFIG[\"num_classes\"],\n",
        "    #     # ignore_mismatched_sizes=True\n",
        "    # )\n",
        "\n",
        "    # Cargar la configuración del modelo pre-entrenado y modificarla para 2 clases\n",
        "    config = TimesformerConfig.from_pretrained(\n",
        "        CONFIG[\"pretrained_model\"],\n",
        "        num_frames=CONFIG[\"num_frames\"],\n",
        "        image_size=CONFIG[\"image_size\"],\n",
        "        num_labels=CONFIG[\"num_classes\"], # Asegura que la config especifique 2 clases\n",
        "    )\n",
        "\n",
        "    # Crear una instancia del modelo *con la configuración de 2 clases*\n",
        "    model = TimesformerForVideoClassification(config)\n",
        "\n",
        "    # Cargar estado del modelo desde checkpoint\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Ahora cargar el state_dict del checkpoint.\n",
        "    # Como la arquitectura ya coincide (capa de 2 clases), strict=True debería funcionar.\n",
        "    # El checkpoint contiene el estado COMPLETO del modelo después del TL.\n",
        "    model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
        "\n",
        "# 1. Descongelar todos los parámetros del modelo\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Verificar parámetros entrenables\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "logger.info(f\"Parámetros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"Parámetros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n",
        "# Asegurar que el modelo está en el dispositivo correcto\n",
        "model.to(device)\n",
        "\n",
        "# 2. Preparar datasets y dataloaders con augmentación adicional para fine-tuning\n",
        "# Definir transformaciones para data augmentation más agresivas\n",
        "class VideoAugmentor:\n",
        "    \"\"\"Aplica augmentación a nivel de frame para videos\"\"\"\n",
        "    def __init__(self, strength=0.3):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ColorJitter(brightness=0.2*strength,\n",
        "                                  contrast=0.2*strength,\n",
        "                                  saturation=0.2*strength,\n",
        "                                  hue=0.1*strength),\n",
        "            transforms.RandomAffine(degrees=5*strength,\n",
        "                                   translate=(0.1*strength, 0.1*strength),\n",
        "                                   scale=(1-0.1*strength, 1+0.1*strength)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        \"\"\"Aplica la misma transformación a todos los frames del video\"\"\"\n",
        "        # Obtener una transformación aleatoria (para consistencia entre frames)\n",
        "        result = []\n",
        "        for frame in frames:\n",
        "            result.append(self.transform(frame))\n",
        "        return result\n",
        "\n",
        "# Crear datasets con augmentación para fine-tuning\n",
        "train_dataset = ViolenceVideoDataset(\n",
        "    root_dir=CONFIG[\"dataset_path\"],\n",
        "    split='train',\n",
        "    num_frames=CONFIG[\"num_frames\"],\n",
        "    image_size=CONFIG[\"image_size\"],\n",
        "    transform=VideoAugmentor(strength=0.5)  # Augmentación más fuerte para fine-tuning\n",
        ")\n",
        "\n",
        "val_dataset = ViolenceVideoDataset(\n",
        "    root_dir=CONFIG[\"dataset_path\"],\n",
        "    split='val',\n",
        "    num_frames=CONFIG[\"num_frames\"],\n",
        "    image_size=CONFIG[\"image_size\"]\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG[\"ft_batch_size\"],  # Tamaño de batch más pequeño para fine-tuning\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG[\"ft_batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# 3. Configurar optimizador con learning rate diferenciado y weight decay discriminativo\n",
        "# Agrupar parámetros con diferentes LRs y weight decay\n",
        "param_groups = [\n",
        "    # Capas base (backbone) - LR más bajo, WD más alto\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters()\n",
        "                  if 'timesformer.encoder.layer' in n and int(n.split('.')[3]) < 8],\n",
        "        'lr': CONFIG[\"ft_learning_rate\"] * 0.05,\n",
        "        'weight_decay': CONFIG[\"ft_weight_decay\"] * 2.0\n",
        "    },\n",
        "    # Capas intermedias - LR medio, WD estándar\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters()\n",
        "                  if 'timesformer.encoder.layer' in n and int(n.split('.')[3]) >= 8],\n",
        "        'lr': CONFIG[\"ft_learning_rate\"] * 0.1,\n",
        "        'weight_decay': CONFIG[\"ft_weight_decay\"]\n",
        "    },\n",
        "    # Embeddings de tiempo - LR más alto, WD bajo\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters() if 'time_embeddings' in n],\n",
        "        'lr': CONFIG[\"ft_learning_rate\"] * 0.5,\n",
        "        'weight_decay': CONFIG[\"ft_weight_decay\"] * 0.5\n",
        "    },\n",
        "    # Clasificador - LR más alto, WD estándar\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters() if 'classifier' in n],\n",
        "        'lr': CONFIG[\"ft_learning_rate\"],\n",
        "        'weight_decay': CONFIG[\"ft_weight_decay\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "optimizer = optim.AdamW(param_groups)\n",
        "\n",
        "# 4. Scheduler con warm-up y cosine decay\n",
        "num_training_steps = len(train_dataloader) * CONFIG[\"ft_num_epochs\"]\n",
        "num_warmup_steps = int(num_training_steps * 0.1)  # 10% del total como warm-up\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# 5. Criterio de pérdida con label smoothing\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing para mejor generalización\n",
        "\n",
        "# 6. Early stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_f1):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_f1\n",
        "        elif val_f1 < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_f1\n",
        "            self.counter = 0\n",
        "\n",
        "early_stopping = EarlyStopping(patience=3)\n",
        "\n",
        "# 7. Inicializar tracking de métricas\n",
        "best_val_f1 = 0.0\n",
        "train_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []}\n",
        "val_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []}\n",
        "\n",
        "# Función para guardar todas las métricas y visualizaciones\n",
        "def save_complete_metrics(eval_results, config, phase='fine_tuning'):\n",
        "    \"\"\"Guarda y visualiza todas las métricas requeridas\"\"\"\n",
        "    # 1. Guardar informe detallado como JSON\n",
        "    report = {\n",
        "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        'phase': phase,\n",
        "        'metrics': {\n",
        "            'loss': float(eval_results['loss']),\n",
        "            'accuracy': float(eval_results['accuracy']),\n",
        "            'precision': float(eval_results['precision']),\n",
        "            'recall': float(eval_results['recall']),\n",
        "            'specificity': float(eval_results['specificity']),\n",
        "            'f1_score': float(eval_results['f1']),\n",
        "            'roc_auc': float(eval_results['roc_auc']),\n",
        "            'true_positive_rate': float(eval_results['recall']),  # TPR = Recall\n",
        "            'false_positive_rate': 1 - float(eval_results['specificity']),  # FPR = 1 - Specificity\n",
        "        },\n",
        "        'confusion_matrix': eval_results['confusion_matrix'].tolist(),\n",
        "    }\n",
        "\n",
        "    # Guardar informe en formato JSON\n",
        "    with open(os.path.join(config[\"output_dir\"], f\"evaluation_report_{phase}.json\"), 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "\n",
        "    # 2. Matriz de confusión\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = eval_results['confusion_matrix']\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['No Violencia', 'Violencia'],\n",
        "                yticklabels=['No Violencia', 'Violencia'])\n",
        "    plt.xlabel('Predicción')\n",
        "    plt.ylabel('Real')\n",
        "    plt.title('Matriz de Confusión')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], f\"confusion_matrix_{phase}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Curva ROC\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(eval_results['fpr'], eval_results['tpr'], color='darkorange', lw=2,\n",
        "             label=f'ROC curve (area = {eval_results[\"roc_auc\"]:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], f\"roc_curve_{phase}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Curva Precision-Recall\n",
        "    precision, recall, _ = precision_recall_curve(\n",
        "        eval_results['labels'],\n",
        "        eval_results['predictions']\n",
        "    )\n",
        "    pr_auc = average_precision_score(eval_results['labels'], eval_results['predictions'])\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], f\"precision_recall_curve_{phase}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Gráfico de barras de métricas principales\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall (TPR)', 'Specificity', 'F1-Score', 'ROC AUC']\n",
        "    values = [\n",
        "        report['metrics']['accuracy'],\n",
        "        report['metrics']['precision'],\n",
        "        report['metrics']['recall'],\n",
        "        report['metrics']['specificity'],\n",
        "        report['metrics']['f1_score'],\n",
        "        report['metrics']['roc_auc']\n",
        "    ]\n",
        "\n",
        "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'teal']\n",
        "    plt.bar(metrics, values, color=colors)\n",
        "    plt.ylim([0, 1.05])\n",
        "    plt.ylabel('Valor')\n",
        "    plt.title('Resumen de Métricas de Rendimiento')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Añadir valores en las barras\n",
        "    for i, v in enumerate(values):\n",
        "        plt.text(i, v + 0.02, f\"{v:.4f}\", ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], f\"metrics_summary_{phase}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 6. Crear informe en texto para fácil lectura\n",
        "    with open(os.path.join(config[\"output_dir\"], f\"evaluation_report_{phase}.txt\"), 'w') as f:\n",
        "        f.write(f\"=== REPORTE COMPLETO DE EVALUACIÓN - {phase.upper()} ===\\n\")\n",
        "        f.write(f\"Fecha: {report['timestamp']}\\n\\n\")\n",
        "\n",
        "        f.write(\"=== MÉTRICAS DE RENDIMIENTO ===\\n\")\n",
        "        f.write(f\"• Accuracy: {report['metrics']['accuracy']:.4f}\\n\")\n",
        "        f.write(f\"• Precision: {report['metrics']['precision']:.4f}\\n\")\n",
        "        f.write(f\"• Recall (Sensibilidad / TPR): {report['metrics']['recall']:.4f}\\n\")\n",
        "        f.write(f\"• Specificity: {report['metrics']['specificity']:.4f}\\n\")\n",
        "        f.write(f\"• False Positive Rate (FPR): {report['metrics']['false_positive_rate']:.4f}\\n\")\n",
        "        f.write(f\"• F1-Score: {report['metrics']['f1_score']:.4f}\\n\")\n",
        "        f.write(f\"• ROC AUC: {report['metrics']['roc_auc']:.4f}\\n\")\n",
        "        f.write(f\"• Precision-Recall AUC: {pr_auc:.4f}\\n\\n\")\n",
        "\n",
        "        f.write(\"=== MATRIZ DE CONFUSIÓN ===\\n\")\n",
        "        f.write(\"                   | Pred: No Violencia | Pred: Violencia |\\n\")\n",
        "        f.write(f\"Real: No Violencia | {cm[0][0]:^18} | {cm[0][1]:^15} |\\n\")\n",
        "        f.write(f\"Real: Violencia    | {cm[1][0]:^18} | {cm[1][1]:^15} |\\n\\n\")\n",
        "\n",
        "        f.write(\"=== INTERPRETACIÓN ===\\n\")\n",
        "        acc_quality = \"EXCELENTE\" if report['metrics']['accuracy'] > 0.9 else \"BUENO\" if report['metrics']['accuracy'] > 0.8 else \"REGULAR\"\n",
        "        f1_quality = \"EXCELENTE\" if report['metrics']['f1_score'] > 0.9 else \"BUENO\" if report['metrics']['f1_score'] > 0.8 else \"REGULAR\"\n",
        "\n",
        "        f.write(f\"• Calidad del modelo (Accuracy): {acc_quality}\\n\")\n",
        "        f.write(f\"• Calidad del modelo (F1-Score): {f1_quality}\\n\")\n",
        "        f.write(f\"• Equilibrio Precision-Recall: {min(report['metrics']['precision'], report['metrics']['recall'])/max(report['metrics']['precision'], report['metrics']['recall']):.2f}\\n\")\n",
        "\n",
        "        # Análisis de errores\n",
        "        if cm[0][1] > cm[1][0]:\n",
        "            f.write(\"• El modelo tiende a generar más falsos positivos (clasificar incorrectamente como violencia)\\n\")\n",
        "        elif cm[0][1] < cm[1][0]:\n",
        "            f.write(\"• El modelo tiende a generar más falsos negativos (no detectar violencia real)\\n\")\n",
        "        else:\n",
        "            f.write(\"• El modelo es equilibrado en sus errores\\n\")\n",
        "\n",
        "    return report\n",
        "\n",
        "# 8. Entrenamiento por épocas\n",
        "for epoch in range(CONFIG[\"ft_num_epochs\"]):\n",
        "    logger.info(f\"Iniciando época {epoch+1}/{CONFIG['ft_num_epochs']} (fine-tuning)\")\n",
        "    print(f\"Iniciando época {epoch+1}/{CONFIG['ft_num_epochs']} (fine-tuning)\")\n",
        "\n",
        "    # Entrenamiento\n",
        "    train_results = train_epoch(\n",
        "        model=model,\n",
        "        dataloader=train_dataloader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        criterion=criterion,\n",
        "        device=device,\n",
        "        epoch=epoch,\n",
        "        config=CONFIG\n",
        "    )\n",
        "\n",
        "    # Evaluación\n",
        "    eval_results = evaluate(\n",
        "        model=model,\n",
        "        dataloader=val_dataloader,\n",
        "        criterion=criterion,\n",
        "        device=device,\n",
        "        config=CONFIG\n",
        "    )\n",
        "\n",
        "    # Registrar métricas\n",
        "    for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1', 'specificity']:\n",
        "        if metric in train_results:\n",
        "            train_metrics[metric].append(train_results[metric])\n",
        "        if metric in eval_results:\n",
        "            val_metrics[metric].append(eval_results[metric])\n",
        "\n",
        "    # Mostrar resultados\n",
        "    logger.info(f\"Epoch {epoch+1}/{CONFIG['ft_num_epochs']} (FT) - \"\n",
        "               f\"Train Loss: {train_results['loss']:.4f}, \"\n",
        "               f\"Val Loss: {eval_results['loss']:.4f}, \"\n",
        "               f\"Train Acc: {train_results['accuracy']:.4f}, \"\n",
        "               f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n",
        "               f\"Val F1: {eval_results['f1']:.4f}, \"\n",
        "               f\"Val Specificity: {eval_results['specificity']:.4f}\")\n",
        "    print(f\"Epoch {epoch+1}/{CONFIG['ft_num_epochs']} (FT) - \"\n",
        "          f\"Train Loss: {train_results['loss']:.4f}, \"\n",
        "          f\"Val Loss: {eval_results['loss']:.4f}, \"\n",
        "          f\"Train Acc: {train_results['accuracy']:.4f}, \"\n",
        "          f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n",
        "          f\"Val F1: {eval_results['f1']:.4f}, \"\n",
        "          f\"Val Specificity: {eval_results['specificity']:.4f}\")\n",
        "\n",
        "    # Guardar mejor modelo\n",
        "    if eval_results['f1'] > best_val_f1:\n",
        "        best_val_f1 = eval_results['f1']\n",
        "\n",
        "        # Guardar modelo\n",
        "        model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "            'val_f1': best_val_f1,\n",
        "            'config': CONFIG,\n",
        "        }, model_path)\n",
        "\n",
        "        logger.info(f\"Guardado mejor modelo (FT) con F1: {best_val_f1:.4f} en {model_path}\")\n",
        "        print(f\"Guardado mejor modelo (FT) con F1: {best_val_f1:.4f} en {model_path}\")\n",
        "\n",
        "    # Guardar checkpoint al final de cada época\n",
        "    checkpoint_path = os.path.join(CONFIG[\"output_dir\"], f\"checkpoint_ft_epoch{epoch+1}.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'train_metrics': train_metrics,\n",
        "        'val_metrics': val_metrics,\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    logger.info(f\"Guardado checkpoint de fine-tuning época {epoch+1} en {checkpoint_path}\")\n",
        "    print(f\"Guardado checkpoint de fine-tuning época {epoch+1} en {checkpoint_path}\")\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping(eval_results['f1'])\n",
        "    if early_stopping.early_stop:\n",
        "        logger.info(f\"Early stopping activado en época {epoch+1}\")\n",
        "        print(f\"Early stopping activado en época {epoch+1}\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8fcbf0cf6f1d49548bb0c0081502d49f",
            "80ab0f773c4a4e3cadd2aeec9d5de9b0",
            "880f5dfce09047df8dc7c798accc166e",
            "1abdbe734d9f4ff68b0622e8d7fc2e38",
            "8eb9535ab8bd4c1fba6b784a79dc7965",
            "36d4caa942934ce892267b17c9677632",
            "6237cca58e51426da5c043262277893c",
            "e62617bd7b194d7da07032b04fc69ed8",
            "2b5ff5cf648d4503bfd005fd75b0bd98",
            "cbfb1e3fd58e4c79a28df0a968c74132",
            "53627f45829541208ec35da5085c9793",
            "d44944a1b684496781c9e2def4dd8347",
            "64333764a96141c980d304f0f4b6009b",
            "7c2b06ceb55b46ed9c1e444060a7106a",
            "c8d2e3fa7c9645579ef66da0fc30c4ac",
            "7ee1c6b3cdf842e0a6e196d7bec9c6e4",
            "4e1171c0b43d4bd1ba05f88cc7ee512c",
            "03c6b59427064dd7aff79ac299f0154c",
            "41fb33a7b9c2498197d830bff6c2fc2b",
            "ce5950fa3cdf44b5a6f6461e1c20de3d",
            "26194c7196434dd690fc5f3c366f8cdf",
            "b26e93238a2642428215534fd7c58aa1"
          ]
        },
        "id": "7p79LAiweFyx",
        "outputId": "5162c132-b153-4378-ce1d-17a8edd53ca7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iniciando fase de Fine-Tuning\n",
            "Creando y cargando modelo desde checkpoint\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8fcbf0cf6f1d49548bb0c0081502d49f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/22.7k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Parámetros entrenables: 121,260,290 / 121,260,290 (100.00%)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d44944a1b684496781c9e2def4dd8347",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "preprocessor_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargados 8000 videos para split 'train'\n",
            "Violencia: 4000, No Violencia: 4000\n",
            "Cargados 1500 videos para split 'val'\n",
            "Violencia: 750, No Violencia: 750\n",
            "Iniciando época 1/5 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 1:  20%|█▉        | 199/1000 [04:34<17:22,  1.30s/it, loss=0.01, acc=1, prec=1, rec=1, f1=1]           ERROR:__main__:Error en paso 199, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1:  40%|███▉      | 399/1000 [08:15<12:05,  1.21s/it, loss=0.0917, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1:  60%|█████▉    | 599/1000 [12:03<07:56,  1.19s/it, loss=0.116, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1:  80%|███████▉  | 799/1000 [15:51<04:23,  1.31s/it, loss=0.0251, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 799, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1: 100%|█████████▉| 999/1000 [19:38<00:01,  1.68s/it, loss=0.0286, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 999, época 1: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 1: 100%|██████████| 1000/1000 [19:38<00:00,  1.18s/it, loss=0.0286, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [05:38<00:00,  1.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/5 (FT) - Train Loss: 0.1218, Val Loss: 0.0993, Train Acc: 0.9499, Val Acc: 0.9280, Val F1: 0.9227, Val Specificity: 0.9960\n",
            "Guardado mejor modelo (FT) con F1: 0.9227 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft2.pt\n",
            "Guardado checkpoint de fine-tuning época 1 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch1_2.pt\n",
            "Iniciando época 2/5 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 2:  20%|█▉        | 199/1000 [02:22<09:14,  1.45it/s, loss=0.326, acc=0.75, prec=0.8, rec=0.8, f1=0.8] ERROR:__main__:Error en paso 199, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2:  40%|███▉      | 399/1000 [04:41<06:55,  1.45it/s, loss=0.0251, acc=0.875, prec=0.833, rec=1, f1=0.909]ERROR:__main__:Error en paso 399, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2:  60%|█████▉    | 599/1000 [06:59<04:37,  1.44it/s, loss=0.0831, acc=0.875, prec=0.75, rec=1, f1=0.857]ERROR:__main__:Error en paso 599, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2:  80%|███████▉  | 799/1000 [09:18<02:19,  1.44it/s, loss=0.0889, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 799, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2: 100%|█████████▉| 999/1000 [11:36<00:00,  1.45it/s, loss=0.00203, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 999, época 2: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 2: 100%|██████████| 1000/1000 [11:36<00:00,  1.44it/s, loss=0.00203, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [00:55<00:00,  3.39it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/5 (FT) - Train Loss: 0.0822, Val Loss: 0.0747, Train Acc: 0.9681, Val Acc: 0.9573, Val F1: 0.9558, Val Specificity: 0.9920\n",
            "Guardado mejor modelo (FT) con F1: 0.9558 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft2.pt\n",
            "Guardado checkpoint de fine-tuning época 2 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch2_2.pt\n",
            "Iniciando época 3/5 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 3:  20%|█▉        | 199/1000 [02:48<09:14,  1.45it/s, loss=0.0127, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 199, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3:  40%|███▉      | 399/1000 [05:07<06:55,  1.45it/s, loss=0.0138, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 399, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3:  60%|█████▉    | 599/1000 [07:25<04:37,  1.45it/s, loss=0.00139, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3:  80%|███████▉  | 799/1000 [09:44<02:19,  1.44it/s, loss=0.0688, acc=0.875, prec=0.833, rec=1, f1=0.909]ERROR:__main__:Error en paso 799, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3: 100%|█████████▉| 999/1000 [12:02<00:00,  1.45it/s, loss=0.0015, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 999, época 3: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 3: 100%|██████████| 1000/1000 [12:02<00:00,  1.38it/s, loss=0.0015, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [00:55<00:00,  3.38it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/5 (FT) - Train Loss: 0.0578, Val Loss: 0.0650, Train Acc: 0.9775, Val Acc: 0.9600, Val F1: 0.9586, Val Specificity: 0.9933\n",
            "Guardado mejor modelo (FT) con F1: 0.9586 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft2.pt\n",
            "Guardado checkpoint de fine-tuning época 3 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch3_2.pt\n",
            "Iniciando época 4/5 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 4:  20%|█▉        | 199/1000 [02:47<09:14,  1.45it/s, loss=0.000395, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 199, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4:  40%|███▉      | 399/1000 [05:06<06:55,  1.45it/s, loss=0.00713, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4:  60%|█████▉    | 599/1000 [07:24<04:37,  1.45it/s, loss=0.00531, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4:  80%|███████▉  | 799/1000 [09:43<02:19,  1.44it/s, loss=0.00629, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 799, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4: 100%|█████████▉| 999/1000 [12:01<00:00,  1.45it/s, loss=0.0112, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 999, época 4: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 4: 100%|██████████| 1000/1000 [12:01<00:00,  1.39it/s, loss=0.0112, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [00:55<00:00,  3.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/5 (FT) - Train Loss: 0.0446, Val Loss: 0.0601, Train Acc: 0.9832, Val Acc: 0.9620, Val F1: 0.9608, Val Specificity: 0.9933\n",
            "Guardado mejor modelo (FT) con F1: 0.9608 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft2.pt\n",
            "Guardado checkpoint de fine-tuning época 4 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch4_2.pt\n",
            "Iniciando época 5/5 (fine-tuning)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Época 5:  20%|█▉        | 199/1000 [02:49<09:14,  1.45it/s, loss=0.0221, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 199, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5:  40%|███▉      | 399/1000 [05:07<06:55,  1.45it/s, loss=0.00234, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5:  60%|█████▉    | 599/1000 [07:26<04:38,  1.44it/s, loss=0.00742, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5:  80%|███████▉  | 799/1000 [09:44<02:19,  1.44it/s, loss=0.000511, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 799, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5: 100%|█████████▉| 999/1000 [12:03<00:00,  1.45it/s, loss=0.0033, acc=1, prec=1, rec=1, f1=1]         ERROR:__main__:Error en paso 999, época 5: cannot access local variable 'loss' where it is not associated with a value\n",
            "Época 5: 100%|██████████| 1000/1000 [12:03<00:00,  1.38it/s, loss=0.0033, acc=1, prec=1, rec=1, f1=1]\n",
            "Evaluando: 100%|██████████| 188/188 [00:55<00:00,  3.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/5 (FT) - Train Loss: 0.0399, Val Loss: 0.0599, Train Acc: 0.9854, Val Acc: 0.9613, Val F1: 0.9601, Val Specificity: 0.9933\n",
            "Guardado checkpoint de fine-tuning época 5 en /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/checkpoint_ft_epoch5_2.pt\n"
          ]
        }
      ],
      "source": [
        "# ============================== FINE TUNING ==============================\n",
        "\n",
        "logger.info(\"Iniciando fase de Fine-Tuning\")\n",
        "print(\"Iniciando fase de Fine-Tuning\")\n",
        "\n",
        "# Cargar el mejor modelo de Transfer Learning\n",
        "best_tl_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_tl.pt\")\n",
        "checkpoint = torch.load(best_tl_model_path)\n",
        "\n",
        "# Comprobar si ya tenemos el modelo cargado (de la celda anterior) o necesitamos cargarlo\n",
        "try:\n",
        "    # Intentar acceder al modelo, si no está definido, lanzará una excepción\n",
        "    model\n",
        "    logger.info(\"Usando modelo ya cargado de celda anterior\")\n",
        "    print(\"Usando modelo ya cargado de celda anterior\")\n",
        "    # Cargar estado del modelo desde checkpoint\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "except NameError:\n",
        "    # Si el modelo no está definido, crear uno nuevo y cargarlo\n",
        "    logger.info(\"Creando y cargando modelo desde checkpoint\")\n",
        "    print(\"Creando y cargando modelo desde checkpoint\")\n",
        "    # model = TimesformerForVideoClassification.from_pretrained(\n",
        "    #     CONFIG[\"pretrained_model\"],\n",
        "    #     num_frames=CONFIG[\"num_frames\"],\n",
        "    #     image_size=CONFIG[\"image_size\"],\n",
        "    #     num_labels=CONFIG[\"num_classes\"],\n",
        "    #     # ignore_mismatched_sizes=True\n",
        "    # )\n",
        "\n",
        "    # Cargar la configuración del modelo pre-entrenado y modificarla para 2 clases\n",
        "    config = TimesformerConfig.from_pretrained(\n",
        "        CONFIG[\"pretrained_model\"],\n",
        "        num_frames=CONFIG[\"num_frames\"],\n",
        "        image_size=CONFIG[\"image_size\"],\n",
        "        num_labels=CONFIG[\"num_classes\"], # Asegura que la config especifique 2 clases\n",
        "    )\n",
        "\n",
        "    # Crear una instancia del modelo *con la configuración de 2 clases*\n",
        "    model = TimesformerForVideoClassification(config)\n",
        "\n",
        "    # Cargar estado del modelo desde checkpoint\n",
        "    # model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Ahora cargar el state_dict del checkpoint.\n",
        "    # Como la arquitectura ya coincide (capa de 2 clases), strict=True debería funcionar.\n",
        "    # El checkpoint contiene el estado COMPLETO del modelo después del TL.\n",
        "    model.load_state_dict(checkpoint['model_state_dict'], strict=True)\n",
        "\n",
        "# 1. Descongelar todos los parámetros del modelo\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# Verificar parámetros entrenables\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "logger.info(f\"Parámetros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "print(f\"Parámetros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
        "\n",
        "# Asegurar que el modelo está en el dispositivo correcto\n",
        "model.to(device)\n",
        "\n",
        "# 2. Preparar datasets y dataloaders con augmentación adicional para fine-tuning\n",
        "# Definir transformaciones para data augmentation más agresivas\n",
        "class VideoAugmentor:\n",
        "    \"\"\"Aplica augmentación a nivel de frame para videos\"\"\"\n",
        "    def __init__(self, strength=0.3):\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.ColorJitter(brightness=0.2*strength,\n",
        "                                  contrast=0.2*strength,\n",
        "                                  saturation=0.2*strength,\n",
        "                                  hue=0.1*strength),\n",
        "            transforms.RandomAffine(degrees=5*strength,\n",
        "                                   translate=(0.1*strength, 0.1*strength),\n",
        "                                   scale=(1-0.1*strength, 1+0.1*strength)),\n",
        "            transforms.RandomHorizontalFlip(p=0.5)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, frames):\n",
        "        \"\"\"Aplica la misma transformación a todos los frames del video\"\"\"\n",
        "        # Obtener una transformación aleatoria (para consistencia entre frames)\n",
        "        result = []\n",
        "        for frame in frames:\n",
        "            result.append(self.transform(frame))\n",
        "        return result\n",
        "\n",
        "# Crear datasets con augmentación para fine-tuning\n",
        "train_dataset = ViolenceVideoDataset(\n",
        "    root_dir=CONFIG[\"dataset_path\"],\n",
        "    split='train',\n",
        "    num_frames=CONFIG[\"num_frames\"],\n",
        "    image_size=CONFIG[\"image_size\"],\n",
        "    transform=VideoAugmentor(strength=0.5)  # Augmentación más fuerte para fine-tuning\n",
        ")\n",
        "\n",
        "val_dataset = ViolenceVideoDataset(\n",
        "    root_dir=CONFIG[\"dataset_path\"],\n",
        "    split='val',\n",
        "    num_frames=CONFIG[\"num_frames\"],\n",
        "    image_size=CONFIG[\"image_size\"]\n",
        ")\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=CONFIG[\"ft_batch_size\"],  # Tamaño de batch más pequeño para fine-tuning\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "val_dataloader = DataLoader(\n",
        "    val_dataset,\n",
        "    batch_size=CONFIG[\"ft_batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# 3. Configurar optimizador con learning rate diferenciado y weight decay discriminativo\n",
        "# Agrupar parámetros con diferentes LRs y weight decay\n",
        "param_groups = [\n",
        "    # Capas base (backbone) - LR más bajo, WD más alto\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters()\n",
        "                  if 'timesformer.encoder.layer' in n and int(n.split('.')[3]) < 8],\n",
        "        'lr': CONFIG[\"ft_learning_rate\"] * 0.05,\n",
        "        'weight_decay': CONFIG[\"ft_weight_decay\"] * 2.0\n",
        "    },\n",
        "    # Capas intermedias - LR medio, WD estándar\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters()\n",
        "                  if 'timesformer.encoder.layer' in n and int(n.split('.')[3]) >= 8],\n",
        "        'lr': CONFIG[\"ft_learning_rate\"] * 0.1,\n",
        "        'weight_decay': CONFIG[\"ft_weight_decay\"]\n",
        "    },\n",
        "    # Embeddings de tiempo - LR más alto, WD bajo\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters() if 'time_embeddings' in n],\n",
        "        'lr': CONFIG[\"ft_learning_rate\"] * 0.5,\n",
        "        'weight_decay': CONFIG[\"ft_weight_decay\"] * 0.5\n",
        "    },\n",
        "    # Clasificador - LR más alto, WD estándar\n",
        "    {\n",
        "        'params': [p for n, p in model.named_parameters() if 'classifier' in n],\n",
        "        'lr': CONFIG[\"ft_learning_rate\"],\n",
        "        'weight_decay': CONFIG[\"ft_weight_decay\"]\n",
        "    }\n",
        "]\n",
        "\n",
        "optimizer = optim.AdamW(param_groups)\n",
        "\n",
        "# 4. Scheduler con warm-up y cosine decay\n",
        "num_training_steps = len(train_dataloader) * CONFIG[\"ft_num_epochs\"]\n",
        "num_warmup_steps = int(num_training_steps * 0.1)  # 10% del total como warm-up\n",
        "\n",
        "scheduler = get_cosine_schedule_with_warmup(\n",
        "    optimizer,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    num_training_steps=num_training_steps\n",
        ")\n",
        "\n",
        "# 5. Criterio de pérdida con label smoothing\n",
        "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)  # Label smoothing para mejor generalización\n",
        "\n",
        "# 6. Early stopping\n",
        "class EarlyStopping:\n",
        "    def __init__(self, patience=3, min_delta=0.001):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.counter = 0\n",
        "        self.best_score = None\n",
        "        self.early_stop = False\n",
        "\n",
        "    def __call__(self, val_f1):\n",
        "        if self.best_score is None:\n",
        "            self.best_score = val_f1\n",
        "        elif val_f1 < self.best_score + self.min_delta:\n",
        "            self.counter += 1\n",
        "            if self.counter >= self.patience:\n",
        "                self.early_stop = True\n",
        "        else:\n",
        "            self.best_score = val_f1\n",
        "            self.counter = 0\n",
        "\n",
        "early_stopping = EarlyStopping(patience=3)\n",
        "\n",
        "# 7. Inicializar tracking de métricas\n",
        "best_val_f1 = 0.0\n",
        "train_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []}\n",
        "val_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': [], 'specificity': []}\n",
        "\n",
        "# Función para guardar todas las métricas y visualizaciones\n",
        "def save_complete_metrics(eval_results, config, phase='fine_tuning'):\n",
        "    \"\"\"Guarda y visualiza todas las métricas requeridas\"\"\"\n",
        "    # 1. Guardar informe detallado como JSON\n",
        "    report = {\n",
        "        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "        'phase': phase,\n",
        "        'metrics': {\n",
        "            'loss': float(eval_results['loss']),\n",
        "            'accuracy': float(eval_results['accuracy']),\n",
        "            'precision': float(eval_results['precision']),\n",
        "            'recall': float(eval_results['recall']),\n",
        "            'specificity': float(eval_results['specificity']),\n",
        "            'f1_score': float(eval_results['f1']),\n",
        "            'roc_auc': float(eval_results['roc_auc']),\n",
        "            'true_positive_rate': float(eval_results['recall']),  # TPR = Recall\n",
        "            'false_positive_rate': 1 - float(eval_results['specificity']),  # FPR = 1 - Specificity\n",
        "        },\n",
        "        'confusion_matrix': eval_results['confusion_matrix'].tolist(),\n",
        "    }\n",
        "\n",
        "    # Guardar informe en formato JSON\n",
        "    with open(os.path.join(config[\"output_dir\"], f\"evaluation_report_{phase}.json\"), 'w') as f:\n",
        "        json.dump(report, f, indent=4)\n",
        "\n",
        "    # 2. Matriz de confusión\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    cm = eval_results['confusion_matrix']\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['No Violencia', 'Violencia'],\n",
        "                yticklabels=['No Violencia', 'Violencia'])\n",
        "    plt.xlabel('Predicción')\n",
        "    plt.ylabel('Real')\n",
        "    plt.title('Matriz de Confusión')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], f\"confusion_matrix_{phase}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 3. Curva ROC\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(eval_results['fpr'], eval_results['tpr'], color='darkorange', lw=2,\n",
        "             label=f'ROC curve (area = {eval_results[\"roc_auc\"]:.2f})')\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC)')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], f\"roc_curve_{phase}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 4. Curva Precision-Recall\n",
        "    precision, recall, _ = precision_recall_curve(\n",
        "        eval_results['labels'],\n",
        "        eval_results['predictions']\n",
        "    )\n",
        "    pr_auc = average_precision_score(eval_results['labels'], eval_results['predictions'])\n",
        "\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')\n",
        "    plt.xlabel('Recall')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title('Precision-Recall Curve')\n",
        "    plt.legend(loc=\"lower left\")\n",
        "    plt.grid(True)\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], f\"precision_recall_curve_{phase}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 5. Gráfico de barras de métricas principales\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    metrics = ['Accuracy', 'Precision', 'Recall (TPR)', 'Specificity', 'F1-Score', 'ROC AUC']\n",
        "    values = [\n",
        "        report['metrics']['accuracy'],\n",
        "        report['metrics']['precision'],\n",
        "        report['metrics']['recall'],\n",
        "        report['metrics']['specificity'],\n",
        "        report['metrics']['f1_score'],\n",
        "        report['metrics']['roc_auc']\n",
        "    ]\n",
        "\n",
        "    colors = ['blue', 'green', 'red', 'purple', 'orange', 'teal']\n",
        "    plt.bar(metrics, values, color=colors)\n",
        "    plt.ylim([0, 1.05])\n",
        "    plt.ylabel('Valor')\n",
        "    plt.title('Resumen de Métricas de Rendimiento')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "    # Añadir valores en las barras\n",
        "    for i, v in enumerate(values):\n",
        "        plt.text(i, v + 0.02, f\"{v:.4f}\", ha='center')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join(config[\"output_dir\"], f\"metrics_summary_{phase}.png\"))\n",
        "    plt.close()\n",
        "\n",
        "    # 6. Crear informe en texto para fácil lectura\n",
        "    with open(os.path.join(config[\"output_dir\"], f\"evaluation_report_{phase}.txt\"), 'w') as f:\n",
        "        f.write(f\"=== REPORTE COMPLETO DE EVALUACIÓN - {phase.upper()} ===\\n\")\n",
        "        f.write(f\"Fecha: {report['timestamp']}\\n\\n\")\n",
        "\n",
        "        f.write(\"=== MÉTRICAS DE RENDIMIENTO ===\\n\")\n",
        "        f.write(f\"• Accuracy: {report['metrics']['accuracy']:.4f}\\n\")\n",
        "        f.write(f\"• Precision: {report['metrics']['precision']:.4f}\\n\")\n",
        "        f.write(f\"• Recall (Sensibilidad / TPR): {report['metrics']['recall']:.4f}\\n\")\n",
        "        f.write(f\"• Specificity: {report['metrics']['specificity']:.4f}\\n\")\n",
        "        f.write(f\"• False Positive Rate (FPR): {report['metrics']['false_positive_rate']:.4f}\\n\")\n",
        "        f.write(f\"• F1-Score: {report['metrics']['f1_score']:.4f}\\n\")\n",
        "        f.write(f\"• ROC AUC: {report['metrics']['roc_auc']:.4f}\\n\")\n",
        "        f.write(f\"• Precision-Recall AUC: {pr_auc:.4f}\\n\\n\")\n",
        "\n",
        "        f.write(\"=== MATRIZ DE CONFUSIÓN ===\\n\")\n",
        "        f.write(\"                   | Pred: No Violencia | Pred: Violencia |\\n\")\n",
        "        f.write(f\"Real: No Violencia | {cm[0][0]:^18} | {cm[0][1]:^15} |\\n\")\n",
        "        f.write(f\"Real: Violencia    | {cm[1][0]:^18} | {cm[1][1]:^15} |\\n\\n\")\n",
        "\n",
        "        f.write(\"=== INTERPRETACIÓN ===\\n\")\n",
        "        acc_quality = \"EXCELENTE\" if report['metrics']['accuracy'] > 0.9 else \"BUENO\" if report['metrics']['accuracy'] > 0.8 else \"REGULAR\"\n",
        "        f1_quality = \"EXCELENTE\" if report['metrics']['f1_score'] > 0.9 else \"BUENO\" if report['metrics']['f1_score'] > 0.8 else \"REGULAR\"\n",
        "\n",
        "        f.write(f\"• Calidad del modelo (Accuracy): {acc_quality}\\n\")\n",
        "        f.write(f\"• Calidad del modelo (F1-Score): {f1_quality}\\n\")\n",
        "        f.write(f\"• Equilibrio Precision-Recall: {min(report['metrics']['precision'], report['metrics']['recall'])/max(report['metrics']['precision'], report['metrics']['recall']):.2f}\\n\")\n",
        "\n",
        "        # Análisis de errores\n",
        "        if cm[0][1] > cm[1][0]:\n",
        "            f.write(\"• El modelo tiende a generar más falsos positivos (clasificar incorrectamente como violencia)\\n\")\n",
        "        elif cm[0][1] < cm[1][0]:\n",
        "            f.write(\"• El modelo tiende a generar más falsos negativos (no detectar violencia real)\\n\")\n",
        "        else:\n",
        "            f.write(\"• El modelo es equilibrado en sus errores\\n\")\n",
        "\n",
        "    return report\n",
        "\n",
        "# 8. Entrenamiento por épocas\n",
        "for epoch in range(CONFIG[\"ft_num_epochs\"]):\n",
        "    logger.info(f\"Iniciando época {epoch+1}/{CONFIG['ft_num_epochs']} (fine-tuning)\")\n",
        "    print(f\"Iniciando época {epoch+1}/{CONFIG['ft_num_epochs']} (fine-tuning)\")\n",
        "\n",
        "    # Entrenamiento\n",
        "    train_results = train_epoch(\n",
        "        model=model,\n",
        "        dataloader=train_dataloader,\n",
        "        optimizer=optimizer,\n",
        "        scheduler=scheduler,\n",
        "        criterion=criterion,\n",
        "        device=device,\n",
        "        epoch=epoch,\n",
        "        config=CONFIG\n",
        "    )\n",
        "\n",
        "    # Evaluación\n",
        "    eval_results = evaluate(\n",
        "        model=model,\n",
        "        dataloader=val_dataloader,\n",
        "        criterion=criterion,\n",
        "        device=device,\n",
        "        config=CONFIG\n",
        "    )\n",
        "\n",
        "    # Registrar métricas\n",
        "    for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1', 'specificity']:\n",
        "        if metric in train_results:\n",
        "            train_metrics[metric].append(train_results[metric])\n",
        "        if metric in eval_results:\n",
        "            val_metrics[metric].append(eval_results[metric])\n",
        "\n",
        "    # Mostrar resultados\n",
        "    logger.info(f\"Epoch {epoch+1}/{CONFIG['ft_num_epochs']} (FT) - \"\n",
        "               f\"Train Loss: {train_results['loss']:.4f}, \"\n",
        "               f\"Val Loss: {eval_results['loss']:.4f}, \"\n",
        "               f\"Train Acc: {train_results['accuracy']:.4f}, \"\n",
        "               f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n",
        "               f\"Val F1: {eval_results['f1']:.4f}, \"\n",
        "               f\"Val Specificity: {eval_results['specificity']:.4f}\")\n",
        "    print(f\"Epoch {epoch+1}/{CONFIG['ft_num_epochs']} (FT) - \"\n",
        "          f\"Train Loss: {train_results['loss']:.4f}, \"\n",
        "          f\"Val Loss: {eval_results['loss']:.4f}, \"\n",
        "          f\"Train Acc: {train_results['accuracy']:.4f}, \"\n",
        "          f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n",
        "          f\"Val F1: {eval_results['f1']:.4f}, \"\n",
        "          f\"Val Specificity: {eval_results['specificity']:.4f}\")\n",
        "\n",
        "    # Guardar mejor modelo\n",
        "    if eval_results['f1'] > best_val_f1:\n",
        "        best_val_f1 = eval_results['f1']\n",
        "\n",
        "        # Guardar modelo\n",
        "        model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft2.pt\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "            'val_f1': best_val_f1,\n",
        "            'config': CONFIG,\n",
        "        }, model_path)\n",
        "\n",
        "        logger.info(f\"Guardado mejor modelo (FT) con F1: {best_val_f1:.4f} en {model_path}\")\n",
        "        print(f\"Guardado mejor modelo (FT) con F1: {best_val_f1:.4f} en {model_path}\")\n",
        "\n",
        "    # Guardar checkpoint al final de cada época\n",
        "    checkpoint_path = os.path.join(CONFIG[\"output_dir\"], f\"checkpoint_ft_epoch{epoch+1}_2.pt\")\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
        "        'train_metrics': train_metrics,\n",
        "        'val_metrics': val_metrics,\n",
        "    }, checkpoint_path)\n",
        "\n",
        "    logger.info(f\"Guardado checkpoint de fine-tuning época {epoch+1} en {checkpoint_path}\")\n",
        "    print(f\"Guardado checkpoint de fine-tuning época {epoch+1} en {checkpoint_path}\")\n",
        "\n",
        "    # Early stopping\n",
        "    early_stopping(eval_results['f1'])\n",
        "    if early_stopping.early_stop:\n",
        "        logger.info(f\"Early stopping activado en época {epoch+1}\")\n",
        "        print(f\"Early stopping activado en época {epoch+1}\")\n",
        "        break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWLLzso1vExh",
        "outputId": "f533f7d0-3f4d-4bf1-dba0-a2608a42d56d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargando el mejor modelo desde /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft2.pt\n",
            "Evaluando mejor modelo de Fine-Tuning (F1: 0.9608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluando: 100%|██████████| 188/188 [00:55<00:00,  3.37it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargados 800 videos para split 'test'\n",
            "Violencia: 400, No Violencia: 400\n",
            "Evaluando en conjunto de prueba para validar generalización\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluando: 100%|██████████| 100/100 [02:00<00:00,  1.20s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Completada fase de Fine-Tuning\n"
          ]
        }
      ],
      "source": [
        "# 9. Visualizar métricas\n",
        "plt.figure(figsize=(20, 15))\n",
        "\n",
        "metrics_to_plot = ['loss', 'accuracy', 'precision', 'recall', 'f1', 'specificity']\n",
        "# Usar la longitud de val_metrics, ya que evaluate siempre calcula todas las métricas\n",
        "# y val_metrics tendrá la longitud correcta en caso de Early Stopping.\n",
        "epochs = range(1, len(val_metrics['loss']) + 1)\n",
        "\n",
        "for i, metric in enumerate(metrics_to_plot):\n",
        "    plt.subplot(3, 2, i+1)\n",
        "    # Graficar métrica de entrenamiento SOLO SI existe en train_metrics y no está vacía\n",
        "    if metric in train_metrics and train_metrics[metric]:\n",
        "        # Asegurarse de tomar solo los datos hasta el número de épocas graficadas\n",
        "        plt.plot(epochs, train_metrics[metric][:len(epochs)], 'b-', label=f'Training {metric}')\n",
        "    # Graficar métrica de validación SIEMPRE que exista (evaluate siempre las devuelve)\n",
        "    if metric in val_metrics and val_metrics[metric]:\n",
        "        # Asegurarse de tomar solo los datos hasta el número de épocas graficadas\n",
        "        plt.plot(epochs, val_metrics[metric][:len(epochs)], 'r-', label=f'Validation {metric}')\n",
        "    else:\n",
        "        # Si por alguna razón (inesperada) tampoco hay datos de validación, saltar este gráfico\n",
        "        continue\n",
        "\n",
        "    plt.title(f'{metric.capitalize()} vs. Epochs (Fine-Tuning)')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(metric.capitalize())\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"fine_tuning_metrics.png\"))\n",
        "plt.close()\n",
        "\n",
        "# 10. Evaluación final del mejor modelo\n",
        "# Cargar el mejor modelo\n",
        "# Comprobar si existe el modelo 'best_ft2.pt' y si no, usar 'best_ft.pt'\n",
        "best_model_path_ft2 = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft2.pt\")\n",
        "best_model_path_ft = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n",
        "\n",
        "if os.path.exists(best_model_path_ft2):\n",
        "    best_model_path = best_model_path_ft2\n",
        "    print(f\"Cargando el mejor modelo desde {best_model_path_ft2}\")\n",
        "    logger.info(f\"Cargando el mejor modelo desde {best_model_path_ft2}\")\n",
        "elif os.path.exists(best_model_path_ft):\n",
        "    best_model_path = best_model_path_ft\n",
        "    print(f\"Cargando el mejor modelo desde {best_model_path_ft}\")\n",
        "    logger.info(f\"Cargando el mejor modelo desde {best_model_path_ft}\")\n",
        "else:\n",
        "     raise FileNotFoundError(f\"No se encontró ningún modelo 'best_ft2.pt' o 'best_ft.pt' en {CONFIG['output_dir']}\")\n",
        "\n",
        "\n",
        "checkpoint = torch.load(best_model_path)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "logger.info(f\"Evaluando mejor modelo de Fine-Tuning (F1: {checkpoint['val_f1']:.4f})\")\n",
        "print(f\"Evaluando mejor modelo de Fine-Tuning (F1: {checkpoint['val_f1']:.4f}\")\n",
        "\n",
        "final_eval_results = evaluate(\n",
        "    model=model,\n",
        "    dataloader=val_dataloader,\n",
        "    criterion=criterion,\n",
        "    device=device,\n",
        "    config=CONFIG\n",
        ")\n",
        "\n",
        "# Guardar y visualizar todas las métricas requeridas\n",
        "metrics_report = save_complete_metrics(final_eval_results, CONFIG, phase='fine_tuning')\n",
        "\n",
        "# 11. Evaluar también en el conjunto de prueba para validar generalización\n",
        "test_dataset = ViolenceVideoDataset(\n",
        "    root_dir=CONFIG[\"dataset_path\"],\n",
        "    split='test',\n",
        "    num_frames=CONFIG[\"num_frames\"],\n",
        "    image_size=CONFIG[\"image_size\"]\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG[\"ft_batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "logger.info(\"Evaluando en conjunto de prueba para validar generalización\")\n",
        "print(\"Evaluando en conjunto de prueba para validar generalización\")\n",
        "\n",
        "test_results = evaluate(\n",
        "    model=model,\n",
        "    dataloader=test_dataloader,\n",
        "    criterion=criterion,\n",
        "    device=device,\n",
        "    config=CONFIG\n",
        ")\n",
        "\n",
        "# Guardar y visualizar las métricas en el conjunto de prueba\n",
        "test_metrics_report = save_complete_metrics(test_results, CONFIG, phase='test')\n",
        "\n",
        "# 12. Visualizar algunos ejemplos de predicciones (correctas e incorrectas)\n",
        "def visualize_examples(model, dataloader, config, num_examples=5, phase='test_examples'):\n",
        "    \"\"\"Visualiza ejemplos de predicciones para análisis cualitativo\"\"\"\n",
        "    model.eval()\n",
        "    correct_examples = []\n",
        "    incorrect_examples = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            if len(correct_examples) >= num_examples and len(incorrect_examples) >= num_examples:\n",
        "                break\n",
        "\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "            video_paths = batch['video_path']\n",
        "\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "            for i, (pred, label, video_path) in enumerate(zip(preds, labels, video_paths)):\n",
        "                # Decodificar video para mostrar un frame\n",
        "                try:\n",
        "                    vr = VideoReader(video_path, ctx=cpu(0))\n",
        "                    if len(vr) == 0:\n",
        "                         logger.warning(f\"Skipping visualization for empty video: {video_path}\")\n",
        "                         continue\n",
        "                    mid_frame = vr[len(vr)//2].asnumpy()\n",
        "                except Exception as e:\n",
        "                    logger.error(f\"Error decoding video for visualization {video_path}: {str(e)}\")\n",
        "                    continue # Skip this video if decoding fails\n",
        "\n",
        "\n",
        "                example = {\n",
        "                    'frame': mid_frame,\n",
        "                    'prediction': pred.item(),\n",
        "                    'true_label': label.item(),\n",
        "                    'confidence': probs[i, pred].item(),\n",
        "                    'video_path': video_path\n",
        "                }\n",
        "\n",
        "                if pred.item() == label.item():\n",
        "                    if len(correct_examples) < num_examples:\n",
        "                        correct_examples.append(example)\n",
        "                else:\n",
        "                    if len(incorrect_examples) < num_examples:\n",
        "                        incorrect_examples.append(example)\n",
        "\n",
        "    # Visualizar ejemplos correctos\n",
        "    if correct_examples:\n",
        "        fig, axes = plt.subplots(1, len(correct_examples), figsize=(20, 4))\n",
        "        if len(correct_examples) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, example in enumerate(correct_examples):\n",
        "            axes[i].imshow(example['frame'])\n",
        "            label_text = \"Violencia\" if example['true_label'] == 1 else \"No Violencia\"\n",
        "            axes[i].set_title(f\"Correcto: {label_text}\\nConf: {example['confidence']:.2f}\")\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(config[\"output_dir\"], f\"correct_predictions_{phase}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "    # Visualizar ejemplos incorrectos\n",
        "    if incorrect_examples:\n",
        "        fig, axes = plt.subplots(1, len(incorrect_examples), figsize=(20, 4))\n",
        "        if len(incorrect_examples) == 1:\n",
        "            axes = [axes]\n",
        "\n",
        "        for i, example in enumerate(incorrect_examples):\n",
        "            axes[i].imshow(example['frame'])\n",
        "            true_label = \"Violencia\" if example['true_label'] == 1 else \"No Violencia\"\n",
        "            pred_label = \"Violencia\" if example['prediction'] == 1 else \"No Violencia\"\n",
        "            axes[i].set_title(f\"Error\\nReal: {true_label}\\nPred: {pred_label}\\nConf: {example['confidence']:.2f}\")\n",
        "            axes[i].axis('off')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(os.path.join(config[\"output_dir\"], f\"incorrect_predictions_{phase}.png\"))\n",
        "        plt.close()\n",
        "\n",
        "# Visualizar ejemplos de predicciones\n",
        "visualize_examples(model, test_dataloader, CONFIG)\n",
        "\n",
        "logger.info(\"Completada fase de Fine-Tuning\")\n",
        "print(\"Completada fase de Fine-Tuning\")\n",
        "\n",
        "# Guardar resultados finales\n",
        "ft_results = final_eval_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FiIInTroO9o",
        "outputId": "6286ed3b-d54e-4a49-83e3-398857a94939"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cargados 800 videos para split 'test'\n",
            "Violencia: 400, No Violencia: 400\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluando: 100%|██████████| 200/200 [05:38<00:00,  1.69s/it]\n"
          ]
        }
      ],
      "source": [
        "# ============================== EVALUACIÓN EN CONJUNTO DE PRUEBA ==============================\n",
        "\n",
        "logger.info(\"Evaluando modelo en conjunto de prueba\")\n",
        "\n",
        "# Cargar el mejor modelo de Fine-Tuning\n",
        "best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt2\")\n",
        "\n",
        "# Comprobar si ya tenemos el modelo cargado\n",
        "try:\n",
        "    # Intentar acceder al modelo\n",
        "    model\n",
        "    logger.info(\"Usando modelo ya cargado de celda anterior\")\n",
        "    # Cargar estado del mejor modelo de fine-tuning\n",
        "    checkpoint = torch.load(best_ft_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "except NameError:\n",
        "    # Si el modelo no está definido, crear uno nuevo y cargarlo\n",
        "    logger.info(\"Cargando modelo desde checkpoint\")\n",
        "    model = TimesformerForVideoClassification.from_pretrained(\n",
        "        CONFIG[\"pretrained_model\"],\n",
        "        num_frames=CONFIG[\"num_frames\"],\n",
        "        image_size=CONFIG[\"image_size\"],\n",
        "    )\n",
        "\n",
        "    # Adaptarlo a nuestra tarea\n",
        "    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n",
        "        model.classifier = nn.Sequential(\n",
        "            nn.Dropout(CONFIG[\"tl_dropout\"]),\n",
        "            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n",
        "        )\n",
        "\n",
        "    checkpoint = torch.load(best_ft_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Asegurar que el modelo está en el dispositivo correcto\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Cargar dataset de prueba\n",
        "test_dataset = ViolenceVideoDataset(\n",
        "    root_dir=CONFIG[\"dataset_path\"],\n",
        "    split='test',\n",
        "    num_frames=CONFIG[\"num_frames\"],\n",
        "    image_size=CONFIG[\"image_size\"]\n",
        ")\n",
        "\n",
        "test_dataloader = DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=CONFIG[\"ft_batch_size\"],\n",
        "    shuffle=False,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n",
        "\n",
        "# Evaluar\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "test_results = evaluate(\n",
        "    model=model,\n",
        "    dataloader=test_dataloader,\n",
        "    criterion=criterion,\n",
        "    device=device,\n",
        "    config=CONFIG\n",
        ")\n",
        "\n",
        "# Visualizar matriz de confusión\n",
        "plot_confusion_matrix(test_results['confusion_matrix'], CONFIG, phase='test')\n",
        "\n",
        "# Visualizar curva ROC\n",
        "plot_roc_curve(\n",
        "    test_results['fpr'],\n",
        "    test_results['tpr'],\n",
        "    test_results['roc_auc'],\n",
        "    CONFIG,\n",
        "    phase='test'\n",
        ")\n",
        "\n",
        "# Generar y guardar reporte detallado\n",
        "save_evaluation_report(test_results, CONFIG, phase='test')\n",
        "\n",
        "# Métricas adicionales: Precision-Recall curve\n",
        "precision, recall, _ = precision_recall_curve(\n",
        "    test_results['labels'],\n",
        "    test_results['predictions']\n",
        ")\n",
        "pr_auc = average_precision_score(test_results['labels'], test_results['predictions'])\n",
        "\n",
        "# Graficar curva Precision-Recall\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.legend(loc=\"lower left\")\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"precision_recall_curve_test.png\"))\n",
        "plt.close()\n",
        "\n",
        "# Análisis de mejores umbrales\n",
        "# Calcular métricas para diferentes umbrales\n",
        "thresholds = np.linspace(0.1, 0.9, 9)\n",
        "threshold_metrics = []\n",
        "\n",
        "for threshold in thresholds:\n",
        "    binary_preds = (test_results['predictions'] >= threshold).astype(int)\n",
        "\n",
        "    acc = accuracy_score(test_results['labels'], binary_preds)\n",
        "    prec = precision_score(test_results['labels'], binary_preds, zero_division=0)\n",
        "    rec = recall_score(test_results['labels'], binary_preds, zero_division=0)\n",
        "    f1 = f1_score(test_results['labels'], binary_preds, zero_division=0)\n",
        "\n",
        "    threshold_metrics.append({\n",
        "        'threshold': threshold,\n",
        "        'accuracy': acc,\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1\n",
        "    })\n",
        "\n",
        "# Convertir a DataFrame para mejor visualización\n",
        "threshold_df = pd.DataFrame(threshold_metrics)\n",
        "\n",
        "# Graficar métricas vs umbral\n",
        "plt.figure(figsize=(12, 8))\n",
        "for metric in ['accuracy', 'precision', 'recall', 'f1']:\n",
        "    plt.plot(threshold_df['threshold'], threshold_df[metric], marker='o', label=metric)\n",
        "\n",
        "plt.xlabel('Umbral de decisión')\n",
        "plt.ylabel('Valor de métrica')\n",
        "plt.title('Métricas vs Umbral de decisión')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"threshold_analysis.png\"))\n",
        "plt.close()\n",
        "\n",
        "# Encontrar mejor umbral según F1\n",
        "best_threshold_idx = threshold_df['f1'].idxmax()\n",
        "best_threshold = threshold_df.loc[best_threshold_idx, 'threshold']\n",
        "\n",
        "logger.info(f\"Mejor umbral encontrado: {best_threshold:.2f} con F1: {threshold_df.loc[best_threshold_idx, 'f1']:.4f}\")\n",
        "\n",
        "# Guardar análisis de umbrales\n",
        "threshold_df.to_csv(os.path.join(CONFIG[\"output_dir\"], \"threshold_analysis.csv\"), index=False)\n",
        "\n",
        "# Actualizar el umbral en la configuración\n",
        "CONFIG[\"threshold\"] = float(best_threshold)\n",
        "with open(os.path.join(CONFIG[\"output_dir\"], \"config.json\"), 'w') as f:\n",
        "    json.dump(CONFIG, f, indent=4)\n",
        "\n",
        "# Calcular y visualizar curva ROC detallada con punto óptimo\n",
        "fpr, tpr, thresholds_roc = roc_curve(test_results['labels'], test_results['predictions'])\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "# Calcular distancia al punto óptimo (0,1)\n",
        "distances = np.sqrt((1-tpr)**2 + fpr**2)\n",
        "optimal_idx = np.argmin(distances)\n",
        "optimal_threshold = thresholds_roc[optimal_idx]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red',\n",
        "            label=f'Punto óptimo (umbral={optimal_threshold:.2f})')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('Tasa de Falsos Positivos')\n",
        "plt.ylabel('Tasa de Verdaderos Positivos')\n",
        "plt.title('Curva ROC con punto óptimo')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"roc_curve_optimal_test.png\"))\n",
        "plt.close()\n",
        "\n",
        "logger.info(f\"Umbral óptimo según distancia a punto ideal en ROC: {optimal_threshold:.4f}\")\n",
        "\n",
        "# Guardar este umbral también\n",
        "with open(os.path.join(CONFIG[\"output_dir\"], \"optimal_thresholds.json\"), 'w') as f:\n",
        "    json.dump({\n",
        "        'f1_optimal': float(best_threshold),\n",
        "        'roc_optimal': float(optimal_threshold)\n",
        "    }, f, indent=4)\n",
        "\n",
        "# Mostrar resumen de resultados\n",
        "logger.info(f\"Resumen de evaluación en conjunto de prueba:\")\n",
        "logger.info(f\"Accuracy: {test_results['accuracy']:.4f}\")\n",
        "logger.info(f\"Precision: {test_results['precision']:.4f}\")\n",
        "logger.info(f\"Recall (Sensibilidad): {test_results['recall']:.4f}\")\n",
        "logger.info(f\"Specificity: {test_results['specificity']:.4f}\")\n",
        "logger.info(f\"F1-Score: {test_results['f1']:.4f}\")\n",
        "logger.info(f\"ROC AUC: {test_results['roc_auc']:.4f}\")\n",
        "logger.info(f\"PR AUC: {pr_auc:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_S1s8xBcglYk",
        "outputId": "e91c6c71-57ee-4a00-b221-7c707ccb5711"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m116.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m118.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oE54ic9cCL9k"
      },
      "source": [
        "## EXPORTACIÓN DEL MODELO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaOwAhZXGslt",
        "outputId": "bdfd9954-85c9-4209-abb1-2f0c729ea111"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting onnx\n",
            "  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (5.29.4)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.11/dist-packages (from onnx) (4.13.2)\n",
            "Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m120.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: onnx\n",
            "Successfully installed onnx-1.18.0\n",
            "Collecting onnxruntime\n",
            "  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Collecting coloredlogs (from onnxruntime)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (25.2.10)\n",
            "Requirement already satisfied: numpy>=1.21.6 in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (5.29.4)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime) (1.13.1)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m121.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime\n",
            "Successfully installed coloredlogs-15.0.1 humanfriendly-10.0 onnxruntime-1.22.0\n"
          ]
        }
      ],
      "source": [
        "!pip install onnx\n",
        "!pip install onnxruntime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBnrVST8CJ2b",
        "outputId": "4e33612f-72b2-4ecd-f195-52b742c6c4d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Usando dispositivo: cuda\n",
            "=== EXPORTACIÓN DEL MODELO TIMESFORMER PARA DETECCIÓN DE VIOLENCIA ===\n",
            "Cargando modelo entrenado desde: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/timesformer_violence_detector_best_ft2.pt\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Modelo cargado exitosamente. F1-Score: 0.9607708189951824\n",
            "\n",
            "1. Guardando modelo PyTorch original...\n",
            "✓ Modelo original guardado: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models/timesformer_violence_detector_original.pt\n",
            "  Tamaño: 462.68 MB\n",
            "\n",
            "2. Creando modelo optimizado para inferencia...\n",
            "✓ Modelo optimizado guardado: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models/timesformer_violence_detector_inference.pt\n",
            "  Tamaño: 462.73 MB\n",
            "\n",
            "3. Exportando modelo en precisión FP16...\n",
            "✓ Modelo FP16 guardado: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models/timesformer_violence_detector_fp16.pt\n",
            "  Tamaño: 231.44 MB\n",
            "\n",
            "4. Exportando a TorchScript (opción half precision)...\n",
            "✓ Modelo TorchScript (half) guardado: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models/timesformer_violence_detector_scripted_half.pt\n",
            "  Tamaño: 231.39 MB\n",
            "\n",
            "5. Exportando a TorchScript (precisión original)...\n",
            "✓ Modelo TorchScript (float) guardado: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models/timesformer_violence_detector_scripted_float.pt\n",
            "  Tamaño: 462.68 MB\n",
            "\n",
            "6. Exportando a ONNX (opción half precision)...\n",
            "✓ Modelo ONNX (half) guardado: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models/timesformer_violence_detector_half.onnx\n",
            "  Tamaño: 232.13 MB\n",
            "\n",
            "7. Exportando a ONNX (precisión original)...\n",
            "✓ Modelo ONNX (float) guardado: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models/timesformer_violence_detector_float.onnx\n",
            "  Tamaño: 463.40 MB\n",
            "\n",
            "8. Guardando procesador de imágenes...\n",
            "✓ Procesador guardado en: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models/processor\n",
            "\n",
            "9. Guardando configuración de inferencia...\n",
            "✓ Configuración guardada: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models/inference_config.json\n",
            "\n",
            "10. Creando script de ejemplo para inferencia...\n",
            "✓ Script de ejemplo guardado: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models/inference_example.py\n",
            "\n",
            "=== RESUMEN DE EXPORTACIÓN ===\n",
            "Directorio de modelos exportados: /content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer/exported_models\n",
            "Formatos disponibles:\n",
            "- pytorch_original: timesformer_violence_detector_original.pt (462.68 MB)\n",
            "- pytorch_inference: timesformer_violence_detector_inference.pt (462.73 MB)\n",
            "- pytorch_fp16: timesformer_violence_detector_fp16.pt (231.44 MB)\n",
            "- torchscript_half: timesformer_violence_detector_scripted_half.pt (231.39 MB)\n",
            "- torchscript_float: timesformer_violence_detector_scripted_float.pt (462.68 MB)\n",
            "- onnx_half: timesformer_violence_detector_half.onnx (232.13 MB)\n",
            "- onnx_float: timesformer_violence_detector_float.onnx (463.40 MB)\n",
            "\n",
            "Próximos pasos:\n",
            "1. Selecciona el formato más adecuado para tu despliegue\n",
            "2. Adapta el script de ejemplo (inference_example.py) para tu aplicación web\n",
            "3. Para React: Configura un servidor backend que utilice el modelo o explora ONNX Runtime Web\n"
          ]
        }
      ],
      "source": [
        "# Importar bibliotecas necesarias\n",
        "import os\n",
        "import json\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import copy\n",
        "from datetime import datetime\n",
        "import logging\n",
        "from transformers import TimesformerForVideoClassification, AutoImageProcessor\n",
        "\n",
        "# Configuración básica\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Verificar disponibilidad de GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Usando dispositivo: {device}\")\n",
        "\n",
        "# Configurar directorios y rutas\n",
        "CONFIG = {\n",
        "    \"output_dir\": \"/content/drive/MyDrive/TrabajoProyecto_IA3/modelo_timesformer\",\n",
        "    \"model_name\": \"timesformer_violence_detector\",\n",
        "    \"pretrained_model\": \"facebook/timesformer-base-finetuned-k400\",\n",
        "    \"num_frames\": 8,\n",
        "    \"image_size\": 224,\n",
        "    \"num_classes\": 2,\n",
        "    \"threshold\": 0.70,\n",
        "}\n",
        "\n",
        "# Rutas de los modelos\n",
        "best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft2.pt\")\n",
        "exports_dir = os.path.join(CONFIG[\"output_dir\"], \"exported_models\")\n",
        "os.makedirs(exports_dir, exist_ok=True)\n",
        "\n",
        "print(\"=== EXPORTACIÓN DEL MODELO TIMESFORMER PARA DETECCIÓN DE VIOLENCIA ===\")\n",
        "print(f\"Cargando modelo entrenado desde: {best_ft_model_path}\")\n",
        "\n",
        "# Cargar el modelo entrenado\n",
        "try:\n",
        "    # Crear la instancia del modelo con la misma configuración usada en entrenamiento\n",
        "    model = TimesformerForVideoClassification.from_pretrained(\n",
        "        CONFIG[\"pretrained_model\"],\n",
        "        num_frames=CONFIG[\"num_frames\"],\n",
        "        image_size=CONFIG[\"image_size\"],\n",
        "        num_labels=CONFIG[\"num_classes\"],\n",
        "        ignore_mismatched_sizes=True\n",
        "    )\n",
        "\n",
        "    # Cargar los pesos del modelo entrenado\n",
        "    checkpoint = torch.load(best_ft_model_path, map_location=device)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "    # Mover a GPU si está disponible y establecer modo evaluación\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"Modelo cargado exitosamente. F1-Score: {checkpoint.get('val_f1', 'N/A')}\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar el modelo: {str(e)}\")\n",
        "    raise\n",
        "\n",
        "# CLASE PARA INFERENCIA OPTIMIZADA\n",
        "class TimesformerOptimizedInference(nn.Module):\n",
        "    \"\"\"Modelo optimizado para inferencia que simplifica la salida\"\"\"\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.timesformer = model\n",
        "        # Desactivar dropout para inferencia\n",
        "        for module in self.modules():\n",
        "            if hasattr(module, 'dropout'):\n",
        "                module.dropout.p = 0\n",
        "\n",
        "    def forward(self, pixel_values):\n",
        "        self.timesformer.eval()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.timesformer(pixel_values=pixel_values)\n",
        "            logits = outputs.logits\n",
        "            # Retornar probabilidades\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            return probs\n",
        "\n",
        "# 1. Guardar modelo PyTorch original\n",
        "print(\"\\n1. Guardando modelo PyTorch original...\")\n",
        "original_model_path = os.path.join(exports_dir, f\"{CONFIG['model_name']}_original.pt\")\n",
        "\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'config': CONFIG,\n",
        "    'val_f1': checkpoint.get('val_f1', None),\n",
        "    'epoch': checkpoint.get('epoch', None),\n",
        "}, original_model_path)\n",
        "\n",
        "print(f\"✓ Modelo original guardado: {original_model_path}\")\n",
        "print(f\"  Tamaño: {os.path.getsize(original_model_path) / (1024*1024):.2f} MB\")\n",
        "\n",
        "# 2. Crear modelo de inferencia optimizado\n",
        "print(\"\\n2. Creando modelo optimizado para inferencia...\")\n",
        "optimized_model = TimesformerOptimizedInference(model)\n",
        "optimized_model.eval()\n",
        "\n",
        "# Guardar modelo optimizado\n",
        "optimized_model_path = os.path.join(exports_dir, f\"{CONFIG['model_name']}_inference.pt\")\n",
        "torch.save(optimized_model, optimized_model_path)\n",
        "print(f\"✓ Modelo optimizado guardado: {optimized_model_path}\")\n",
        "print(f\"  Tamaño: {os.path.getsize(optimized_model_path) / (1024*1024):.2f} MB\")\n",
        "\n",
        "# 3. Exportar modelo FP16 (half precision)\n",
        "print(\"\\n3. Exportando modelo en precisión FP16...\")\n",
        "optimized_model_fp16 = copy.deepcopy(optimized_model).half()\n",
        "optimized_model_fp16.eval()\n",
        "\n",
        "fp16_model_path = os.path.join(exports_dir, f\"{CONFIG['model_name']}_fp16.pt\")\n",
        "torch.save(optimized_model_fp16, fp16_model_path)\n",
        "print(f\"✓ Modelo FP16 guardado: {fp16_model_path}\")\n",
        "print(f\"  Tamaño: {os.path.getsize(fp16_model_path) / (1024*1024):.2f} MB\")\n",
        "\n",
        "\n",
        "\n",
        "# 4. EXPORTACIÓN A TORCHSCRIPT - OPCIÓN 1 (Con half precision)\n",
        "print(\"\\n4. Exportando a TorchScript (opción half precision)...\")\n",
        "try:\n",
        "    # Crear un wrapper para manejar la reorganización de dimensiones\n",
        "    class TSModelWrapper(nn.Module):\n",
        "        def __init__(self, model):\n",
        "            super().__init__()\n",
        "            self.model = model\n",
        "\n",
        "        def forward(self, x):\n",
        "            # x tiene forma [batch_size, channels, num_frames, height, width]\n",
        "            # Reordenar a [batch_size, num_frames, channels, height, width]\n",
        "            x = x.permute(0, 2, 1, 3, 4)\n",
        "            return self.model(pixel_values=x)\n",
        "\n",
        "    # Crear el wrapper con el modelo\n",
        "    ts_wrapper_half = TSModelWrapper(optimized_model_fp16)\n",
        "\n",
        "    # Crear input de ejemplo con la forma que espera el wrapper\n",
        "    example_input_half = torch.randn(\n",
        "        1, 3, CONFIG[\"num_frames\"], CONFIG[\"image_size\"], CONFIG[\"image_size\"],\n",
        "        device=device).half()\n",
        "\n",
        "    # Trazar modelo\n",
        "    scripted_model_half = torch.jit.trace(ts_wrapper_half, example_input_half)\n",
        "    scripted_model_half = torch.jit.optimize_for_inference(scripted_model_half)\n",
        "\n",
        "    # Guardar modelo TorchScript (half)\n",
        "    ts_half_path = os.path.join(exports_dir, f\"{CONFIG['model_name']}_scripted_half.pt\")\n",
        "    torch.jit.save(scripted_model_half, ts_half_path)\n",
        "\n",
        "    print(f\"✓ Modelo TorchScript (half) guardado: {ts_half_path}\")\n",
        "    print(f\"  Tamaño: {os.path.getsize(ts_half_path) / (1024*1024):.2f} MB\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al exportar a TorchScript (half): {str(e)}\")\n",
        "    print(f\"✗ Error al exportar a TorchScript (half): {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "# 5. EXPORTACIÓN A TORCHSCRIPT - OPCIÓN 2 (Con precisión original)\n",
        "print(\"\\n5. Exportando a TorchScript (precisión original)...\")\n",
        "try:\n",
        "    # Crear wrapper\n",
        "    ts_wrapper_float = TSModelWrapper(optimized_model)\n",
        "\n",
        "    # Crear input de ejemplo en precisión completa\n",
        "    example_input_float = torch.randn(\n",
        "        1, 3, CONFIG[\"num_frames\"], CONFIG[\"image_size\"], CONFIG[\"image_size\"],\n",
        "        device=device)\n",
        "\n",
        "    # Trazar modelo original\n",
        "    scripted_model_float = torch.jit.trace(ts_wrapper_float, example_input_float)\n",
        "    scripted_model_float = torch.jit.optimize_for_inference(scripted_model_float)\n",
        "\n",
        "    # Guardar modelo TorchScript (float)\n",
        "    ts_float_path = os.path.join(exports_dir, f\"{CONFIG['model_name']}_scripted_float.pt\")\n",
        "    torch.jit.save(scripted_model_float, ts_float_path)\n",
        "\n",
        "    print(f\"✓ Modelo TorchScript (float) guardado: {ts_float_path}\")\n",
        "    print(f\"  Tamaño: {os.path.getsize(ts_float_path) / (1024*1024):.2f} MB\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al exportar a TorchScript (float): {str(e)}\")\n",
        "    print(f\"✗ Error al exportar a TorchScript (float): {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 6. EXPORTACIÓN A ONNX - OPCIÓN 1 (Con half precision)\n",
        "print(\"\\n6. Exportando a ONNX (opción half precision)...\")\n",
        "try:\n",
        "    # Clase wrapper para ONNX con reordenamiento de dimensiones\n",
        "    class ONNXWrapperHalf(nn.Module):\n",
        "        def __init__(self, model):\n",
        "            super().__init__()\n",
        "            self.model = model\n",
        "\n",
        "        def forward(self, x):\n",
        "            # x tiene forma [batch_size, channels, num_frames, height, width]\n",
        "            # Reordenar a [batch_size, num_frames, channels, height, width]\n",
        "            x = x.permute(0, 2, 1, 3, 4)\n",
        "            return self.model(pixel_values=x)\n",
        "\n",
        "    # Crear wrapper y entrada\n",
        "    onnx_wrapper_half = ONNXWrapperHalf(optimized_model_fp16)\n",
        "    dummy_input_half = torch.randn(\n",
        "        1, 3, CONFIG[\"num_frames\"], CONFIG[\"image_size\"], CONFIG[\"image_size\"],\n",
        "        device=device).half()\n",
        "\n",
        "    # Ruta para el modelo ONNX\n",
        "    onnx_half_path = os.path.join(exports_dir, f\"{CONFIG['model_name']}_half.onnx\")\n",
        "\n",
        "    # Exportar a ONNX\n",
        "    torch.onnx.export(\n",
        "        onnx_wrapper_half,\n",
        "        dummy_input_half,\n",
        "        onnx_half_path,\n",
        "        export_params=True,\n",
        "        opset_version=12,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['input'],\n",
        "        output_names=['output'],\n",
        "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Modelo ONNX (half) guardado: {onnx_half_path}\")\n",
        "    print(f\"  Tamaño: {os.path.getsize(onnx_half_path) / (1024*1024):.2f} MB\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al exportar a ONNX (half): {str(e)}\")\n",
        "    print(f\"✗ Error al exportar a ONNX (half): {str(e)}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 7. EXPORTACIÓN A ONNX - OPCIÓN 2 (Con precisión original)\n",
        "print(\"\\n7. Exportando a ONNX (precisión original)...\")\n",
        "try:\n",
        "    # Clase wrapper para ONNX\n",
        "    class ONNXWrapperFloat(nn.Module):\n",
        "        def __init__(self, model):\n",
        "            super().__init__()\n",
        "            self.model = model\n",
        "\n",
        "        def forward(self, x):\n",
        "            # x tiene forma [batch_size, channels, num_frames, height, width]\n",
        "            # Reordenar a [batch_size, num_frames, channels, height, width]\n",
        "            x = x.permute(0, 2, 1, 3, 4)\n",
        "            return self.model(pixel_values=x)\n",
        "\n",
        "    # Crear wrapper y entrada\n",
        "    onnx_wrapper_float = ONNXWrapperFloat(optimized_model)\n",
        "    dummy_input_float = torch.randn(\n",
        "        1, 3, CONFIG[\"num_frames\"], CONFIG[\"image_size\"], CONFIG[\"image_size\"],\n",
        "        device=device)\n",
        "\n",
        "    # Ruta para el modelo ONNX\n",
        "    onnx_float_path = os.path.join(exports_dir, f\"{CONFIG['model_name']}_float.onnx\")\n",
        "\n",
        "    # Exportar a ONNX\n",
        "    torch.onnx.export(\n",
        "        onnx_wrapper_float,\n",
        "        dummy_input_float,\n",
        "        onnx_float_path,\n",
        "        export_params=True,\n",
        "        opset_version=12,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['input'],\n",
        "        output_names=['output'],\n",
        "        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Modelo ONNX (float) guardado: {onnx_float_path}\")\n",
        "    print(f\"  Tamaño: {os.path.getsize(onnx_float_path) / (1024*1024):.2f} MB\")\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al exportar a ONNX (float): {str(e)}\")\n",
        "    print(f\"✗ Error al exportar a ONNX (float): {str(e)}\")\n",
        "\n",
        "# 8. Guardar procesador de imágenes\n",
        "print(\"\\n8. Guardando procesador de imágenes...\")\n",
        "processor = AutoImageProcessor.from_pretrained(CONFIG[\"pretrained_model\"])\n",
        "processor_path = os.path.join(exports_dir, \"processor\")\n",
        "os.makedirs(processor_path, exist_ok=True)\n",
        "processor.save_pretrained(processor_path)\n",
        "print(f\"✓ Procesador guardado en: {processor_path}\")\n",
        "\n",
        "# 9. Guardar configuración de inferencia\n",
        "print(\"\\n9. Guardando configuración de inferencia...\")\n",
        "inference_config = {\n",
        "    \"num_frames\": CONFIG[\"num_frames\"],\n",
        "    \"image_size\": CONFIG[\"image_size\"],\n",
        "    \"threshold\": CONFIG[\"threshold\"],\n",
        "    \"model_type\": \"TimesformerForVideoClassification\",\n",
        "    \"labels\": [\"no_violencia\", \"violencia\"],\n",
        "    \"fps\": 15,\n",
        "    \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "    \"formats_available\": {\n",
        "        \"pytorch_original\": os.path.basename(original_model_path),\n",
        "        \"pytorch_inference\": os.path.basename(optimized_model_path),\n",
        "        \"pytorch_fp16\": os.path.basename(fp16_model_path),\n",
        "        \"torchscript_half\": os.path.basename(ts_half_path) if 'ts_half_path' in locals() else None,\n",
        "        \"torchscript_float\": os.path.basename(ts_float_path) if 'ts_float_path' in locals() else None,\n",
        "        \"onnx_half\": os.path.basename(onnx_half_path) if 'onnx_half_path' in locals() else None,\n",
        "        \"onnx_float\": os.path.basename(onnx_float_path) if 'onnx_float_path' in locals() else None,\n",
        "    },\n",
        "    \"input_format\": {\n",
        "        \"TorchScript_and_ONNX\": \"[batch_size, channels, num_frames, height, width]\",\n",
        "        \"PyTorch_original\": \"[batch_size, num_frames, channels, height, width]\",\n",
        "        \"note\": \"Es necesario permuter las dimensiones según el formato elegido\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Guardar configuración\n",
        "inference_config_path = os.path.join(exports_dir, \"inference_config.json\")\n",
        "with open(inference_config_path, 'w') as f:\n",
        "    json.dump(inference_config, f, indent=4)\n",
        "\n",
        "print(f\"✓ Configuración guardada: {inference_config_path}\")\n",
        "\n",
        "# 10. Crear script de ejemplo para inferencia\n",
        "print(\"\\n10. Creando script de ejemplo para inferencia...\")\n",
        "example_script = \"\"\"import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from transformers import AutoImageProcessor\n",
        "\n",
        "# Configuración\n",
        "CONFIG = {\n",
        "    \"model_path\": \"MODEL_PATH\",  # Reemplazar con la ruta al modelo exportado\n",
        "    \"processor_path\": \"PROCESSOR_PATH\",  # Ruta al procesador\n",
        "    \"num_frames\": 8,\n",
        "    \"image_size\": 224,\n",
        "    \"threshold\": 0.70,\n",
        "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "}\n",
        "\n",
        "# Determinar tipo de modelo\n",
        "is_torchscript = CONFIG[\"model_path\"].endswith(\".pt\") and \"scripted\" in CONFIG[\"model_path\"]\n",
        "is_onnx = CONFIG[\"model_path\"].endswith(\".onnx\")\n",
        "\n",
        "# Cargar procesador\n",
        "processor = AutoImageProcessor.from_pretrained(CONFIG[\"processor_path\"])\n",
        "\n",
        "# Cargar modelo según su tipo\n",
        "if is_torchscript:\n",
        "    model = torch.jit.load(CONFIG[\"model_path\"]).to(CONFIG[\"device\"])\n",
        "    model.eval()\n",
        "elif is_onnx:\n",
        "    import onnxruntime as ort\n",
        "    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if torch.cuda.is_available() else ['CPUExecutionProvider']\n",
        "    model = ort.InferenceSession(CONFIG[\"model_path\"], providers=providers)\n",
        "else:\n",
        "    # Modelo PyTorch estándar\n",
        "    model = torch.load(CONFIG[\"model_path\"], map_location=CONFIG[\"device\"])\n",
        "    model.eval()\n",
        "\n",
        "def extract_frames(video_path, num_frames):\n",
        "    \"\"Extrae frames uniformemente distribuidos de un video\"\"\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    # Calcular índices de frames a extraer\n",
        "    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n",
        "    frames = []\n",
        "\n",
        "    for idx in indices:\n",
        "        cap.set(cv2.CAP_PROP_POS_FRAMES, idx)\n",
        "        ret, frame = cap.read()\n",
        "        if ret:\n",
        "            # Convertir de BGR a RGB\n",
        "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            # Redimensionar\n",
        "            frame = cv2.resize(frame, (CONFIG[\"image_size\"], CONFIG[\"image_size\"]))\n",
        "            frames.append(frame)\n",
        "\n",
        "    cap.release()\n",
        "    return frames\n",
        "\n",
        "def predict_violence(video_path):\n",
        "    \"\"Predice si hay violencia en un video\"\"\n",
        "    # Extraer frames\n",
        "    frames = extract_frames(video_path, CONFIG[\"num_frames\"])\n",
        "    if len(frames) != CONFIG[\"num_frames\"]:\n",
        "        raise ValueError(f\"No se pudieron extraer {CONFIG['num_frames']} frames del video\")\n",
        "\n",
        "    # Preprocesar frames según el tipo de modelo\n",
        "    if is_torchscript or is_onnx:\n",
        "        # Para TorchScript/ONNX (espera [batch, C, T, H, W])\n",
        "        # Crear tensor [B, C, T, H, W]\n",
        "        tensor_input = np.array(frames).transpose(3, 0, 1, 2) / 255.0  # [C, T, H, W]\n",
        "        tensor_input = np.expand_dims(tensor_input, 0)  # [B, C, T, H, W]\n",
        "        tensor_input = torch.from_numpy(tensor_input).float()\n",
        "\n",
        "        if is_torchscript:\n",
        "            tensor_input = tensor_input.to(CONFIG[\"device\"])\n",
        "            if \"half\" in CONFIG[\"model_path\"]:\n",
        "                tensor_input = tensor_input.half()\n",
        "\n",
        "            # Inferencia\n",
        "            with torch.no_grad():\n",
        "                outputs = model(tensor_input)\n",
        "                probs = outputs.cpu().numpy()[0]\n",
        "        else:  # ONNX\n",
        "            # Ejecutar inferencia ONNX\n",
        "            ort_inputs = {model.get_inputs()[0].name: tensor_input.cpu().numpy()}\n",
        "            outputs = model.run(None, ort_inputs)\n",
        "            probs = outputs[0][0]\n",
        "    else:\n",
        "        # Para modelo PyTorch normal (espera [B, T, C, H, W])\n",
        "        inputs = processor(frames, return_tensors=\"pt\")\n",
        "        pixel_values = inputs[\"pixel_values\"].to(CONFIG[\"device\"])\n",
        "\n",
        "        # Inferencia\n",
        "        with torch.no_grad():\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            probs = outputs[0].cpu().numpy()\n",
        "\n",
        "    # Procesar resultado\n",
        "    violence_prob = probs[1]  # Probabilidad de clase \"violencia\"\n",
        "    is_violence = violence_prob >= CONFIG[\"threshold\"]\n",
        "\n",
        "    return {\n",
        "        \"is_violence\": bool(is_violence),\n",
        "        \"violence_probability\": float(violence_prob),\n",
        "        \"no_violence_probability\": float(probs[0])\n",
        "    }\n",
        "\n",
        "# Ejemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    video_path = \"ruta/a/tu/video.mp4\"  # Reemplazar con ruta a un video\n",
        "    result = predict_violence(video_path)\n",
        "    print(f\"Predicción: {'VIOLENCIA' if result['is_violence'] else 'NO VIOLENCIA'}\")\n",
        "    print(f\"Probabilidad de violencia: {result['violence_probability']:.4f}\")\n",
        "    print(f\"Probabilidad de no violencia: {result['no_violence_probability']:.4f}\")\n",
        "\"\"\"\n",
        "\n",
        "# Guardar script de ejemplo\n",
        "example_script_path = os.path.join(exports_dir, \"inference_example.py\")\n",
        "with open(example_script_path, 'w') as f:\n",
        "    f.write(example_script)\n",
        "\n",
        "print(f\"✓ Script de ejemplo guardado: {example_script_path}\")\n",
        "\n",
        "# Resumen final\n",
        "print(\"\\n=== RESUMEN DE EXPORTACIÓN ===\")\n",
        "print(f\"Directorio de modelos exportados: {exports_dir}\")\n",
        "print(\"Formatos disponibles:\")\n",
        "for format_name, filename in inference_config[\"formats_available\"].items():\n",
        "    if filename:\n",
        "        format_path = os.path.join(exports_dir, filename)\n",
        "        size_mb = os.path.getsize(format_path) / (1024*1024) if os.path.exists(format_path) else 0\n",
        "        print(f\"- {format_name}: {filename} ({size_mb:.2f} MB)\")\n",
        "print(\"\\nPróximos pasos:\")\n",
        "print(\"1. Selecciona el formato más adecuado para tu despliegue\")\n",
        "print(\"2. Adapta el script de ejemplo (inference_example.py) para tu aplicación web\")\n",
        "print(\"3. Para React: Configura un servidor backend que utilice el modelo o explora ONNX Runtime Web\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UfhRTZDESCC",
        "outputId": "ae84ce59-181e-4676-9e7a-d9b5b1c805ce"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "TimesformerForVideoClassification(\n",
              "  (timesformer): TimesformerModel(\n",
              "    (embeddings): TimesformerEmbeddings(\n",
              "      (patch_embeddings): TimesformerPatchEmbeddings(\n",
              "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
              "      )\n",
              "      (pos_drop): Dropout(p=0.0, inplace=False)\n",
              "      (time_drop): Dropout(p=0.0, inplace=False)\n",
              "    )\n",
              "    (encoder): TimesformerEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x TimesformerLayer(\n",
              "          (drop_path): Identity()\n",
              "          (attention): TimeSformerAttention(\n",
              "            (attention): TimesformerSelfAttention(\n",
              "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): TimesformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): TimesformerIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (dropout): Dropout(p=0, inplace=False)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): TimesformerOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (dropout): Dropout(p=0, inplace=False)\n",
              "          )\n",
              "          (layernorm_before): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (layernorm_after): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (temporal_layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "          (temporal_attention): TimeSformerAttention(\n",
              "            (attention): TimesformerSelfAttention(\n",
              "              (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
              "              (attn_drop): Dropout(p=0.0, inplace=False)\n",
              "            )\n",
              "            (output): TimesformerSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (temporal_dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (layernorm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
              "  )\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LaslpW1oc-x",
        "outputId": "eb7979a4-415b-44bc-bbb4-0bce6393ddde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Realizando pruebas de inferencia en muestras\n",
            "Cargados 800 videos para split 'test'\n",
            "Violencia: 400, No Violencia: 400\n",
            "Precisión en muestras de prueba: 5/5 (100.0%)\n"
          ]
        }
      ],
      "source": [
        "# ============================== PRUEBA DE INFERENCIA ==============================\n",
        "\n",
        "logger.info(\"Realizando pruebas de inferencia en muestras\")\n",
        "print(\"Realizando pruebas de inferencia en muestras\")\n",
        "\n",
        "# Comprobar si ya tenemos el modelo cargado\n",
        "try:\n",
        "    # Intentar acceder al modelo\n",
        "    model\n",
        "    # Asegurarse de que tiene cargado el mejor modelo de fine-tuning\n",
        "    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft2.pt\")\n",
        "    checkpoint = torch.load(best_ft_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "except NameError:\n",
        "    # Si el modelo no está definido, crear uno nuevo y cargarlo\n",
        "    logger.info(\"Cargando modelo desde checkpoint\")\n",
        "    print(\"Cargando modelo desde checkpoint\")\n",
        "    model = TimesformerForVideoClassification.from_pretrained(\n",
        "        CONFIG[\"pretrained_model\"],\n",
        "        num_frames=CONFIG[\"num_frames\"],\n",
        "        image_size=CONFIG[\"image_size\"],\n",
        "    )\n",
        "\n",
        "    # Adaptarlo a nuestra tarea\n",
        "    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n",
        "        model.classifier = nn.Sequential(\n",
        "            nn.Dropout(CONFIG[\"tl_dropout\"]),\n",
        "            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n",
        "        )\n",
        "\n",
        "    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft2.pt\")\n",
        "    checkpoint = torch.load(best_ft_model_path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "\n",
        "# Asegurar que el modelo está en el dispositivo correcto\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Cargar dataset de prueba\n",
        "test_dataset = ViolenceVideoDataset(\n",
        "    root_dir=CONFIG[\"dataset_path\"],\n",
        "    split='test',\n",
        "    num_frames=CONFIG[\"num_frames\"],\n",
        "    image_size=CONFIG[\"image_size\"]\n",
        ")\n",
        "\n",
        "# Seleccionar algunas muestras aleatorias\n",
        "num_samples = min(5, len(test_dataset))\n",
        "sample_indices = random.sample(range(len(test_dataset)), num_samples)\n",
        "\n",
        "# Resultados\n",
        "results = []\n",
        "\n",
        "# Crear figura para visualización\n",
        "fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4*num_samples))\n",
        "if num_samples == 1:\n",
        "    axes = axes.reshape(1, 2)\n",
        "\n",
        "for i, idx in enumerate(sample_indices):\n",
        "    try:\n",
        "        # Obtener muestra\n",
        "        sample = test_dataset[idx]\n",
        "        pixel_values = sample['pixel_values'].unsqueeze(0).to(device)  # Añadir dimensión de batch\n",
        "        label = sample['labels'].item()\n",
        "        video_path = sample['video_path']\n",
        "\n",
        "        # Inferencia\n",
        "        with torch.no_grad():\n",
        "            outputs = model(pixel_values=pixel_values)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.softmax(logits, dim=1)\n",
        "            violence_prob = probs[0, 1].item()\n",
        "            prediction = violence_prob >= CONFIG[\"threshold\"]\n",
        "\n",
        "        # Extraer un frame para visualización\n",
        "        video_reader = VideoReader(video_path, ctx=cpu(0))\n",
        "        mid_frame_idx = len(video_reader) // 2\n",
        "        frame = video_reader[mid_frame_idx].asnumpy()\n",
        "\n",
        "        # Guardar resultado\n",
        "        results.append({\n",
        "            'video_path': video_path,\n",
        "            'true_label': label,\n",
        "            'violence_prob': violence_prob,\n",
        "            'prediction': prediction,\n",
        "            'correct': (prediction == label)\n",
        "        })\n",
        "\n",
        "        # Visualizar\n",
        "        axes[i, 0].imshow(frame)\n",
        "        axes[i, 0].set_title(f\"Video: {os.path.basename(video_path)}\")\n",
        "        axes[i, 0].axis('off')\n",
        "\n",
        "        # Graficar probabilidad\n",
        "        bar_colors = ['green', 'red']\n",
        "        class_names = ['No Violencia', 'Violencia']\n",
        "        class_probs = [1 - violence_prob, violence_prob]\n",
        "\n",
        "        axes[i, 1].barh(class_names, class_probs, color=bar_colors)\n",
        "        axes[i, 1].set_xlim(0, 1)\n",
        "        axes[i, 1].set_title(f\"Predicción: {'Violencia' if prediction else 'No Violencia'} \" +\n",
        "                          f\"(Real: {'Violencia' if label else 'No Violencia'})\")\n",
        "        axes[i, 1].axvline(x=CONFIG[\"threshold\"], color='black', linestyle='--',\n",
        "                      label=f'Umbral: {CONFIG[\"threshold\"]:.2f}')\n",
        "        axes[i, 1].legend()\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Error al procesar muestra {idx}: {str(e)}\")\n",
        "        # En caso de error, dejar la posición vacía\n",
        "        axes[i, 0].axis('off')\n",
        "        axes[i, 1].axis('off')\n",
        "        continue\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"inference_samples.png\"))\n",
        "plt.close()\n",
        "\n",
        "# Guardar resultados\n",
        "if results:\n",
        "    results_df = pd.DataFrame(results)\n",
        "    results_df.to_csv(os.path.join(CONFIG[\"output_dir\"], \"inference_samples_results.csv\"), index=False)\n",
        "\n",
        "    # Mostrar resumen\n",
        "    correct_count = sum(1 for r in results if r['correct'])\n",
        "    logger.info(f\"Precisión en muestras de prueba: {correct_count}/{len(results)} ({100 * correct_count / len(results):.1f}%)\")\n",
        "    print(f\"Precisión en muestras de prueba: {correct_count}/{len(results)} ({100 * correct_count / len(results):.1f}%)\")\n",
        "else:\n",
        "    logger.warning(\"No se pudieron procesar muestras para pruebas de inferencia\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GcqFvvp4ohU5"
      },
      "outputs": [],
      "source": [
        "# ============================== RESUMEN FINAL DE MÉTRICAS ==============================\n",
        "\n",
        "logger.info(\"=== RESUMEN FINAL DE MÉTRICAS ===\")\n",
        "\n",
        "# Intentar cargar informes de evaluación\n",
        "try:\n",
        "    # Transfer Learning\n",
        "    tl_report_path = os.path.join(CONFIG[\"output_dir\"], \"evaluation_report_transfer_learning.json\")\n",
        "    with open(tl_report_path, 'r') as f:\n",
        "        tl_report = json.load(f)\n",
        "\n",
        "    # Fine-Tuning\n",
        "    ft_report_path = os.path.join(CONFIG[\"output_dir\"], \"evaluation_report_fine_tuning.json\")\n",
        "    with open(ft_report_path, 'r') as f:\n",
        "        ft_report = json.load(f)\n",
        "\n",
        "    # Test\n",
        "    test_report_path = os.path.join(CONFIG[\"output_dir\"], \"evaluation_report_test.json\")\n",
        "    with open(test_report_path, 'r') as f:\n",
        "        test_report = json.load(f)\n",
        "\n",
        "    # Mostrar métricas\n",
        "    logger.info(\"Métricas en Transfer Learning (validación):\")\n",
        "    logger.info(f\"  - Accuracy: {tl_report['metrics']['accuracy']:.4f}\")\n",
        "    logger.info(f\"  - Precision: {tl_report['metrics']['precision']:.4f}\")\n",
        "    logger.info(f\"  - Recall (Sensibilidad): {tl_report['metrics']['recall']:.4f}\")\n",
        "    logger.info(f\"  - Specificity: {tl_report['metrics']['specificity']:.4f}\")\n",
        "    logger.info(f\"  - F1-Score: {tl_report['metrics']['f1_score']:.4f}\")\n",
        "    logger.info(f\"  - ROC AUC: {tl_report['metrics']['roc_auc']:.4f}\")\n",
        "\n",
        "    logger.info(\"Métricas en Fine-Tuning (validación):\")\n",
        "    logger.info(f\"  - Accuracy: {ft_report['metrics']['accuracy']:.4f}\")\n",
        "    logger.info(f\"  - Precision: {ft_report['metrics']['precision']:.4f}\")\n",
        "    logger.info(f\"  - Recall (Sensibilidad): {ft_report['metrics']['recall']:.4f}\")\n",
        "    logger.info(f\"  - Specificity: {ft_report['metrics']['specificity']:.4f}\")\n",
        "    logger.info(f\"  - F1-Score: {ft_report['metrics']['f1_score']:.4f}\")\n",
        "    logger.info(f\"  - ROC AUC: {ft_report['metrics']['roc_auc']:.4f}\")\n",
        "\n",
        "    logger.info(\"Métricas en Test (final):\")\n",
        "    logger.info(f\"  - Accuracy: {test_report['metrics']['accuracy']:.4f}\")\n",
        "    logger.info(f\"  - Precision: {test_report['metrics']['precision']:.4f}\")\n",
        "    logger.info(f\"  - Recall (Sensibilidad): {test_report['metrics']['recall']:.4f}\")\n",
        "    logger.info(f\"  - Specificity: {test_report['metrics']['specificity']:.4f}\")\n",
        "    logger.info(f\"  - F1-Score: {test_report['metrics']['f1_score']:.4f}\")\n",
        "    logger.info(f\"  - ROC AUC: {test_report['metrics']['roc_auc']:.4f}\")\n",
        "\n",
        "    # Crear tabla comparativa\n",
        "    metrics = ['accuracy', 'precision', 'recall', 'specificity', 'f1_score', 'roc_auc']\n",
        "    data = {\n",
        "        'Métrica': metrics,\n",
        "        'Transfer Learning': [tl_report['metrics'][m] for m in metrics],\n",
        "        'Fine-Tuning': [ft_report['metrics'][m] for m in metrics],\n",
        "        'Test': [test_report['metrics'][m] for m in metrics]\n",
        "    }\n",
        "\n",
        "    df = pd.DataFrame(data)\n",
        "\n",
        "    # Formatear para mostrar resultados\n",
        "    pd.set_option('display.max_columns', None)\n",
        "    pd.set_option('display.width', 120)\n",
        "    pd.set_option('display.precision', 4)\n",
        "\n",
        "    print(\"\\n=== TABLA COMPARATIVA DE MÉTRICAS ===\")\n",
        "    print(df)\n",
        "\n",
        "    # Guardar tabla\n",
        "    df.to_csv(os.path.join(CONFIG[\"output_dir\"], \"metrics_comparison.csv\"), index=False)\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar informes de evaluación: {str(e)}\")\n",
        "    logger.info(\"Asegúrate de que las fases de Transfer Learning, Fine-Tuning y Test ya se han ejecutado.\")\n",
        "\n",
        "# Mostrar información sobre el modelo final\n",
        "try:\n",
        "    # Cargar información del benchmark\n",
        "    benchmark_path = os.path.join(CONFIG[\"output_dir\"], \"benchmark_results.json\")\n",
        "    with open(benchmark_path, 'r') as f:\n",
        "        benchmark = json.load(f)\n",
        "\n",
        "    logger.info(\"\\nRendimiento del modelo:\")\n",
        "    logger.info(f\"  - Tiempo por inferencia: {benchmark['avg_time_per_inference_ms']:.2f} ms\")\n",
        "    logger.info(f\"  - Frames por segundo: {benchmark['fps']:.2f} FPS\")\n",
        "\n",
        "    # Cargar umbrales óptimos\n",
        "    thresholds_path = os.path.join(CONFIG[\"output_dir\"], \"optimal_thresholds.json\")\n",
        "    with open(thresholds_path, 'r') as f:\n",
        "        thresholds = json.load(f)\n",
        "\n",
        "    logger.info(\"\\nUmbrales óptimos:\")\n",
        "    logger.info(f\"  - Umbral óptimo según F1: {thresholds['f1_optimal']:.4f}\")\n",
        "    logger.info(f\"  - Umbral óptimo según ROC: {thresholds['roc_optimal']:.4f}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al cargar información de rendimiento: {str(e)}\")\n",
        "\n",
        "# Mostrar rutas de los modelos exportados\n",
        "try:\n",
        "    model_paths = {\n",
        "        'Modelo PyTorch': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_final.pt\"),\n",
        "        'Modelo de Inferencia': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_inference.pt\"),\n",
        "        'Modelo Hugging Face': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_hf\"),\n",
        "        'Modelo ONNX': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}.onnx\"),\n",
        "        'Script de Inferencia': os.path.join(CONFIG[\"output_dir\"], \"inference_example.py\")\n",
        "    }\n",
        "\n",
        "    logger.info(\"\\nModelos exportados:\")\n",
        "    for name, path in model_paths.items():\n",
        "        exists = \"✓\" if os.path.exists(path) else \"✗\"\n",
        "        logger.info(f\"  - {name}: {path} {exists}\")\n",
        "\n",
        "except Exception as e:\n",
        "    logger.error(f\"Error al verificar rutas de modelos: {str(e)}\")\n",
        "\n",
        "logger.info(\"\\n¡Entrenamiento y evaluación del modelo TimeSformer para detección de violencia completados!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HJlJnDv2hxBV"
      },
      "source": [
        "# CONCLUSIONES"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0036e20bbb244e398638463dc411bb00": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0141526f8d0141b6a812b0a6941272bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e507e90e67e40e18ebd1dfcaf570fe0",
            "max": 412,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0c68530852a4c03822a5eddb7d167ab",
            "value": 412
          }
        },
        "02c3a9e0ab7b41f1b730034c6825a120": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_993c467478ac462893cdee1ada619cae",
              "IPY_MODEL_0141526f8d0141b6a812b0a6941272bb",
              "IPY_MODEL_70084cb51d8d4c769b8bba3924790cc0"
            ],
            "layout": "IPY_MODEL_f3b7cd4fde9441599d0f88d16d8d50ea"
          }
        },
        "03c6b59427064dd7aff79ac299f0154c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "05a7492588824d1183fbc5f2057c7097": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0ab87c4962c24854977b1720b6e397e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5a3d9f8c76540d0845eda6990761822",
            "placeholder": "​",
            "style": "IPY_MODEL_f696fd99ca4c45fb8072d9ccf4ef18b3",
            "value": "config.json: 100%"
          }
        },
        "163caaa69e744e359df1b98f551e936b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17843f701f674c7b836a2d232a81b1ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1abdbe734d9f4ff68b0622e8d7fc2e38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbfb1e3fd58e4c79a28df0a968c74132",
            "placeholder": "​",
            "style": "IPY_MODEL_53627f45829541208ec35da5085c9793",
            "value": " 22.7k/22.7k [00:00&lt;00:00, 2.71MB/s]"
          }
        },
        "20359e031e13479185630f6f08c81deb": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7039014ae1f24d6da3fb528b30122076",
            "max": 412,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb869f720924426eb8cf0d86e80a5fe5",
            "value": 412
          }
        },
        "26194c7196434dd690fc5f3c366f8cdf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b5ff5cf648d4503bfd005fd75b0bd98": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3015681206ea445c8a441d38f86abd65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31a0ce2d230b42978c6455e06367ee9a",
            "placeholder": "​",
            "style": "IPY_MODEL_88c3145659364b82a7500247fe48fc5e",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "304defd4210344dea1df926daca3b354": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31a0ce2d230b42978c6455e06367ee9a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "36d4caa942934ce892267b17c9677632": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3cbcffb047f94bee9efb6a256821b4cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "3fb063b921f54cdcb828d47e570e0f31": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41fb33a7b9c2498197d830bff6c2fc2b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "430de3d2c7e64d89b931da4f4f4c015d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8f8d3646a0ae4a8da2ca2a47baa46c1a",
            "max": 486348721,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_43c20c9b6cff452d97ff206c646a8203",
            "value": 486348721
          }
        },
        "43c20c9b6cff452d97ff206c646a8203": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "48ba9105fa0c4a84abddb5c2188974f8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c920dd08c514eeb9a0a89e8784a85d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e1171c0b43d4bd1ba05f88cc7ee512c": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "53627f45829541208ec35da5085c9793": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5442d68d0851435c83c97d3b87b97673": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3fb063b921f54cdcb828d47e570e0f31",
            "max": 486296528,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3cbcffb047f94bee9efb6a256821b4cc",
            "value": 486296528
          }
        },
        "55fdfca0729742b5a9606b88cbb2f58c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e507e90e67e40e18ebd1dfcaf570fe0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6237cca58e51426da5c043262277893c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "64333764a96141c980d304f0f4b6009b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e1171c0b43d4bd1ba05f88cc7ee512c",
            "placeholder": "​",
            "style": "IPY_MODEL_03c6b59427064dd7aff79ac299f0154c",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "6b2871975c2f4da8865b9a6712ab66b2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c621cfb7bcf4bbf93f9f0cafe354c11": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d9802e791374f19b92af905ade50637": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d935d37c6b2c434a8036b33cb20ebe61",
            "placeholder": "​",
            "style": "IPY_MODEL_17843f701f674c7b836a2d232a81b1ed",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "70084cb51d8d4c769b8bba3924790cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7fe7a616069a405899b51b5e0443e383",
            "placeholder": "​",
            "style": "IPY_MODEL_163caaa69e744e359df1b98f551e936b",
            "value": " 412/412 [00:00&lt;00:00, 51.6kB/s]"
          }
        },
        "7039014ae1f24d6da3fb528b30122076": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78be3b1743e74d8f9c46ad9686ea4d1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48ba9105fa0c4a84abddb5c2188974f8",
            "placeholder": "​",
            "style": "IPY_MODEL_304defd4210344dea1df926daca3b354",
            "value": " 22.7k/22.7k [00:00&lt;00:00, 1.92MB/s]"
          }
        },
        "7c2b06ceb55b46ed9c1e444060a7106a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41fb33a7b9c2498197d830bff6c2fc2b",
            "max": 412,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ce5950fa3cdf44b5a6f6461e1c20de3d",
            "value": 412
          }
        },
        "7ee1c6b3cdf842e0a6e196d7bec9c6e4": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7fe7a616069a405899b51b5e0443e383": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80ab0f773c4a4e3cadd2aeec9d5de9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_36d4caa942934ce892267b17c9677632",
            "placeholder": "​",
            "style": "IPY_MODEL_6237cca58e51426da5c043262277893c",
            "value": "config.json: 100%"
          }
        },
        "86494bab4a8448f090bf2ac7221b02c6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "86fab07cbc724a3c921ac54a20dc184f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3015681206ea445c8a441d38f86abd65",
              "IPY_MODEL_430de3d2c7e64d89b931da4f4f4c015d",
              "IPY_MODEL_d6ecf9ce6b514df89a07ce6e12f0c9c1"
            ],
            "layout": "IPY_MODEL_9347ea6584014eb99612fd52683d3b91"
          }
        },
        "880f5dfce09047df8dc7c798accc166e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e62617bd7b194d7da07032b04fc69ed8",
            "max": 22723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2b5ff5cf648d4503bfd005fd75b0bd98",
            "value": 22723
          }
        },
        "88c3145659364b82a7500247fe48fc5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8eb9535ab8bd4c1fba6b784a79dc7965": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f8d3646a0ae4a8da2ca2a47baa46c1a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fcbf0cf6f1d49548bb0c0081502d49f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_80ab0f773c4a4e3cadd2aeec9d5de9b0",
              "IPY_MODEL_880f5dfce09047df8dc7c798accc166e",
              "IPY_MODEL_1abdbe734d9f4ff68b0622e8d7fc2e38"
            ],
            "layout": "IPY_MODEL_8eb9535ab8bd4c1fba6b784a79dc7965"
          }
        },
        "9347ea6584014eb99612fd52683d3b91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "94ab69d17cc74403816d9ef3ee847ee2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad153ad30f8f485798e898d835d43f6d",
            "placeholder": "​",
            "style": "IPY_MODEL_55fdfca0729742b5a9606b88cbb2f58c",
            "value": "model.safetensors: 100%"
          }
        },
        "993c467478ac462893cdee1ada619cae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_abb316a8a9c445f2bb087c34de97759a",
            "placeholder": "​",
            "style": "IPY_MODEL_0036e20bbb244e398638463dc411bb00",
            "value": "preprocessor_config.json: 100%"
          }
        },
        "9b20c26e11fe4ddeb978790d5dbda4df": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ede8fbc4df74511ad73a7cd5faa4ef0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b625666bee8e43c789a6c0adae62706f",
            "placeholder": "​",
            "style": "IPY_MODEL_c341479863d24a299bd5f91f7505924f",
            "value": " 486M/486M [00:01&lt;00:00, 304MB/s]"
          }
        },
        "a21a2e5fe08743b19089010ab6643f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6d9802e791374f19b92af905ade50637",
              "IPY_MODEL_20359e031e13479185630f6f08c81deb",
              "IPY_MODEL_dad3a562e2d04632b508f7a14540a6e6"
            ],
            "layout": "IPY_MODEL_6b2871975c2f4da8865b9a6712ab66b2"
          }
        },
        "a4794311dbca483ab835bcb8fcdc8b65": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6d4ecffbf66449a837f623669b4f90b",
            "max": 22723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4c920dd08c514eeb9a0a89e8784a85d0",
            "value": 22723
          }
        },
        "abb316a8a9c445f2bb087c34de97759a": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad153ad30f8f485798e898d835d43f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b26e93238a2642428215534fd7c58aa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b625666bee8e43c789a6c0adae62706f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bba839d147104ce3800c7bcd6a8af8a3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0c68530852a4c03822a5eddb7d167ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c341479863d24a299bd5f91f7505924f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c70dc8fa9c75469a8c69b04b7b345fd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0ab87c4962c24854977b1720b6e397e5",
              "IPY_MODEL_a4794311dbca483ab835bcb8fcdc8b65",
              "IPY_MODEL_78be3b1743e74d8f9c46ad9686ea4d1f"
            ],
            "layout": "IPY_MODEL_9b20c26e11fe4ddeb978790d5dbda4df"
          }
        },
        "c8d2e3fa7c9645579ef66da0fc30c4ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_26194c7196434dd690fc5f3c366f8cdf",
            "placeholder": "​",
            "style": "IPY_MODEL_b26e93238a2642428215534fd7c58aa1",
            "value": " 412/412 [00:00&lt;00:00, 51.8kB/s]"
          }
        },
        "cbe04538156045e3aa920af8473d7be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_94ab69d17cc74403816d9ef3ee847ee2",
              "IPY_MODEL_5442d68d0851435c83c97d3b87b97673",
              "IPY_MODEL_9ede8fbc4df74511ad73a7cd5faa4ef0"
            ],
            "layout": "IPY_MODEL_bba839d147104ce3800c7bcd6a8af8a3"
          }
        },
        "cbfb1e3fd58e4c79a28df0a968c74132": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce5950fa3cdf44b5a6f6461e1c20de3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d44944a1b684496781c9e2def4dd8347": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_64333764a96141c980d304f0f4b6009b",
              "IPY_MODEL_7c2b06ceb55b46ed9c1e444060a7106a",
              "IPY_MODEL_c8d2e3fa7c9645579ef66da0fc30c4ac"
            ],
            "layout": "IPY_MODEL_7ee1c6b3cdf842e0a6e196d7bec9c6e4"
          }
        },
        "d6ecf9ce6b514df89a07ce6e12f0c9c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_86494bab4a8448f090bf2ac7221b02c6",
            "placeholder": "​",
            "style": "IPY_MODEL_05a7492588824d1183fbc5f2057c7097",
            "value": " 486M/486M [00:01&lt;00:00, 330MB/s]"
          }
        },
        "d935d37c6b2c434a8036b33cb20ebe61": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dad3a562e2d04632b508f7a14540a6e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1a3a9e38df94c03aacfb1a0ab07d0f6",
            "placeholder": "​",
            "style": "IPY_MODEL_6c621cfb7bcf4bbf93f9f0cafe354c11",
            "value": " 412/412 [00:00&lt;00:00, 48.5kB/s]"
          }
        },
        "e1a3a9e38df94c03aacfb1a0ab07d0f6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5a3d9f8c76540d0845eda6990761822": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e62617bd7b194d7da07032b04fc69ed8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f3b7cd4fde9441599d0f88d16d8d50ea": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f696fd99ca4c45fb8072d9ccf4ef18b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f6d4ecffbf66449a837f623669b4f90b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb869f720924426eb8cf0d86e80a5fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
