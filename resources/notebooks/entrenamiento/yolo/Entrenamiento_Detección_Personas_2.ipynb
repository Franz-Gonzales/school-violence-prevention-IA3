{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNg7zX/oasdkKqKNtyDPgxe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xelZeg4P-3iR","executionInfo":{"status":"ok","timestamp":1747497981849,"user_tz":240,"elapsed":117245,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}},"outputId":"aab2a1d3-e33d-476b-bb2c-1927474bc7c9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.137-py3-none-any.whl.metadata (37 kB)\n","Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n","Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n","Downloading ultralytics-8.3.137-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m41.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.137 ultralytics-thop-2.0.14\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n","Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m12.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n","Installing collected packages: lightning-utilities, torchmetrics\n","Successfully installed lightning-utilities-0.14.3 torchmetrics-1.7.1\n"]}],"source":["!pip install ultralytics # Puedes especificar una versión si lo deseas\n","!pip install torchmetrics # Puedes especificar una versión si lo deseas"]},{"cell_type":"code","source":["# ===================== CONFIGURACIÓN INICIAL =====================\n","import os\n","import sys\n","import time\n","import math\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from pathlib import Path\n","from tqdm.auto import tqdm\n","from datetime import datetime\n","from PIL import Image\n","import cv2\n","import yaml\n","import json\n","import torch\n","import torchvision\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from torch.optim.lr_scheduler import (\n","    OneCycleLR,\n","    ReduceLROnPlateau,\n","    CosineAnnealingLR,\n","    CosineAnnealingWarmRestarts\n",")\n","from torchvision import transforms\n","from torchmetrics.detection.mean_ap import MeanAveragePrecision\n","from sklearn.metrics import (\n","    confusion_matrix,\n","    classification_report,\n","    precision_recall_curve,\n","    average_precision_score,\n","    roc_curve,\n","    roc_auc_score\n",")\n","from ultralytics import YOLO\n","import gc\n","\n","import wandb\n","import shutil"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9DsIE15z_XdF","executionInfo":{"status":"ok","timestamp":1747498069173,"user_tz":240,"elapsed":18926,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}},"outputId":"42035e41-5e2f-40db-c5a3-fa39fa19e089"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}]},{"cell_type":"code","source":["# Montar Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"95eoV-lh_hNt","executionInfo":{"status":"ok","timestamp":1747498100047,"user_tz":240,"elapsed":24373,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}},"outputId":"5561cef0-216c-499f-8384-7338652f671d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import wandb\n","\n","# Iniciar Weights & Biases para monitoreo\n","apy_key = '30a7a34eb742653f47d9208966cbed1dc6f31e2d' # Replace with your actual API key\n","wandb.login(key=apy_key) # Log in using the API key\n","wandb.init(project=\"yolov11_person_detectionV2\", name=\"training_run\", config={\"seed\": 42})\n","\n","# Rest of your code...\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f\"Utilizando dispositivo: {device}\")\n","print(f\"PyTorch versión: {torch.__version__}\")\n","print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n","if torch.cuda.is_available():\n","    print(f\"CUDA versión: {torch.version.cuda}\")\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Memoria GPU total: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n","\n","# Configurar semillas para reproducibilidad\n","def seed_everything(seed=42):\n","    random.seed(seed)\n","    os.environ['PYTHONHASHSEED'] = str(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","seed_everything(42)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":315},"id":"RfpmIjZG_i4k","executionInfo":{"status":"ok","timestamp":1747498109455,"user_tz":240,"elapsed":3516,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}},"outputId":"bc30fa1d-a02e-40d3-c9a4-139ddbf4376b"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n","\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mgonzalesfranzreinaldo\u001b[0m (\u001b[33mgonzalesfranzreinaldo-usfx\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Tracking run with wandb version 0.19.11"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Run data is saved locally in <code>/content/wandb/run-20250517_160828-mo447k9h</code>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["Syncing run <strong><a href='https://wandb.ai/gonzalesfranzreinaldo-usfx/yolov11_person_detectionV2/runs/mo447k9h' target=\"_blank\">training_run</a></strong> to <a href='https://wandb.ai/gonzalesfranzreinaldo-usfx/yolov11_person_detectionV2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View project at <a href='https://wandb.ai/gonzalesfranzreinaldo-usfx/yolov11_person_detectionV2' target=\"_blank\">https://wandb.ai/gonzalesfranzreinaldo-usfx/yolov11_person_detectionV2</a>"]},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[" View run at <a href='https://wandb.ai/gonzalesfranzreinaldo-usfx/yolov11_person_detectionV2/runs/mo447k9h' target=\"_blank\">https://wandb.ai/gonzalesfranzreinaldo-usfx/yolov11_person_detectionV2/runs/mo447k9h</a>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Utilizando dispositivo: cuda\n","PyTorch versión: 2.6.0+cu124\n","CUDA disponible: True\n","CUDA versión: 12.4\n","GPU: Tesla T4\n","Memoria GPU total: 15.83 GB\n"]}]},{"cell_type":"code","source":["# ===================== CONFIGURACIÓN DE DIRECTORIOS =====================\n","DRIVE_PATH = '/content/drive/MyDrive'\n","DATASET_PATH = f'{DRIVE_PATH}/dataset_people'\n","OUTPUT_PATH = f'{DRIVE_PATH}/Trabajo_IA-3_FGS/yolov11_training'\n","CHECKPOINTS_PATH = f'{OUTPUT_PATH}/checkpoints'\n","LOGS_PATH = f'{OUTPUT_PATH}/logs'\n","RESULTS_PATH = f'{OUTPUT_PATH}/results'\n","\n","# Crear directorios de salida\n","for path in [OUTPUT_PATH, CHECKPOINTS_PATH, LOGS_PATH, RESULTS_PATH]:\n","    os.makedirs(path, exist_ok=True)\n","print(f\"Estructura de directorios creada en: {OUTPUT_PATH}\")\n","\n","# Verificar acceso a Google Drive\n","if not os.path.exists(DRIVE_PATH):\n","    raise FileNotFoundError(\"Google Drive no está montado correctamente\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pmteU5tz_lON","executionInfo":{"status":"ok","timestamp":1747498158939,"user_tz":240,"elapsed":15,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}},"outputId":"d2b07555-16a7-4193-9e47-979f447e91f4"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Estructura de directorios creada en: /content/drive/MyDrive/Trabajo_IA-3_FGS/yolov11_training\n"]}]},{"cell_type":"code","source":["# ===================== VERIFICACIÓN Y ANÁLISIS DEL DATASET =====================\n","print(\"\\n=== ANÁLISIS DEL DATASET ===\")\n","print(f\"Ruta del dataset: {DATASET_PATH}\")\n","\n","def count_files(directory, extension='.jpg'):\n","    return len([f for f in os.listdir(directory) if f.endswith(extension)])\n","\n","# Verificar estructura del dataset\n","images_train = f\"{DATASET_PATH}/images/train\"\n","images_val = f\"{DATASET_PATH}/images/val\"\n","images_test = f\"{DATASET_PATH}/images/test\"\n","labels_train = f\"{DATASET_PATH}/labels/train\"\n","labels_val = f\"{DATASET_PATH}/labels/val\"\n","labels_test = f\"{DATASET_PATH}/labels/test\"\n","\n","# Contar imágenes y etiquetas\n","train_images_count = count_files(images_train, '.jpg')\n","val_images_count = count_files(images_val, '.jpg')\n","test_images_count = count_files(images_test, '.jpg')\n","train_labels_count = count_files(labels_train, '.txt')\n","val_labels_count = count_files(labels_val, '.txt')\n","test_labels_count = count_files(labels_test, '.txt')\n","\n","# Verificación de integridad\n","print(f\"Imágenes de entrenamiento: {train_images_count} (etiquetas: {train_labels_count})\")\n","print(f\"Imágenes de validación: {val_images_count} (etiquetas: {val_labels_count})\")\n","print(f\"Imágenes de prueba: {test_images_count} (etiquetas: {test_labels_count})\")\n","if train_images_count != train_labels_count or val_images_count != val_labels_count or test_images_count != test_labels_count:\n","    raise ValueError(\"Mismatch entre imágenes y etiquetas\")\n","\n","# Analizar etiquetas\n","def analyze_labels(labels_dir, sample_size=100):\n","    label_files = [os.path.join(labels_dir, f) for f in os.listdir(labels_dir) if f.endswith('.txt')]\n","    if not label_files:\n","        return None\n","    sample_size = min(sample_size, len(label_files))\n","    selected_files = np.random.choice(label_files, sample_size, replace=False)\n","    boxes_per_image = []\n","    box_sizes = []\n","    for file_path in selected_files:\n","        with open(file_path, 'r') as f:\n","            lines = f.readlines()\n","            boxes_per_image.append(len(lines))\n","            for line in lines:\n","                values = line.strip().split()\n","                if len(values) == 5:\n","                    w, h = float(values[3]), float(values[4])\n","                    box_sizes.append(w * h)\n","    return {\n","        'avg_boxes_per_image': np.mean(boxes_per_image),\n","        'median_boxes_per_image': np.median(boxes_per_image),\n","        'max_boxes_per_image': np.max(boxes_per_image),\n","        'min_boxes_per_image': np.min(boxes_per_image),\n","        'avg_box_size': np.mean(box_sizes),\n","        'min_box_size': np.min(box_sizes),\n","        'max_box_size': np.max(box_sizes)\n","    }\n","\n","print(\"\\n=== ANÁLISIS DE ETIQUETAS ===\")\n","train_stats = analyze_labels(labels_train)\n","if train_stats:\n","    print(\"Estadísticas del conjunto de entrenamiento:\")\n","    print(f\"- Promedio de cajas por imagen: {train_stats['avg_boxes_per_image']:.2f}\")\n","    print(f\"- Mediana de cajas por imagen: {train_stats['median_boxes_per_image']:.2f}\")\n","    print(f\"- Rango de cajas por imagen: {train_stats['min_boxes_per_image']} - {train_stats['max_boxes_per_image']}\")\n","    print(f\"- Tamaño promedio de caja: {train_stats['avg_box_size']:.4f}\")\n","    print(f\"- Rango de tamaños de caja: {train_stats['min_box_size']:.4f} - {train_stats['max_box_size']:.4f}\")\n","\n","# Verificar archivo data.yaml\n","yaml_path = f\"{DATASET_PATH}/data.yaml\"\n","if os.path.exists(yaml_path):\n","    with open(yaml_path, 'r') as f:\n","        data_yaml = yaml.safe_load(f)\n","    print(\"\\nContenido del archivo data.yaml:\")\n","    print(data_yaml)\n","else:\n","    print(\"\\nCreando archivo data.yaml...\")\n","    data_yaml = {\n","        'train': './images/train',\n","        'val': './images/val',\n","        'test': './images/test',\n","        'nc': 1,\n","        'names': ['persona'],\n","        'transforms': {'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}}\n","    }\n","    with open(yaml_path, 'w') as f:\n","        yaml.dump(data_yaml, f)\n","    print(\"Archivo data.yaml creado con éxito.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aBvgipDk_wDU","executionInfo":{"status":"ok","timestamp":1747498336950,"user_tz":240,"elapsed":163696,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}},"outputId":"4f549637-65fd-4ca8-ca2f-738364c8400e"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== ANÁLISIS DEL DATASET ===\n","Ruta del dataset: /content/drive/MyDrive/dataset_people\n","Imágenes de entrenamiento: 10200 (etiquetas: 10200)\n","Imágenes de validación: 2500 (etiquetas: 2500)\n","Imágenes de prueba: 1200 (etiquetas: 1200)\n","\n","=== ANÁLISIS DE ETIQUETAS ===\n","Estadísticas del conjunto de entrenamiento:\n","- Promedio de cajas por imagen: 2.47\n","- Mediana de cajas por imagen: 2.00\n","- Rango de cajas por imagen: 1 - 8\n","- Tamaño promedio de caja: 0.0734\n","- Rango de tamaños de caja: 0.0084 - 0.2655\n","\n","Contenido del archivo data.yaml:\n","{'names': ['persona'], 'nc': 1, 'test': './images/test', 'train': './images/train', 'val': './images/val'}\n"]}]},{"cell_type":"code","source":["# ===================== VISUALIZACIÓN DE MUESTRAS DEL DATASET =====================\n","def visualize_sample_images(images_dir, labels_dir, num_samples=12):\n","    image_files = sorted([f for f in os.listdir(images_dir) if f.endswith('.jpg')])\n","    if not image_files:\n","        print(f\"No se encontraron imágenes en {images_dir}\")\n","        return\n","    indices = np.random.choice(len(image_files), min(num_samples, len(image_files)), replace=False)\n","    fig, axes = plt.subplots(3, 4, figsize=(20, 15))\n","    axes = axes.flatten()\n","    for i, idx in enumerate(indices):\n","        img_name = image_files[idx]\n","        img_path = os.path.join(images_dir, img_name)\n","        label_path = os.path.join(labels_dir, img_name.replace('.jpg', '.txt'))\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        if os.path.exists(label_path):\n","            with open(label_path, 'r') as f:\n","                for line in f.readlines():\n","                    values = line.strip().split()\n","                    if len(values) == 5:\n","                        x_center = float(values[1]) * img.shape[1]\n","                        y_center = float(values[2]) * img.shape[0]\n","                        width = float(values[3]) * img.shape[1]\n","                        height = float(values[4]) * img.shape[0]\n","                        x1 = int(x_center - width / 2)\n","                        y1 = int(y_center - height / 2)\n","                        x2 = int(x_center + width / 2)\n","                        y2 = int(y_center + height / 2)\n","                        cv2.rectangle(img, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","                        cv2.putText(img, \"Persona\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","        axes[i].imshow(img)\n","        axes[i].set_title(f\"Imagen: {img_name}\")\n","        axes[i].axis('off')\n","    plt.tight_layout()\n","    plt.savefig(f\"{RESULTS_PATH}/dataset_samples.png\")\n","    plt.show()\n","\n","print(\"\\n=== VISUALIZACIÓN DE MUESTRAS ===\")\n","visualize_sample_images(images_train, labels_train)"],"metadata":{"id":"Xv-xk-eu_zAU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== PREPARACIÓN DEL ARCHIVO DE CONFIGURACIÓN =====================\n","def create_custom_yaml():\n","    custom_yaml_path = f\"{OUTPUT_PATH}/custom_dataset.yaml\"\n","    data = {\n","        'path': DATASET_PATH,\n","        'train': 'images/train',\n","        'val': 'images/val',\n","        'test': 'images/test',\n","        'nc': 1,\n","        'names': ['persona'],\n","        'transforms': {'normalize': {'mean': [0.485, 0.456, 0.406], 'std': [0.229, 0.224, 0.225]}},\n","        'roboflow': {\n","            'workspace': 'proyecto-violencia-escolar',\n","            'project': 'deteccion-personas',\n","            'version': 1,\n","            'license': 'MIT',\n","            'url': 'https://universe.roboflow.com/proyecto-violencia-escolar/deteccion-personas/dataset/1'\n","        }\n","    }\n","    with open(custom_yaml_path, 'w') as f:\n","        yaml.dump(data, f, sort_keys=False)\n","    print(f\"Archivo de configuración creado en: {custom_yaml_path}\")\n","    return custom_yaml_path\n","\n","custom_yaml_path = create_custom_yaml()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c0RhodgT_1S0","executionInfo":{"status":"ok","timestamp":1747498448305,"user_tz":240,"elapsed":572,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}},"outputId":"d40f1f16-9fc0-4231-9f97-9ff05caa863e"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Archivo de configuración creado en: /content/drive/MyDrive/Trabajo_IA-3_FGS/yolov11_training/custom_dataset.yaml\n"]}]},{"cell_type":"code","source":["# ===================== FUNCIONES DE MÉTRICAS Y EVALUACIÓN =====================\n","def compute_confusion_matrix(model, test_dir, test_labels_dir, conf_threshold=0.4, iou_threshold=0.5):\n","    \"\"\"Calcula la matriz de confusión para detección de objetos.\"\"\"\n","    model.eval()\n","    TP, FP, FN = 0, 0, 0\n","    image_files = [f for f in os.listdir(test_dir) if f.endswith('.jpg')]\n","    for img_file in tqdm(image_files, desc=\"Calculando matriz de confusión\"):\n","        img_path = os.path.join(test_dir, img_file)\n","        label_path = os.path.join(test_labels_dir, img_file.replace('.jpg', '.txt'))\n","        gt_boxes = []\n","        if os.path.exists(label_path):\n","            with open(label_path, 'r') as f:\n","                for line in f.readlines():\n","                    values = line.strip().split()\n","                    if len(values) == 5:\n","                        _, x, y, w, h = map(float, values)\n","                        gt_boxes.append([x, y, w, h])\n","        with torch.no_grad():\n","            results = model.predict(img_path, conf=conf_threshold, iou=iou_threshold, verbose=False)[0]\n","        pred_boxes = [box.xywhn[0].cpu().numpy() for box in results.boxes]\n","        matched_gt = set()\n","        for pred in pred_boxes:\n","            px, py, pw, ph = pred\n","            matched = False\n","            for i, gt in enumerate(gt_boxes):\n","                if i in matched_gt:\n","                    continue\n","                gx, gy, gw, gh = gt\n","                x_left = max(px - pw/2, gx - gw/2)\n","                y_top = max(py - ph/2, gy - gh/2)\n","                x_right = min(px + pw/2, gx + gw/2)\n","                y_bottom = min(py + ph/2, gy + gh/2)\n","                if x_right < x_left or y_bottom < y_top:\n","                    continue\n","                intersection = (x_right - x_left) * (y_bottom - y_top)\n","                p_area, g_area = pw * ph, gw * gh\n","                iou = intersection / (p_area + g_area - intersection)\n","                if iou >= iou_threshold:\n","                    matched_gt.add(i)\n","                    matched = True\n","                    break\n","            if matched:\n","                TP += 1\n","            else:\n","                FP += 1\n","        FN += len(gt_boxes) - len(matched_gt)\n","    cm = np.array([[TP, FP], [FN, 0]])  # TN no aplicable\n","    return cm\n","\n","def plot_confusion_matrix(cm, save_path=None):\n","    plt.figure(figsize=(8, 6))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n","    plt.xlabel('Predicciones')\n","    plt.ylabel('Ground Truth')\n","    plt.title('Matriz de Confusión')\n","    if save_path:\n","        plt.savefig(save_path)\n","    plt.show()\n","\n","def calculate_precision_recall_thresholds(model, test_dir, test_labels_dir, thresholds=np.arange(0.05, 1.0, 0.05)):\n","    precisions, recalls = [], []\n","    total_gt = sum(len(open(os.path.join(test_labels_dir, f)).readlines())\n","                   for f in os.listdir(test_labels_dir) if f.endswith('.txt'))\n","    if total_gt == 0:\n","        print(\"No se encontraron anotaciones en el conjunto de prueba\")\n","        return [], []\n","    for threshold in tqdm(thresholds, desc=\"Calculando PR curve\"):\n","        correct_detections = 0\n","        total_predictions = 0\n","        for img_file in os.listdir(test_dir):\n","            if not img_file.endswith('.jpg'):\n","                continue\n","            img_path = os.path.join(test_dir, img_file)\n","            label_path = os.path.join(test_labels_dir, img_file.replace('.jpg', '.txt'))\n","            gt_boxes = []\n","            if os.path.exists(label_path):\n","                with open(label_path, 'r') as f:\n","                    for line in f.readlines():\n","                        values = line.strip().split()\n","                        if len(values) == 5:\n","                            _, x, y, w, h = map(float, values)\n","                            gt_boxes.append([x, y, w, h])\n","            with torch.no_grad():\n","                results = model.predict(img_path, conf=threshold, verbose=False)[0]\n","            pred_boxes = [box.xywhn[0].cpu().numpy() for box in results.boxes]\n","            total_predictions += len(pred_boxes)\n","            matched_gt = set()\n","            for pred in pred_boxes:\n","                px, py, pw, ph = pred\n","                for i, gt in enumerate(gt_boxes):\n","                    if i in matched_gt:\n","                        continue\n","                    gx, gy, gw, gh = gt\n","                    x_left = max(px - pw/2, gx - gw/2)\n","                    y_top = max(py - ph/2, gy - gh/2)\n","                    x_right = min(px + pw/2, gx + gw/2)\n","                    y_bottom = min(py + ph/2, gy + gh/2)\n","                    if x_right < x_left or y_bottom < y_top:\n","                        continue\n","                    intersection = (x_right - x_left) * (y_bottom - y_top)\n","                    p_area, g_area = pw * ph, gw * gh\n","                    iou = intersection / (p_area + g_area - intersection)\n","                    if iou >= 0.5:\n","                        matched_gt.add(i)\n","                        correct_detections += 1\n","                        break\n","        precision = correct_detections / total_predictions if total_predictions > 0 else 1.0\n","        recall = correct_detections / total_gt\n","        precisions.append(precision)\n","        recalls.append(recall)\n","    return thresholds, precisions, recalls\n","\n","def calculate_roc_curve(model, test_dir, test_labels_dir, thresholds=np.arange(0.05, 1.0, 0.05)):\n","    tprs, fprs = [], []\n","    total_gt = sum(len(open(os.path.join(test_labels_dir, f)).readlines())\n","                   for f in os.listdir(test_labels_dir) if f.endswith('.txt'))\n","    for threshold in tqdm(thresholds, desc=\"Calculando ROC curve\"):\n","        TP, FP, FN = 0, 0, 0\n","        for img_file in os.listdir(test_dir):\n","            if not img_file.endswith('.jpg'):\n","                continue\n","            img_path = os.path.join(test_dir, img_file)\n","            label_path = os.path.join(test_labels_dir, img_file.replace('.jpg', '.txt'))\n","            gt_boxes = []\n","            if os.path.exists(label_path):\n","                with open(label_path, 'r') as f:\n","                    for line in f.readlines():\n","                        values = line.strip().split()\n","                        if len(values) == 5:\n","                            _, x, y, w, h = map(float, values)\n","                            gt_boxes.append([x, y, w, h])\n","            with torch.no_grad():\n","                results = model.predict(img_path, conf=threshold, verbose=False)[0]\n","            pred_boxes = [box.xywhn[0].cpu().numpy() for box in results.boxes]\n","            matched_gt = set()\n","            for pred in pred_boxes:\n","                px, py, pw, ph = pred\n","                matched = False\n","                for i, gt in enumerate(gt_boxes):\n","                    if i in matched_gt:\n","                        continue\n","                    gx, gy, gw, gh = gt\n","                    x_left = max(px - pw/2, gx - gw/2)\n","                    y_top = max(py - ph/2, gy - gh/2)\n","                    x_right = min(px + pw/2, gx + gw/2)\n","                    y_bottom = min(py + ph/2, gy + gh/2)\n","                    if x_right < x_left or y_bottom < y_top:\n","                        continue\n","                    intersection = (x_right - x_left) * (y_bottom - y_top)\n","                    p_area, g_area = pw * ph, gw * gh\n","                    iou = intersection / (p_area + g_area - intersection)\n","                    if iou >= 0.5:\n","                        matched_gt.add(i)\n","                        matched = True\n","                        break\n","                if matched:\n","                    TP += 1\n","                else:\n","                    FP += 1\n","            FN += len(gt_boxes) - len(matched_gt)\n","        tpr = TP / (TP + FN) if (TP + FN) > 0 else 0\n","        fpr = FP / (FP + 0) if FP > 0 else 0\n","        tprs.append(tpr)\n","        fprs.append(fpr)\n","    auc = np.trapz(tprs, fprs)\n","    return thresholds, tprs, fprs, auc\n","\n","def plot_precision_recall_curve(precisions, recalls, ap, save_path=None):\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(recalls, precisions, 'b-', label=f'AP = {ap:.4f}')\n","    plt.xlabel('Recall')\n","    plt.ylabel('Precision')\n","    plt.title('Curva de Precisión-Recall')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.grid(True)\n","    plt.legend()\n","    if save_path:\n","        plt.savefig(save_path)\n","    plt.show()\n","\n","def plot_roc_curve(tprs, fprs, auc, save_path=None):\n","    plt.figure(figsize=(10, 6))\n","    plt.plot(fprs, tprs, 'b-', label=f'ROC (AUC = {auc:.4f})')\n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlabel('FPR')\n","    plt.ylabel('TPR')\n","    plt.title('Curva ROC')\n","    plt.grid(True)\n","    plt.legend()\n","    if save_path:\n","        plt.savefig(save_path)\n","    plt.show()\n","\n","def plot_learning_curves(results_df, save_path=None, phase='transfer'):\n","    plt.figure(figsize=(15, 10))\n","    box_loss = results_df['         train/box_loss'].values\n","    cls_loss = results_df['         train/cls_loss'].values\n","    dfl_loss = results_df['         train/dfl_loss'].values\n","    precision = results_df['       metrics/precision(B)'].values\n","    recall = results_df['          metrics/recall(B)'].values\n","    map50 = results_df['       metrics/mAP50(B)'].values\n","    map50_95 = results_df['    metrics/mAP50-95(B)'].values\n","    f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n","\n","    plt.subplot(2, 2, 1)\n","    plt.plot(box_loss, label='Box Loss')\n","    plt.plot(cls_loss, label='Class Loss')\n","    plt.plot(dfl_loss, label='DFL Loss')\n","    plt.xlabel('Época')\n","    plt.ylabel('Pérdida')\n","    plt.title(f'Evolución de las Pérdidas ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.subplot(2, 2, 2)\n","    plt.plot(map50, label='mAP@0.5')\n","    plt.plot(map50_95, label='mAP@0.5:0.95')\n","    plt.xlabel('Época')\n","    plt.ylabel('mAP')\n","    plt.title(f'Evolución del mAP ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.subplot(2, 2, 3)\n","    plt.plot(precision, label='Precisión')\n","    plt.plot(recall, label='Recall')\n","    plt.xlabel('Época')\n","    plt.ylabel('Valor')\n","    plt.title(f'Precisión y Recall ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.subplot(2, 2, 4)\n","    plt.plot(f1_score, label='F1-Score')\n","    plt.xlabel('Época')\n","    plt.ylabel('F1-Score')\n","    plt.title(f'Evolución del F1-Score ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    if save_path:\n","        plt.savefig(save_path)\n","    plt.show()\n","\n","def plot_model_performance(metrics, save_path=None):\n","    plt.figure(figsize=(10, 6))\n","    sns.barplot(x=['Precisión', 'Recall', 'F1-Score'], y=[metrics['precision'], metrics['recall'], metrics['f1']])\n","    plt.ylim(0, 1)\n","    plt.title('Métricas de Rendimiento del Modelo')\n","    plt.ylabel('Valor')\n","    for i, v in enumerate([metrics['precision'], metrics['recall'], metrics['f1']]):\n","        plt.text(i, v + 0.02, f'{v:.4f}', ha='center')\n","    if save_path:\n","        plt.savefig(save_path)\n","    plt.show()"],"metadata":{"id":"5gDI1U1fAkxN","executionInfo":{"status":"ok","timestamp":1747498451471,"user_tz":240,"elapsed":87,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# ===================== CONFIGURACIÓN GENERAL =====================\n","def get_optimal_batch_size(vram_limit=16):\n","    vram = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0\n","    return 8 if vram < vram_limit else 16\n","\n","CONFIG = {\n","    'model_type': 'yolo11n',\n","    'img_size': 640,\n","    # 'batch_size': get_optimal_batch_size(),\n","    'batch_size': 16,\n","    'num_workers': 4,\n","    'epochs_transfer': 30, # Épocas para transfer learning\n","    'epochs_finetune': 50, # Épocas para fine-tuning\n","    'patience': 8,  # Early stopping patience\n","    'classes': ['persona'],   # Sólo una clase: persona\n","    'pretrained_weights': 'yolo11n.pt',   # Pesos pre-entrenados\n","    'learning_rate_transfer': 0.001,\n","    'learning_rate_finetune': 0.0001,\n","    'weight_decay': 0.0005,\n","    'momentum': 0.937,\n","    'save_period': 5  # Guardar cada 5 épocas\n","}"],"metadata":{"id":"IC918-ieAnVM","executionInfo":{"status":"ok","timestamp":1747498454557,"user_tz":240,"elapsed":12,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}}},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":["# ENTRENAMIENTO CON FINE TUNIGN V2"],"metadata":{"id":"WSKqAbHLAp9c"}},{"cell_type":"code","source":["\n","# ===================== INICIANDO ENTRENAMIENTO: FINE-TUNING ITERACIÓN 2 =====================\n","CONFIG = {\n","    'model_type': 'yolo11n',\n","    'img_size': 640,  # Aumentado para mejorar localización\n","    'batch_size': 8,  # Reducido para imgsz=896\n","    'epochs_finetune': 50,\n","    'learning_rate_finetune': 0.00005,  # Reducido para ajustes más finos\n","    'patience': 10,  # Más estricto\n","    'num_workers': 4,\n","    'momentum': 0.937,\n","    'weight_decay': 0.0001,\n","    'save_period': 10\n","}\n","\n","print(\"\\n=== INICIANDO ENTRENAMIENTO: FINE-TUNING ITERACIÓN 2 ===\")\n","print(f\"Configuración de entrenamiento:\")\n","print(f\"- Modelo: {CONFIG['model_type']}\")\n","print(f\"- Tamaño de imagen: {CONFIG['img_size']}x{CONFIG['img_size']}\")\n","print(f\"- Batch size: {CONFIG['batch_size']}\")\n","print(f\"- Épocas: {CONFIG['epochs_finetune']}\")\n","print(f\"- Learning rate: {CONFIG['learning_rate_finetune']}\")\n","\n","# Cargar modelo del fine-tuning anterior\n","finetune_model_path = f\"{OUTPUT_PATH}/fine_tuning_final.pt\"\n","ft_model = YOLO(finetune_model_path)\n","\n","# Configurar el entrenamiento\n","finetune_results = ft_model.train(\n","    data=custom_yaml_path,\n","    epochs=CONFIG['epochs_finetune'],\n","    imgsz=CONFIG['img_size'],\n","    batch=CONFIG['batch_size'],\n","    patience=CONFIG['patience'],\n","    save=True,\n","    save_period=CONFIG['save_period'],\n","    device=0 if torch.cuda.is_available() else 'cpu',\n","    workers=CONFIG['num_workers'],\n","    lr0=CONFIG['learning_rate_finetune'],\n","    lrf=CONFIG['learning_rate_finetune'] / 10,\n","    momentum=CONFIG['momentum'],\n","    weight_decay=CONFIG['weight_decay'],\n","    warmup_epochs=3.0,\n","    warmup_momentum=0.8,\n","    warmup_bias_lr=0.1,\n","    box=10.0,  # Aumentado para priorizar localización\n","    dfl=1.0,  # Reducido para equilibrar\n","    hsv_h=0.02,\n","    hsv_s=0.6,  # Reducido para estabilidad\n","    hsv_v=0.5,\n","    degrees=10.0,\n","    translate=0.2,\n","    scale=0.5,  # Reducido para estabilidad\n","    mixup=0.1,  # Añadido para diversidad\n","    close_mosaic=15,  # Más tiempo sin mosaic\n","    project=OUTPUT_PATH,\n","    name=\"fine_tuning_iter2\",\n","    exist_ok=True,\n","    pretrained=True,\n","    optimizer=\"AdamW\",\n","    verbose=True,\n","    seed=42,\n","    deterministic=True,\n","    cos_lr=True,\n","    freeze=15  # Más capas congeladas\n",")\n","\n","\n","# Guardar resultados\n","finetune_iter2_model_path = f\"{OUTPUT_PATH}/fine_tuning_iter2_final.pt\"\n","ft_model.save(finetune_iter2_model_path)\n","print(f\"Modelo de fine-tuning iteración 2 guardado en: {finetune_iter2_model_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"054A5CfOArDy","outputId":"3615c19b-3500-45c6-811a-afe436ec11cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== INICIANDO ENTRENAMIENTO: FINE-TUNING ITERACIÓN 2 ===\n","Configuración de entrenamiento:\n","- Modelo: yolo11n\n","- Tamaño de imagen: 640x640\n","- Batch size: 8\n","- Épocas: 50\n","- Learning rate: 5e-05\n","Ultralytics 8.3.137 🚀 Python-3.11.12 torch-2.6.0+cu124 CUDA:0 (Tesla T4, 15095MiB)\n","\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=10.0, cache=False, cfg=None, classes=None, close_mosaic=15, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=True, cutmix=0.0, data=/content/drive/MyDrive/Trabajo_IA-3_FGS/yolov11_training/custom_dataset.yaml, degrees=10.0, deterministic=True, device=0, dfl=1.0, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=True, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=15, half=False, hsv_h=0.02, hsv_s=0.6, hsv_v=0.5, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=5e-05, lrf=5e-06, mask_ratio=4, max_det=300, mixup=0.1, mode=train, model=/content/drive/MyDrive/Trabajo_IA-3_FGS/yolov11_training/fine_tuning_final.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=fine_tuning_iter2, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=10, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/content/drive/MyDrive/Trabajo_IA-3_FGS/yolov11_training, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/drive/MyDrive/Trabajo_IA-3_FGS/yolov11_training/fine_tuning_iter2, save_frames=False, save_json=False, save_period=10, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.2, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0001, workers=4, workspace=None\n","\n","                   from  n    params  module                                       arguments                     \n","  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n","  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n","  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n","  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n","  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n","  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n","  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n","  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n","  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n","  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n"," 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n"," 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n"," 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n"," 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n"," 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n"," 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n"," 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n"," 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n"," 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n"," 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n","YOLO11n summary: 181 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n","\n","Transferred 499/499 items from pretrained weights\n","Freezing layer 'model.0.conv.weight'\n","Freezing layer 'model.0.bn.weight'\n","Freezing layer 'model.0.bn.bias'\n","Freezing layer 'model.1.conv.weight'\n","Freezing layer 'model.1.bn.weight'\n","Freezing layer 'model.1.bn.bias'\n","Freezing layer 'model.2.cv1.conv.weight'\n","Freezing layer 'model.2.cv1.bn.weight'\n","Freezing layer 'model.2.cv1.bn.bias'\n","Freezing layer 'model.2.cv2.conv.weight'\n","Freezing layer 'model.2.cv2.bn.weight'\n","Freezing layer 'model.2.cv2.bn.bias'\n","Freezing layer 'model.2.m.0.cv1.conv.weight'\n","Freezing layer 'model.2.m.0.cv1.bn.weight'\n","Freezing layer 'model.2.m.0.cv1.bn.bias'\n","Freezing layer 'model.2.m.0.cv2.conv.weight'\n","Freezing layer 'model.2.m.0.cv2.bn.weight'\n","Freezing layer 'model.2.m.0.cv2.bn.bias'\n","Freezing layer 'model.3.conv.weight'\n","Freezing layer 'model.3.bn.weight'\n","Freezing layer 'model.3.bn.bias'\n","Freezing layer 'model.4.cv1.conv.weight'\n","Freezing layer 'model.4.cv1.bn.weight'\n","Freezing layer 'model.4.cv1.bn.bias'\n","Freezing layer 'model.4.cv2.conv.weight'\n","Freezing layer 'model.4.cv2.bn.weight'\n","Freezing layer 'model.4.cv2.bn.bias'\n","Freezing layer 'model.4.m.0.cv1.conv.weight'\n","Freezing layer 'model.4.m.0.cv1.bn.weight'\n","Freezing layer 'model.4.m.0.cv1.bn.bias'\n","Freezing layer 'model.4.m.0.cv2.conv.weight'\n","Freezing layer 'model.4.m.0.cv2.bn.weight'\n","Freezing layer 'model.4.m.0.cv2.bn.bias'\n","Freezing layer 'model.5.conv.weight'\n","Freezing layer 'model.5.bn.weight'\n","Freezing layer 'model.5.bn.bias'\n","Freezing layer 'model.6.cv1.conv.weight'\n","Freezing layer 'model.6.cv1.bn.weight'\n","Freezing layer 'model.6.cv1.bn.bias'\n","Freezing layer 'model.6.cv2.conv.weight'\n","Freezing layer 'model.6.cv2.bn.weight'\n","Freezing layer 'model.6.cv2.bn.bias'\n","Freezing layer 'model.6.m.0.cv1.conv.weight'\n","Freezing layer 'model.6.m.0.cv1.bn.weight'\n","Freezing layer 'model.6.m.0.cv1.bn.bias'\n","Freezing layer 'model.6.m.0.cv2.conv.weight'\n","Freezing layer 'model.6.m.0.cv2.bn.weight'\n","Freezing layer 'model.6.m.0.cv2.bn.bias'\n","Freezing layer 'model.6.m.0.cv3.conv.weight'\n","Freezing layer 'model.6.m.0.cv3.bn.weight'\n","Freezing layer 'model.6.m.0.cv3.bn.bias'\n","Freezing layer 'model.6.m.0.m.0.cv1.conv.weight'\n","Freezing layer 'model.6.m.0.m.0.cv1.bn.weight'\n","Freezing layer 'model.6.m.0.m.0.cv1.bn.bias'\n","Freezing layer 'model.6.m.0.m.0.cv2.conv.weight'\n","Freezing layer 'model.6.m.0.m.0.cv2.bn.weight'\n","Freezing layer 'model.6.m.0.m.0.cv2.bn.bias'\n","Freezing layer 'model.6.m.0.m.1.cv1.conv.weight'\n","Freezing layer 'model.6.m.0.m.1.cv1.bn.weight'\n","Freezing layer 'model.6.m.0.m.1.cv1.bn.bias'\n","Freezing layer 'model.6.m.0.m.1.cv2.conv.weight'\n","Freezing layer 'model.6.m.0.m.1.cv2.bn.weight'\n","Freezing layer 'model.6.m.0.m.1.cv2.bn.bias'\n","Freezing layer 'model.7.conv.weight'\n","Freezing layer 'model.7.bn.weight'\n","Freezing layer 'model.7.bn.bias'\n","Freezing layer 'model.8.cv1.conv.weight'\n","Freezing layer 'model.8.cv1.bn.weight'\n","Freezing layer 'model.8.cv1.bn.bias'\n","Freezing layer 'model.8.cv2.conv.weight'\n","Freezing layer 'model.8.cv2.bn.weight'\n","Freezing layer 'model.8.cv2.bn.bias'\n","Freezing layer 'model.8.m.0.cv1.conv.weight'\n","Freezing layer 'model.8.m.0.cv1.bn.weight'\n","Freezing layer 'model.8.m.0.cv1.bn.bias'\n","Freezing layer 'model.8.m.0.cv2.conv.weight'\n","Freezing layer 'model.8.m.0.cv2.bn.weight'\n","Freezing layer 'model.8.m.0.cv2.bn.bias'\n","Freezing layer 'model.8.m.0.cv3.conv.weight'\n","Freezing layer 'model.8.m.0.cv3.bn.weight'\n","Freezing layer 'model.8.m.0.cv3.bn.bias'\n","Freezing layer 'model.8.m.0.m.0.cv1.conv.weight'\n","Freezing layer 'model.8.m.0.m.0.cv1.bn.weight'\n","Freezing layer 'model.8.m.0.m.0.cv1.bn.bias'\n","Freezing layer 'model.8.m.0.m.0.cv2.conv.weight'\n","Freezing layer 'model.8.m.0.m.0.cv2.bn.weight'\n","Freezing layer 'model.8.m.0.m.0.cv2.bn.bias'\n","Freezing layer 'model.8.m.0.m.1.cv1.conv.weight'\n","Freezing layer 'model.8.m.0.m.1.cv1.bn.weight'\n","Freezing layer 'model.8.m.0.m.1.cv1.bn.bias'\n","Freezing layer 'model.8.m.0.m.1.cv2.conv.weight'\n","Freezing layer 'model.8.m.0.m.1.cv2.bn.weight'\n","Freezing layer 'model.8.m.0.m.1.cv2.bn.bias'\n","Freezing layer 'model.9.cv1.conv.weight'\n","Freezing layer 'model.9.cv1.bn.weight'\n","Freezing layer 'model.9.cv1.bn.bias'\n","Freezing layer 'model.9.cv2.conv.weight'\n","Freezing layer 'model.9.cv2.bn.weight'\n","Freezing layer 'model.9.cv2.bn.bias'\n","Freezing layer 'model.10.cv1.conv.weight'\n","Freezing layer 'model.10.cv1.bn.weight'\n","Freezing layer 'model.10.cv1.bn.bias'\n","Freezing layer 'model.10.cv2.conv.weight'\n","Freezing layer 'model.10.cv2.bn.weight'\n","Freezing layer 'model.10.cv2.bn.bias'\n","Freezing layer 'model.10.m.0.attn.qkv.conv.weight'\n","Freezing layer 'model.10.m.0.attn.qkv.bn.weight'\n","Freezing layer 'model.10.m.0.attn.qkv.bn.bias'\n","Freezing layer 'model.10.m.0.attn.proj.conv.weight'\n","Freezing layer 'model.10.m.0.attn.proj.bn.weight'\n","Freezing layer 'model.10.m.0.attn.proj.bn.bias'\n","Freezing layer 'model.10.m.0.attn.pe.conv.weight'\n","Freezing layer 'model.10.m.0.attn.pe.bn.weight'\n","Freezing layer 'model.10.m.0.attn.pe.bn.bias'\n","Freezing layer 'model.10.m.0.ffn.0.conv.weight'\n","Freezing layer 'model.10.m.0.ffn.0.bn.weight'\n","Freezing layer 'model.10.m.0.ffn.0.bn.bias'\n","Freezing layer 'model.10.m.0.ffn.1.conv.weight'\n","Freezing layer 'model.10.m.0.ffn.1.bn.weight'\n","Freezing layer 'model.10.m.0.ffn.1.bn.bias'\n","Freezing layer 'model.13.cv1.conv.weight'\n","Freezing layer 'model.13.cv1.bn.weight'\n","Freezing layer 'model.13.cv1.bn.bias'\n","Freezing layer 'model.13.cv2.conv.weight'\n","Freezing layer 'model.13.cv2.bn.weight'\n","Freezing layer 'model.13.cv2.bn.bias'\n","Freezing layer 'model.13.m.0.cv1.conv.weight'\n","Freezing layer 'model.13.m.0.cv1.bn.weight'\n","Freezing layer 'model.13.m.0.cv1.bn.bias'\n","Freezing layer 'model.13.m.0.cv2.conv.weight'\n","Freezing layer 'model.13.m.0.cv2.bn.weight'\n","Freezing layer 'model.13.m.0.cv2.bn.bias'\n","Freezing layer 'model.23.dfl.conv.weight'\n","\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n","\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n","\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.5±0.1 ms, read: 38.5±25.6 MB/s, size: 98.3 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1EKy5MKJmoLNQxoFyuOzjyUbOAB4YbrrM/dataset_people/labels/train.cache... 10200 images, 0 backgrounds, 0 corrupt: 100%|██████████| 10200/10200 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.5±0.2 ms, read: 45.9±21.6 MB/s, size: 82.8 KB)\n"]},{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mval: \u001b[0mScanning /content/drive/.shortcut-targets-by-id/1EKy5MKJmoLNQxoFyuOzjyUbOAB4YbrrM/dataset_people/labels/val.cache... 2500 images, 0 backgrounds, 0 corrupt: 100%|██████████| 2500/2500 [00:00<?, ?it/s]\n"]},{"output_type":"stream","name":"stdout","text":["Plotting labels to /content/drive/MyDrive/Trabajo_IA-3_FGS/yolov11_training/fine_tuning_iter2/labels.jpg... \n","\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=5e-05, momentum=0.937) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0001), 87 bias(decay=0.0)\n","Image sizes 640 train, 640 val\n","Using 2 dataloader workers\n","Logging results to \u001b[1m/content/drive/MyDrive/Trabajo_IA-3_FGS/yolov11_training/fine_tuning_iter2\u001b[0m\n","Starting training for 50 epochs...\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       1/50     0.688G      1.321     0.5971     0.7299         27        640: 100%|██████████| 1275/1275 [1:16:24<00:00,  3.60s/it]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:31<00:00,  5.00it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all       2500       6662      0.957      0.965      0.989      0.738\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       2/50     0.959G      1.273     0.5624     0.7162         41        640: 100%|██████████| 1275/1275 [04:22<00:00,  4.86it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:30<00:00,  5.15it/s]"]},{"output_type":"stream","name":"stdout","text":["                   all       2500       6662      0.958      0.965      0.989      0.744\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"stream","name":"stdout","text":["\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       3/50     0.961G      1.262     0.5491     0.7151         33        640: 100%|██████████| 1275/1275 [04:30<00:00,  4.71it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:32<00:00,  4.89it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all       2500       6662      0.956      0.966      0.989      0.742\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       4/50     0.967G      1.254     0.5427     0.7136         34        640: 100%|██████████| 1275/1275 [04:26<00:00,  4.78it/s]\n","                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 157/157 [00:30<00:00,  5.23it/s]\n"]},{"output_type":"stream","name":"stdout","text":["                   all       2500       6662      0.962      0.962      0.989      0.745\n","\n","      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"]},{"output_type":"stream","name":"stderr","text":["       5/50     0.967G       1.25     0.5514     0.7188         35        640:   8%|▊         | 96/1275 [00:19<03:49,  5.13it/s]"]}]},{"cell_type":"code","source":["# ===================== FUNCIÓN PARA GRAFICAR CURVAS DE APRENDIZAJE =====================\n","def plot_learning_curves(results_df, save_path=None, phase='transfer'):\n","    \"\"\"\n","    Grafica las curvas de aprendizaje para pérdidas, mAP, precisión, recall y F1-score.\n","\n","    Args:\n","        results_df (pd.DataFrame): DataFrame con los resultados del entrenamiento.\n","        save_path (str, optional): Ruta para guardar el gráfico.\n","        phase (str): Fase del entrenamiento ('transfer' o 'fine-tuning').\n","    \"\"\"\n","    plt.figure(figsize=(15, 10))\n","\n","    # Usar column_mapping para obtener los nombres correctos de las columnas\n","    try:\n","        box_loss = results_df[column_mapping['train/box_loss']].values\n","        cls_loss = results_df[column_mapping['train/cls_loss']].values\n","        dfl_loss = results_df[column_mapping['train/dfl_loss']].values\n","        precision = results_df[column_mapping['metrics/precision(B)']].values\n","        recall = results_df[column_mapping['metrics/recall(B)']].values\n","        map50 = results_df[column_mapping['metrics/mAP50(B)']].values\n","        map50_95 = results_df[column_mapping['metrics/mAP50-95(B)']].values\n","        f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n","    except KeyError as e:\n","        print(f\"Error al acceder a las columnas en plot_learning_curves: {e}\")\n","        print(\"Columnas disponibles:\", results_df.columns.tolist())\n","        raise\n","\n","    # Subgráfico 1: Pérdidas\n","    plt.subplot(2, 2, 1)\n","    plt.plot(box_loss, label='Box Loss')\n","    plt.plot(cls_loss, label='Class Loss')\n","    plt.plot(dfl_loss, label='DFL Loss')\n","    plt.xlabel('Época')\n","    plt.ylabel('Pérdida')\n","    plt.title(f'Evolución de las Pérdidas ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Subgráfico 2: mAP\n","    plt.subplot(2, 2, 2)\n","    plt.plot(map50, label='mAP@0.5')\n","    plt.plot(map50_95, label='mAP@0.5:0.95')\n","    plt.xlabel('Época')\n","    plt.ylabel('mAP')\n","    plt.title(f'Evolución del mAP ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Subgráfico 3: Precisión y Recall\n","    plt.subplot(2, 2, 3)\n","    plt.plot(precision, label='Precisión')\n","    plt.plot(recall, label='Recall')\n","    plt.xlabel('Época')\n","    plt.ylabel('Valor')\n","    plt.title(f'Precisión y Recall ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Subgráfico 4: F1-Score\n","    plt.subplot(2, 2, 4)\n","    plt.plot(f1_score, label='F1-Score')\n","    plt.xlabel('Época')\n","    plt.ylabel('F1-Score')\n","    plt.title(f'Evolución del F1-Score ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    if save_path:\n","        plt.savefig(save_path)\n","        print(f\"Gráfico guardado en: {save_path}\")\n","    plt.show()\n","\n","\n","\n","\n","\n","# Cargar resultados\n","results_csv_path = f\"{OUTPUT_PATH}/fine_tuning_iter2/results.csv\"\n","if not os.path.exists(results_csv_path):\n","    raise FileNotFoundError(f\"No se encontró results.csv en {results_csv_path}\")\n","results_df = pd.read_csv(results_csv_path)\n","\n","# Inspeccionar nombres de columnas\n","print(\"Columnas en results.csv:\")\n","print(results_df.columns.tolist())\n","\n","# Usar el mismo column_mapping\n","column_mapping = {\n","    'train/box_loss': 'train/box_loss',\n","    'train/cls_loss': 'train/cls_loss',\n","    'train/dfl_loss': 'train/dfl_loss',\n","    'metrics/precision(B)': 'metrics/precision(B)',\n","    'metrics/recall(B)': 'metrics/recall(B)',\n","    'metrics/mAP50(B)': 'metrics/mAP50(B)',\n","    'metrics/mAP50-95(B)': 'metrics/mAP50-95(B)'\n","}\n","\n","# Verificar y mapear columnas\n","available_columns = results_df.columns.tolist()\n","for expected_col, mapped_col in column_mapping.items():\n","    if mapped_col not in available_columns:\n","        similar_cols = [col for col in available_columns if expected_col.split('/')[-1] in col]\n","        if similar_cols:\n","            column_mapping[expected_col] = similar_cols[0]\n","            print(f\"Advertencia: Columna '{mapped_col}' no encontrada, usando '{similar_cols[0]}' en su lugar\")\n","        else:\n","            raise KeyError(f\"No se encontró columna para '{expected_col}' en results.csv\")\n","\n","# Extraer métricas\n","try:\n","    box_loss = results_df[column_mapping['train/box_loss']].values\n","    cls_loss = results_df[column_mapping['train/cls_loss']].values\n","    dfl_loss = results_df[column_mapping['train/dfl_loss']].values\n","    precision = results_df[column_mapping['metrics/precision(B)']].values\n","    recall = results_df[column_mapping['metrics/recall(B)']].values\n","    map50 = results_df[column_mapping['metrics/mAP50(B)']].values\n","    map50_95 = results_df[column_mapping['metrics/mAP50-95(B)']].values\n","    f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n","except KeyError as e:\n","    print(f\"Error al extraer métricas: {e}\")\n","    raise\n","\n","# Imprimir métricas finales\n","print(\"\\n=== RESULTADOS DEL FINE-TUNING ITERACIÓN 2 ===\")\n","print(f\"Loss final (box): {box_loss[-1]:.4f}\")\n","print(f\"Loss final (cls): {cls_loss[-1]:.4f}\")\n","print(f\"Loss final (dfl): {dfl_loss[-1]:.4f}\")\n","print(f\"Precisión: {precision[-1]:.4f}\")\n","print(f\"Recall: {recall[-1]:.4f}\")\n","print(f\"F1-Score: {f1_score[-1]:.4f}\")\n","print(f\"mAP@0.5: {map50[-1]:.4f}\")\n","print(f\"mAP@0.5:0.95: {map50_95[-1]:.4f}\")\n","\n","# Graficar resultados\n","plot_learning_curves(results_df, f\"{RESULTS_PATH}/fine_tuning_iter2_results.png\", phase=\"Fine-Tuning Iteración 2\")\n","\n","# Loggear métricas en W&B\n","wandb.log({\n","    \"finetune_iter2_box_loss\": box_loss[-1],\n","    \"finetune_iter2_cls_loss\": cls_loss[-1],\n","    \"finetune_iter2_dfl_loss\": dfl_loss[-1],\n","    \"finetune_iter2_precision\": precision[-1],\n","    \"finetune_iter2_recall\": recall[-1],\n","    \"finetune_iter2_f1\": f1_score[-1],\n","    \"finetune_iter2_map50\": map50[-1],\n","    \"finetune_iter2_map50_95\": map50_95[-1]\n","})\n","\n","# Liberar memoria\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"ckSk8d7BEW_2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# EVALUACION DEL MODELO FINAL"],"metadata":{"id":"p-rxieZqDvX2"}},{"cell_type":"code","source":["# ===================== EVALUACIÓN DEL MODELO FINAL =====================\n","print(\"\\n=== EVALUACIÓN DEL MODELO FINAL ===\")\n","val_results = ft_model.val(data=custom_yaml_path, split='test')\n","val_metrics = val_results.results_dict\n","f1_score = 2 * val_metrics['metrics/precision(B)'] * val_metrics['metrics/recall(B)'] / \\\n","           (val_metrics['metrics/precision(B)'] + val_metrics['metrics/recall(B)'] + 1e-6)\n","\n","print(\"Métricas de evaluación:\")\n","print(f\"- Precisión: {val_metrics['metrics/precision(B)']:.4f}\")\n","print(f\"- Recall: {val_metrics['metrics/recall(B)']:.4f}\")\n","print(f\"- F1-Score: {f1_score:.4f}\")\n","print(f\"- mAP@0.5: {val_metrics['metrics/mAP50(B)']:.4f}\")\n","print(f\"- mAP@0.5:0.95: {val_metrics['metrics/mAP50-95(B)']:.4f}\")\n","\n","# Gráfico de radar\n","metrics_names = ['Precisión', 'Recall', 'F1-Score', 'mAP@0.5', 'mAP@0.5:0.95']\n","metrics_values = [\n","    val_metrics['metrics/precision(B)'],\n","    val_metrics['metrics/recall(B)'],\n","    f1_score,\n","    val_metrics['metrics/mAP50(B)'],\n","    val_metrics['metrics/mAP50-95(B)']\n","]\n","plt.figure(figsize=(10, 8))\n","angles = np.linspace(0, 2*np.pi, len(metrics_names), endpoint=False).tolist()\n","angles += angles[:1]\n","metrics_values += metrics_values[:1]\n","ax = plt.subplot(111, polar=True)\n","ax.plot(angles, metrics_values, 'o-', linewidth=2, color='blue')\n","ax.fill(angles, metrics_values, alpha=0.25, color='blue')\n","ax.set_thetagrids(np.degrees(angles[:-1]), metrics_names)\n","ax.set_ylim(0, 1)\n","plt.title('Métricas de Rendimiento del Modelo Final', pad=20)\n","plt.savefig(f\"{RESULTS_PATH}/model_metrics_radar.png\", dpi=300)\n","wandb.log({\"model_metrics_radar\": wandb.Image(f\"{RESULTS_PATH}/model_metrics_radar.png\")})\n","plt.show()\n","\n","# Matriz de confusión\n","cm = compute_confusion_matrix(ft_model, images_test, labels_test)\n","plot_confusion_matrix(cm, f\"{RESULTS_PATH}/confusion_matrix.png\")\n","TP, FP, FN = cm[0, 0], cm[0, 1], cm[1, 0]\n","precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n","recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n","tpr = recall\n","fpr = FP / (FP + cm[1, 1]) if (FP + cm[1, 1]) > 0 else 0  # Corregido para incluir TN\n","print(f\"Especificidad: No aplicable en detección de objetos\")\n","print(f\"TPR: {tpr:.4f}\")\n","print(f\"FPR: {fpr:.4f}\")\n","\n","# Curvas PR y ROC\n","thresholds, precisions, recalls = calculate_precision_recall_thresholds(ft_model, images_test, labels_test)\n","ap = 0\n","for i in range(1, len(recalls)):\n","    ap += (recalls[i] - recalls[i-1]) * precisions[i]\n","plot_precision_recall_curve(precisions, recalls, ap, f\"{RESULTS_PATH}/precision_recall_curve.png\")\n","wandb.log({\"precision_recall_curve\": wandb.Image(f\"{RESULTS_PATH}/precision_recall_curve.png\")})\n","\n","thresholds, tprs, fprs, auc = calculate_roc_curve(ft_model, images_test, labels_test)\n","plot_roc_curve(tprs, fprs, auc, f\"{RESULTS_PATH}/roc_curve.png\")\n","wandb.log({\"roc_curve\": wandb.Image(f\"{RESULTS_PATH}/roc_curve.png\")})\n","\n","# Loggear métricas finales\n","wandb.log({\n","    \"test_precision\": val_metrics['metrics/precision(B)'],\n","    \"test_recall\": val_metrics['metrics/recall(B)'],\n","    \"test_f1\": f1_score,\n","    \"test_map50\": val_metrics['metrics/mAP50(B)'],\n","    \"test_map50_95\": val_metrics['metrics/mAP50-95(B)'],\n","    \"test_ap\": ap,\n","    \"test_roc_auc\": auc,\n","    \"test_tpr\": tpr,\n","    \"test_fpr\": fpr\n","})\n","\n","# ===================== VISUALIZACIÓN DE DETECCIONES =====================\n","def visualize_predictions(model, test_dir, num_samples=6, conf_threshold=0.25):\n","    \"\"\"\n","    Visualiza predicciones del modelo en imágenes de prueba.\n","\n","    Args:\n","        model: Modelo YOLO entrenado.\n","        test_dir (str): Directorio con imágenes de prueba.\n","        num_samples (int): Número de imágenes a visualizar.\n","        conf_threshold (float): Umbral de confianza para detecciones.\n","    \"\"\"\n","    try:\n","        image_files = sorted([f for f in os.listdir(test_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])\n","        if not image_files:\n","            print(f\"No se encontraron imágenes en {test_dir}\")\n","            return\n","        indices = np.random.choice(len(image_files), min(num_samples, len(image_files)), replace=False)\n","        num_cols = min(len(indices), 3)\n","        num_rows = (len(indices) + num_cols - 1) // num_cols\n","        fig, axes = plt.subplots(num_rows, num_cols, figsize=(5*num_cols, 5*num_rows))\n","        axes = np.array(axes).flatten() if num_cols > 1 or num_rows > 1 else [axes]\n","\n","        for i, idx in enumerate(indices):\n","            img_name = image_files[idx]\n","            img_path = os.path.join(test_dir, img_name)\n","            with torch.no_grad():\n","                results = model.predict(img_path, conf=conf_threshold, verbose=False)[0]\n","            im_array = results.plot(boxes=True, labels=True, conf=True)\n","            axes[i].imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n","            axes[i].set_title(f\"Predicción: {img_name}\", fontsize=10)\n","            axes[i].axis('off')\n","\n","        # Ocultar ejes vacíos si hay menos imágenes que subgráficos\n","        for i in range(len(indices), len(axes)):\n","            axes[i].axis('off')\n","\n","        plt.tight_layout()\n","        save_path = f\"{RESULTS_PATH}/sample_predictions.png\"\n","        plt.savefig(save_path, dpi=300)\n","        wandb.log({\"sample_predictions\": wandb.Image(save_path)})\n","        print(f\"Visualizaciones guardadas en: {save_path}\")\n","        plt.show()\n","\n","    except Exception as e:\n","        print(f\"Error al visualizar predicciones: {str(e)}\")\n","\n","print(\"\\n=== VISUALIZACIÓN DE DETECCIONES ===\")\n","visualize_predictions(ft_model, images_test)"],"metadata":{"id":"dCuPgq0sDx4K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== EVALUACIÓN DEL MODELO FINAL =====================\n","print(\"\\n=== EVALUACIÓN DEL MODELO FINAL ===\")\n","val_results = ft_model.val(data=custom_yaml_path, split='test')\n","val_metrics = val_results.results_dict\n","f1_score = 2 * val_metrics['metrics/precision(B)'] * val_metrics['metrics/recall(B)'] / \\\n","           (val_metrics['metrics/precision(B)'] + val_metrics['metrics/recall(B)'] + 1e-6)\n","\n","print(\"Métricas de evaluación:\")\n","print(f\"- Precisión: {val_metrics['metrics/precision(B)']:.4f}\")\n","print(f\"- Recall: {val_metrics['metrics/recall(B)']:.4f}\")\n","print(f\"- F1-Score: {f1_score:.4f}\")\n","print(f\"- mAP@0.5: {val_metrics['metrics/mAP50(B)']:.4f}\")\n","print(f\"- mAP@0.5:0.95: {val_metrics['metrics/mAP50-95(B)']:.4f}\")\n","\n","# Gráfico de radar\n","metrics_names = ['Precisión', 'Recall', 'F1-Score', 'mAP@0.5', 'mAP@0.5:0.95']\n","metrics_values = [\n","    val_metrics['metrics/precision(B)'],\n","    val_metrics['metrics/recall(B)'],\n","    f1_score,\n","    val_metrics['metrics/mAP50(B)'],\n","    val_metrics['metrics/mAP50-95(B)']\n","]\n","plt.figure(figsize=(10, 8))\n","angles = np.linspace(0, 2*np.pi, len(metrics_names), endpoint=False).tolist()\n","angles += angles[:1]\n","metrics_values += metrics_values[:1]\n","ax = plt.subplot(111, polar=True)\n","ax.plot(angles, metrics_values, 'o-', linewidth=2)\n","ax.fill(angles, metrics_values, alpha=0.25)\n","ax.set_thetagrids(np.degrees(angles[:-1]), metrics_names)\n","ax.set_ylim(0, 1)\n","plt.title('Métricas de Rendimiento del Modelo Final')\n","plt.savefig(f\"{RESULTS_PATH}/model_metrics_radar.png\")\n","plt.show()\n","\n","# Matriz de confusión\n","cm = compute_confusion_matrix(ft_model, images_test, labels_test)\n","plot_confusion_matrix(cm, f\"{RESULTS_PATH}/confusion_matrix.png\")\n","TP, FP, FN = cm[0, 0], cm[0, 1], cm[1, 0]\n","precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n","recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n","tpr = recall\n","fpr = FP / (FP + 0) if FP > 0 else 0\n","print(f\"Especificidad: No aplicable en detección de objetos\")\n","print(f\"TPR: {tpr:.4f}\")\n","print(f\"FPR: {fpr:.4f}\")\n","\n","# Curvas PR y ROC\n","thresholds, precisions, recalls = calculate_precision_recall_thresholds(ft_model, images_test, labels_test)\n","ap = 0\n","for i in range(1, len(recalls)):\n","    ap += (recalls[i] - recalls[i-1]) * precisions[i]\n","plot_precision_recall_curve(precisions, recalls, ap, f\"{RESULTS_PATH}/precision_recall_curve.png\")\n","\n","thresholds, tprs, fprs, auc = calculate_roc_curve(ft_model, images_test, labels_test)\n","plot_roc_curve(tprs, fprs, auc, f\"{RESULTS_PATH}/roc_curve.png\")\n","\n","# Loggear métricas finales\n","wandb.log({\n","    \"test_precision\": val_metrics['metrics/precision(B)'],\n","    \"test_recall\": val_metrics['metrics/recall(B)'],\n","    \"test_f1\": f1_score,\n","    \"test_map50\": val_metrics['metrics/mAP50(B)'],\n","    \"test_map50_95\": val_metrics['metrics/mAP50-95(B)'],\n","    \"test_ap\": ap,\n","    \"test_roc_auc\": auc\n","})\n"],"metadata":{"id":"BCN90842F1db"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== VISUALIZACIÓN DE DETECCIONES =====================\n","def visualize_predictions(model, test_dir, num_samples=6, conf_threshold=0.25):\n","    image_files = sorted([f for f in os.listdir(test_dir) if f.endswith('.jpg')])\n","    if not image_files:\n","        print(f\"No se encontraron imágenes en {test_dir}\")\n","        return\n","    indices = np.random.choice(len(image_files), min(num_samples, len(image_files)), replace=False)\n","    fig, axes = plt.subplots(2, len(indices)//2, figsize=(20, 10))\n","    axes = axes.flatten()\n","    for i, idx in enumerate(indices):\n","        img_name = image_files[idx]\n","        img_path = os.path.join(test_dir, img_name)\n","        with torch.no_grad():\n","            results = model.predict(img_path, conf=conf_threshold, verbose=False)[0]\n","        im_array = results.plot()\n","        axes[i].imshow(cv2.cvtColor(im_array, cv2.COLOR_BGR2RGB))\n","        axes[i].set_title(f\"Predicción: {img_name}\")\n","        axes[i].axis('off')\n","    plt.tight_layout()\n","    plt.savefig(f\"{RESULTS_PATH}/sample_predictions.png\")\n","    plt.show()\n","\n","print(\"\\n=== VISUALIZACIÓN DE DETECCIONES ===\")\n","visualize_predictions(ft_model, images_test)"],"metadata":{"id":"RB4AIi7_F3bD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== ANÁLISIS DE RENDIMIENTO POR TAMAÑO DE OBJETO =====================\n","def analyze_performance_by_size(model, test_dir, test_labels_dir):\n","    size_categories = {'small': (0, 0.05), 'medium': (0.05, 0.15), 'large': (0.15, 1.0)}\n","    results_by_size = {size: {'correct': 0, 'total': 0, 'false_positives': 0} for size in size_categories}\n","    image_files = [f for f in os.listdir(test_dir) if f.endswith('.jpg')]\n","    for img_file in tqdm(image_files, desc=\"Evaluando por tamaño\"):\n","        img_path = os.path.join(test_dir, img_file)\n","        label_path = os.path.join(test_labels_dir, img_file.replace('.jpg', '.txt'))\n","        gt_boxes = []\n","        if os.path.exists(label_path):\n","            with open(label_path, 'r') as f:\n","                for line in f.readlines():\n","                    values = line.strip().split()\n","                    if len(values) == 5:\n","                        _, x, y, w, h = map(float, values)\n","                        area = w * h\n","                        size_cat = next((cat for cat, (min_size, max_size) in size_categories.items()\n","                                        if min_size <= area < max_size), None)\n","                        if size_cat:\n","                            results_by_size[size_cat]['total'] += 1\n","                            gt_boxes.append({'box': [x, y, w, h], 'size': size_cat, 'matched': False})\n","        with torch.no_grad():\n","            results = model.predict(img_path, conf=0.4, iou=0.5, verbose=False)[0]\n","        pred_boxes = []\n","        for box in results.boxes:\n","            x, y, w, h = box.xywhn[0].cpu().numpy()\n","            conf = box.conf[0].cpu().numpy()\n","            pred_boxes.append([x, y, w, h, conf])\n","        for pred in pred_boxes:\n","            best_iou = 0\n","            best_match = None\n","            for i, gt in enumerate(gt_boxes):\n","                if gt['matched']:\n","                    continue\n","                px, py, pw, ph, _ = pred\n","                gx, gy, gw, gh = gt['box']\n","                x_left = max(px - pw/2, gx - gw/2)\n","                y_top = max(py - ph/2, gy - gh/2)\n","                x_right = min(px + pw/2, gx + gw/2)\n","                y_bottom = min(py + ph/2, gy + gh/2)\n","                if x_right < x_left or y_bottom < y_top:\n","                    continue\n","                intersection = (x_right - x_left) * (y_bottom - y_top)\n","                p_area, g_area = pw * ph, gw * gh\n","                iou = intersection / (p_area + g_area - intersection)\n","                if iou > best_iou and iou >= 0.5:\n","                    best_iou = iou\n","                    best_match = i\n","            if best_match is not None:\n","                gt_boxes[best_match]['matched'] = True\n","                size = gt_boxes[best_match]['size']\n","                results_by_size[size]['correct'] += 1\n","            else:\n","                area = pred[2] * pred[3]\n","                for cat, (min_size, max_size) in size_categories.items():\n","                    if min_size <= area < max_size:\n","                        results_by_size[cat]['false_positives'] += 1\n","                        break\n","    metrics_by_size = {}\n","    for size, counts in results_by_size.items():\n","        if counts['total'] > 0:\n","            precision = counts['correct'] / (counts['correct'] + counts['false_positives']) if (counts['correct'] + counts['false_positives']) > 0 else 0\n","            recall = counts['correct'] / counts['total'] if counts['total'] > 0 else 0\n","            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n","            metrics_by_size[size] = {\n","                'precision': precision,\n","                'recall': recall,\n","                'f1': f1,\n","                'total': counts['total'],\n","                'correct': counts['correct'],\n","                'false_positives': counts['false_positives']\n","            }\n","    return metrics_by_size\n","\n","print(\"\\n=== ANÁLISIS DE RENDIMIENTO POR TAMAÑO DE OBJETO ===\")\n","size_metrics = analyze_performance_by_size(ft_model, images_test, labels_test)\n","for size, metrics in size_metrics.items():\n","    print(f\"- {size.capitalize()}:\")\n","    print(f\"  * Total: {metrics['total']}\")\n","    print(f\"  * Correctos: {metrics['correct']}\")\n","    print(f\"  * Falsos Positivos: {metrics['false_positives']}\")\n","    print(f\"  * Precisión: {metrics['precision']:.4f}\")\n","    print(f\"  * Recall: {metrics['recall']:.4f}\")\n","    print(f\"  * F1-Score: {metrics['f1']:.4f}\")\n","\n","sizes = list(size_metrics.keys())\n","precisions = [size_metrics[s]['precision'] for s in sizes]\n","recalls = [size_metrics[s]['recall'] for s in sizes]\n","f1_scores = [size_metrics[s]['f1'] for s in sizes]\n","x = np.arange(len(sizes))\n","width = 0.25\n","plt.figure(figsize=(12, 6))\n","plt.bar(x - width, precisions, width, label='Precisión')\n","plt.bar(x, recalls, width, label='Recall')\n","plt.bar(x + width, f1_scores, width, label='F1-Score')\n","plt.xlabel('Tamaño del Objeto')\n","plt.ylabel('Valor')\n","plt.title('Rendimiento por Tamaño de Objeto')\n","plt.xticks(x, [s.capitalize() for s in sizes])\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","for i, v in enumerate(precisions):\n","    plt.text(i - width, v + 0.02, f'{v:.2f}', ha='center')\n","for i, v in enumerate(recalls):\n","    plt.text(i, v + 0.02, f'{v:.2f}', ha='center')\n","for i, v in enumerate(f1_scores):\n","    plt.text(i + width, v + 0.02, f'{v:.2f}', ha='center')\n","plt.tight_layout()\n","plt.savefig(f\"{RESULTS_PATH}/performance_by_size.png\")\n","plt.show()\n","\n","# Loggear métricas por tamaño\n","for size, metrics in size_metrics.items():\n","    wandb.log({f\"size_{size}_precision\": metrics['precision'],\n","               f\"size_{size}_recall\": metrics['recall'],\n","               f\"size_{size}_f1\": metrics['f1']})"],"metadata":{"id":"YHYjqYH8F57j"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== ANÁLISIS POR REGIÓN DE LA IMAGEN =====================\n","def analyze_performance_by_region(model, test_dir, test_labels_dir):\n","    regions = {\n","        'top-left': (0, 0, 1/3, 1/3), 'top-center': (1/3, 0, 2/3, 1/3), 'top-right': (2/3, 0, 1, 1/3),\n","        'middle-left': (0, 1/3, 1/3, 2/3), 'middle-center': (1/3, 1/3, 2/3, 2/3), 'middle-right': (2/3, 1/3, 1, 2/3),\n","        'bottom-left': (0, 2/3, 1/3, 1), 'bottom-center': (1/3, 2/3, 2/3, 1), 'bottom-right': (2/3, 2/3, 1, 1)\n","    }\n","    results_by_region = {region: {'correct': 0, 'total': 0, 'false_positives': 0} for region in regions}\n","    image_files = [f for f in os.listdir(test_dir) if f.endswith('.jpg')]\n","    for img_file in tqdm(image_files, desc=\"Evaluando por región\"):\n","        img_path = os.path.join(test_dir, img_file)\n","        label_path = os.path.join(test_labels_dir, img_file.replace('.jpg', '.txt'))\n","        gt_boxes = []\n","        if os.path.exists(label_path):\n","            with open(label_path, 'r') as f:\n","                for line in f.readlines():\n","                    values = line.strip().split()\n","                    if len(values) == 5:\n","                        _, x, y, w, h = map(float, values)\n","                        region_key = next((key for key, (x1, y1, x2, y2) in regions.items()\n","                                          if x1 <= x < x2 and y1 <= y < y2), None)\n","                        if region_key:\n","                            results_by_region[region_key]['total'] += 1\n","                            gt_boxes.append({'box': [x, y, w, h], 'region': region_key, 'matched': False})\n","        with torch.no_grad():\n","            results = model.predict(img_path, conf=0.4, iou=0.5, verbose=False)[0]\n","        pred_boxes = []\n","        for box in results.boxes:\n","            x, y, w, h = box.xywhn[0].cpu().numpy()\n","            conf = box.conf[0].cpu().numpy()\n","            pred_boxes.append([x, y, w, h, conf])\n","        for pred in pred_boxes:\n","            best_iou = 0\n","            best_match = None\n","            for i, gt in enumerate(gt_boxes):\n","                if gt['matched']:\n","                    continue\n","                px, py, pw, ph, _ = pred\n","                gx, gy, gw, gh = gt['box']\n","                x_left = max(px - pw/2, gx - gw/2)\n","                y_top = max(py - ph/2, gy - gh/2)\n","                x_right = min(px + pw/2, gx + gw/2)\n","                y_bottom = min(py + ph/2, gy + gh/2)\n","                if x_right < x_left or y_bottom < y_top:\n","                    continue\n","                intersection = (x_right - x_left) * (y_bottom - y_top)\n","                p_area, g_area = pw * ph, gw * gh\n","                iou = intersection / (p_area + g_area - intersection)\n","                if iou > best_iou and iou >= 0.5:\n","                    best_iou = iou\n","                    best_match = i\n","            if best_match is not None:\n","                gt_boxes[best_match]['matched'] = True\n","                region = gt_boxes[best_match]['region']\n","                results_by_region[region]['correct'] += 1\n","            else:\n","                px, py = pred[0], pred[1]\n","                for key, (x1, y1, x2, y2) in regions.items():\n","                    if x1 <= px < x2 and y1 <= py < y2:\n","                        results_by_region[key]['false_positives'] += 1\n","                        break\n","    metrics_by_region = {}\n","    for region, counts in results_by_region.items():\n","        if counts['total'] > 0 or counts['false_positives'] > 0:\n","            precision = counts['correct'] / (counts['correct'] + counts['false_positives']) if (counts['correct'] + counts['false_positives']) > 0 else 0\n","            recall = counts['correct'] / counts['total'] if counts['total'] > 0 else 0\n","            f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n","            metrics_by_region[region] = {\n","                'precision': precision,\n","                'recall': recall,\n","                'f1': f1,\n","                'total': counts['total'],\n","                'correct': counts['correct'],\n","                'false_positives': counts['false_positives']\n","            }\n","    return metrics_by_region\n","\n","print(\"\\n=== ANÁLISIS POR REGIÓN DE LA IMAGEN ===\")\n","region_metrics = analyze_performance_by_region(ft_model, images_test, labels_test)\n","plt.figure(figsize=(12, 10))\n","grid = (3, 3)\n","positions = {\n","    'top-left': (0, 0), 'top-center': (0, 1), 'top-right': (0, 2),\n","    'middle-left': (1, 0), 'middle-center': (1, 1), 'middle-right': (1, 2),\n","    'bottom-left': (2, 0), 'bottom-center': (2, 1), 'bottom-right': (2, 2)\n","}\n","max_objects = max([m['total'] for m in region_metrics.values()]) if region_metrics else 1\n","max_f1 = max([m['f1'] for m in region_metrics.values()]) if region_metrics else 1\n","f1_matrix = np.zeros(grid)\n","density_matrix = np.zeros(grid)\n","for region, metrics in region_metrics.items():\n","    row, col = positions[region]\n","    f1_matrix[row, col] = metrics['f1']\n","    density_matrix[row, col] = metrics['total'] / max_objects if max_objects > 0 else 0\n","plt.subplot(1, 2, 1)\n","sns.heatmap(f1_matrix, annot=True, fmt=\".2f\", cmap=\"YlGnBu\", vmin=0, vmax=1)\n","plt.title('F1-Score por Región')\n","plt.xticks([])\n","plt.yticks([])\n","plt.subplot(1, 2, 2)\n","sns.heatmap(density_matrix, annot=True, fmt=\".2f\", cmap=\"YlOrRd\", vmin=0, vmax=1)\n","plt.title('Densidad de Objetos por Región')\n","plt.xticks([])\n","plt.yticks([])\n","plt.tight_layout()\n","plt.savefig(f\"{RESULTS_PATH}/region_analysis.png\")\n","plt.show()\n","\n","print(\"Métricas detalladas por región:\")\n","for region, metrics in sorted(region_metrics.items()):\n","    print(f\"- {region}:\")\n","    print(f\"  * Total objetos: {metrics['total']}\")\n","    print(f\"  * Detecciones correctas: {metrics['correct']}\")\n","    print(f\"  * Falsos positivos: {metrics['false_positives']}\")\n","    print(f\"  * Precisión: {metrics['precision']:.4f}\")\n","    print(f\"  * Recall: {metrics['recall']:.4f}\")\n","    print(f\"  * F1-Score: {metrics['f1']:.4f}\")\n","\n","# Loggear métricas por región\n","for region, metrics in region_metrics.items():\n","    wandb.log({f\"region_{region}_precision\": metrics['precision'],\n","               f\"region_{region}_recall\": metrics['recall'],\n","               f\"region_{region}_f1\": metrics['f1']})"],"metadata":{"id":"OnTQZhoiF8oz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== ANÁLISIS DE VELOCIDAD DE INFERENCIA =====================\n","def benchmark_inference_speed(model, test_dir, batch_sizes=[1, 2, 4, 8], num_runs=10):\n","    image_files = [os.path.join(test_dir, f) for f in os.listdir(test_dir) if f.endswith('.jpg')]\n","    if len(image_files) > 50:\n","        image_files = np.random.choice(image_files, 50, replace=False).tolist()\n","    results = {}\n","    for batch_size in batch_sizes:\n","        times = []\n","        batches = [image_files[i:i+batch_size] for i in range(0, len(image_files), batch_size)]\n","        with torch.no_grad():\n","            _ = model(batches[0], verbose=False)\n","        for _ in range(num_runs):\n","            for batch in batches:\n","                start_time = time.time()\n","                with torch.no_grad():\n","                    _ = model(batch, verbose=False)\n","                end_time = time.time()\n","                times.append((end_time - start_time) / len(batch))\n","        avg_time = np.mean(times)\n","        std_time = np.std(times)\n","        fps = 1.0 / avg_time\n","        results[batch_size] = {'avg_time': avg_time, 'std_time': std_time, 'fps': fps}\n","    return results\n","\n","print(\"\\n=== ANÁLISIS DE VELOCIDAD DE INFERENCIA ===\")\n","speed_results = benchmark_inference_speed(ft_model, images_test)\n","for batch_size, metrics in speed_results.items():\n","    print(f\"- Batch size {batch_size}:\")\n","    print(f\"  * Tiempo promedio por imagen: {metrics['avg_time']*1000:.2f} ms\")\n","    print(f\"  * FPS: {metrics['fps']:.2f}\")\n","\n","plt.figure(figsize=(12, 5))\n","batch_sizes = list(speed_results.keys())\n","avg_times = [metrics['avg_time']*1000 for metrics in speed_results.values()]\n","std_times = [metrics['std_time']*1000 for metrics in speed_results.values()]\n","fps_values = [metrics['fps'] for metrics in speed_results.values()]\n","plt.subplot(1, 2, 1)\n","plt.errorbar(batch_sizes, avg_times, yerr=std_times, fmt='o-', capsize=5)\n","plt.xlabel('Tamaño de Batch')\n","plt.ylabel('Tiempo de Inferencia (ms)')\n","plt.title('Tiempo de Inferencia por Imagen')\n","plt.grid(True, alpha=0.3)\n","plt.subplot(1, 2, 2)\n","plt.bar(batch_sizes, fps_values, color='skyblue')\n","plt.xlabel('Tamaño de Batch')\n","plt.ylabel('FPS')\n","plt.title('Frames Por Segundo')\n","plt.grid(True, alpha=0.3)\n","for i, fps in enumerate(fps_values):\n","    plt.text(batch_sizes[i], fps + 0.5, f'{fps:.2f}', ha='center')\n","plt.tight_layout()\n","plt.savefig(f\"{RESULTS_PATH}/inference_speed.png\")\n","plt.show()\n","\n","# Loggear velocidad\n","wandb.log({f\"speed_batch_{bs}_fps\": metrics['fps'] for bs, metrics in speed_results.items()})\n"],"metadata":{"id":"sxnT-c8MF-z0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== FUNCIÓN PARA GRAFICAR CURVAS DE APRENDIZAJE =====================\n","def plot_learning_curves(results_df, save_path=None, phase='transfer'):\n","    \"\"\"\n","    Grafica las curvas de aprendizaje para pérdidas, mAP, precisión, recall y F1-score.\n","\n","    Args:\n","        results_df (pd.DataFrame): DataFrame con los resultados del entrenamiento.\n","        save_path (str, optional): Ruta para guardar el gráfico.\n","        phase (str): Fase del entrenamiento ('transfer' o 'fine-tuning').\n","    \"\"\"\n","    plt.figure(figsize=(15, 10))\n","\n","    # Usar column_mapping para obtener los nombres correctos de las columnas\n","    try:\n","        # Use the corrected column mapping\n","        box_loss = results_df[column_mapping['train/box_loss']].values\n","        cls_loss = results_df[column_mapping['train/cls_loss']].values\n","        dfl_loss = results_df[column_mapping['train/dfl_loss']].values\n","        precision = results_df[column_mapping['metrics/precision(B)']].values\n","        recall = results_df[column_mapping['metrics/recall(B)']].values\n","        map50 = results_df[column_mapping['metrics/mAP50(B)']].values\n","        map50_95 = results_df[column_mapping['metrics/mAP50-95(B)']].values\n","        f1_score = 2 * (precision * recall) / (precision + recall + 1e-6)\n","    except KeyError as e:\n","        print(f\"Error al acceder a las columnas en plot_learning_curves: {e}\")\n","        print(\"Columnas disponibles:\", results_df.columns.tolist())\n","        # Re-raise the exception after printing diagnostic info\n","        raise\n","\n","    # Subgráfico 1: Pérdidas\n","    plt.subplot(2, 2, 1)\n","    plt.plot(box_loss, label='Box Loss')\n","    plt.plot(cls_loss, label='Class Loss')\n","    plt.plot(dfl_loss, label='DFL Loss')\n","    plt.xlabel('Época')\n","    plt.ylabel('Pérdida')\n","    plt.title(f'Evolución de las Pérdidas ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Subgráfico 2: mAP\n","    plt.subplot(2, 2, 2)\n","    plt.plot(map50, label='mAP@0.5')\n","    plt.plot(map50_95, label='mAP@0.5:0.95')\n","    plt.xlabel('Época')\n","    plt.ylabel('mAP')\n","    plt.title(f'Evolución del mAP ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Subgráfico 3: Precisión y Recall\n","    plt.subplot(2, 2, 3)\n","    plt.plot(precision, label='Precisión')\n","    plt.plot(recall, label='Recall')\n","    plt.xlabel('Época')\n","    plt.ylabel('Valor')\n","    plt.title(f'Precisión y Recall ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    # Subgráfico 4: F1-Score\n","    plt.subplot(2, 2, 4)\n","    plt.plot(f1_score, label='F1-Score')\n","    plt.xlabel('Época')\n","    plt.ylabel('F1-Score')\n","    plt.title(f'Evolución del F1-Score ({phase})')\n","    plt.legend()\n","    plt.grid(True)\n","\n","    plt.tight_layout()\n","    if save_path:\n","        plt.savefig(save_path)\n","        print(f\"Gráfico guardado en: {save_path}\")\n","    plt.show()\n","\n","# ===================== CARGA Y ANÁLISIS DE RESULTADOS =====================\n","print(\"\\n=== CARGANDO RESULTADOS DEL TRANSFER LEARNING ===\")\n","\n","# Cargar resultados\n","results_csv_path_transfer = f\"{OUTPUT_PATH}/transfer_learning/results.csv\"\n","results_csv_path_finetune = f\"{OUTPUT_PATH}/fine_tuning/results.csv\"\n","\n","if not os.path.exists(results_csv_path_transfer):\n","    raise FileNotFoundError(f\"No se encontró results.csv en {results_csv_path_transfer}\")\n","if not os.path.exists(results_csv_path_finetune):\n","    raise FileNotFoundError(f\"No se encontró results.csv en {results_csv_path_finetune}\")\n","\n","# Leer el archivo CSV y limpiar nombres de columnas\n","transfer_df = pd.read_csv(results_csv_path_transfer)\n","transfer_df.columns = transfer_df.columns.str.strip() # Strip whitespace from column names\n","\n","finetune_df = pd.read_csv(results_csv_path_finetune)\n","finetune_df.columns = finetune_df.columns.str.strip() # Strip whitespace from column names\n","\n","\n","# Inspeccionar nombres de columnas\n","print(\"Columnas en results.csv (Transfer Learning):\")\n","print(transfer_df.columns.tolist())\n","print(\"Columnas en results.csv (Fine-Tuning):\")\n","print(finetune_df.columns.tolist())\n","\n","\n","# Definir nombres de columnas esperados (basado en la salida proporcionada, stripped)\n","# We need to make sure these keys match the stripped column names\n","column_mapping = {\n","    'train/box_loss': 'train/box_loss',\n","    'train/cls_loss': 'train/cls_loss',\n","    'train/dfl_loss': 'train/dfl_loss',\n","    'metrics/precision(B)': 'metrics/precision(B)',\n","    'metrics/recall(B)': 'metrics/recall(B)',\n","    'metrics/mAP50(B)': 'metrics/mAP50(B)',\n","    'metrics/mAP50-95(B)': 'metrics/mAP50-95(B)'\n","}\n","\n","# Verify and map columns (optional, as stripping should make names consistent)\n","# You can keep this verification or remove it if confident in stripping\n","available_columns_transfer = transfer_df.columns.tolist()\n","available_columns_finetune = finetune_df.columns.tolist()\n","\n","for expected_col, mapped_col in list(column_mapping.items()): # Use list() to allow modification\n","    if mapped_col not in available_columns_transfer or mapped_col not in available_columns_finetune:\n","         # Attempt a more flexible match if stripping isn't enough\n","         # Find a column that contains the essential part of the key\n","        key_part = expected_col.split('/')[-1].replace('(B)', '').strip()\n","        similar_cols_transfer = [col for col in available_columns_transfer if key_part in col]\n","        similar_cols_finetune = [col for col in available_columns_finetune if key_part in col]\n","\n","        if similar_cols_transfer and similar_cols_finetune and similar_cols_transfer[0] == similar_cols_finetune[0]:\n","            column_mapping[expected_col] = similar_cols_transfer[0]\n","            print(f\"Advertencia: Columna '{mapped_col}' no encontrada exactamente, usando '{similar_cols_transfer[0]}' en su lugar.\")\n","        else:\n","            # If flexible matching fails, check original columns before stripping for more context\n","            original_transfer_columns = pd.read_csv(results_csv_path_transfer).columns.tolist()\n","            original_finetune_columns = pd.read_csv(results_csv_path_finetune).columns.tolist()\n","            print(\"\\n--- Debug Info ---\")\n","            print(\"Original Transfer Columns:\", original_transfer_columns)\n","            print(\"Original Finetune Columns:\", original_finetune_columns)\n","            print(\"Available Stripped Transfer Columns:\", available_columns_transfer)\n","            print(\"Available Stripped Finetune Columns:\", available_columns_finetune)\n","            print(f\"Attempted to match key part: '{key_part}'\")\n","            print(f\"Similar columns found in Transfer: {similar_cols_transfer}\")\n","            print(f\"Similar columns found in Finetune: {similar_cols_finetune}\")\n","            print(\"------------------\\n\")\n","\n","            raise KeyError(f\"No se encontró una columna que coincida con '{expected_col}' en ambos archivos results.csv después de intentar limpiar nombres. Verifique los nombres de las columnas.\")\n","\n","# Extract metrics using the mapped names\n","try:\n","    metrics_to_plot = {\n","        'loss': {\n","            'transfer': transfer_df[column_mapping['train/box_loss']] + transfer_df[column_mapping['train/cls_loss']] + transfer_df[column_mapping['train/dfl_loss']],\n","            'finetune': finetune_df[column_mapping['train/box_loss']] + finetune_df[column_mapping['train/cls_loss']] + finetune_df[column_mapping['train/dfl_loss']]\n","        },\n","        'map50': {\n","            'transfer': transfer_df[column_mapping['metrics/mAP50(B)']],\n","            'finetune': finetune_df[column_mapping['metrics/mAP50(B)']]\n","        },\n","        'map50-95': {\n","            'transfer': transfer_df[column_mapping['metrics/mAP50-95(B)']],\n","            'finetune': finetune_df[column_mapping['metrics/mAP50-95(B)']]\n","        },\n","        'precision': {\n","            'transfer': transfer_df[column_mapping['metrics/precision(B)']],\n","            'finetune': finetune_df[column_mapping['metrics/precision(B)']]\n","        },\n","        'recall': {\n","            'transfer': transfer_df[column_mapping['metrics/recall(B)']],\n","            'finetune': finetune_df[column_mapping['metrics/recall(B)']]\n","        }\n","    }\n","except KeyError as e:\n","    print(f\"Error final al acceder a las columnas después del mapeo: {e}\")\n","    # Print the final mapping to see what was used\n","    print(\"Final Column Mapping Used:\", column_mapping)\n","    raise\n","\n","# Plotting the curves\n","plt.figure(figsize=(15, 10))\n","plt.subplot(2, 2, 1)\n","plt.plot(range(1, len(metrics_to_plot['loss']['transfer']) + 1), metrics_to_plot['loss']['transfer'], 'b-', label='Transfer Learning')\n","plt.plot(np.arange(len(metrics_to_plot['loss']['transfer']) + 1, len(metrics_to_plot['loss']['transfer']) + len(metrics_to_plot['loss']['finetune']) + 1),\n","         metrics_to_plot['loss']['finetune'], 'r-', label='Fine-Tuning')\n","plt.axvline(x=len(metrics_to_plot['loss']['transfer']), color='k', linestyle='--')\n","plt.xlabel('Época')\n","plt.ylabel('Pérdida Total')\n","plt.title('Evolución de la Pérdida')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(2, 2, 2)\n","plt.plot(range(1, len(metrics_to_plot['map50']['transfer']) + 1), metrics_to_plot['map50']['transfer'], 'b-', label='Transfer Learning')\n","plt.plot(np.arange(len(metrics_to_plot['map50']['transfer']) + 1, len(metrics_to_plot['map50']['transfer']) + len(metrics_to_plot['map50']['finetune']) + 1),\n","         metrics_to_plot['map50']['finetune'], 'r-', label='Fine-Tuning')\n","plt.axvline(x=len(metrics_to_plot['map50']['transfer']), color='k', linestyle='--')\n","plt.xlabel('Época')\n","plt.ylabel('mAP@0.5')\n","plt.title('Evolución del mAP@0.5')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(2, 2, 3)\n","plt.plot(range(1, len(metrics_to_plot['map50-95']['transfer']) + 1), metrics_to_plot['map50-95']['transfer'], 'b-', label='Transfer Learning')\n","plt.plot(np.arange(len(metrics_to_plot['map50-95']['transfer']) + 1, len(metrics_to_plot['map50-95']['transfer']) + len(metrics_to_plot['map50-95']['finetune']) + 1),\n","         metrics_to_plot['map50-95']['finetune'], 'r-', label='Fine-Tuning')\n","plt.axvline(x=len(metrics_to_plot['map50-95']['transfer']), color='k', linestyle='--')\n","plt.xlabel('Época')\n","plt.ylabel('mAP@0.5:0.95')\n","plt.title('Evolución del mAP@0.5:0.95')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.subplot(2, 2, 4)\n","plt.plot(range(1, len(metrics_to_plot['precision']['transfer']) + 1), metrics_to_plot['precision']['transfer'], 'b-', label='Precisión (Transfer)')\n","plt.plot(np.arange(len(metrics_to_plot['precision']['transfer']) + 1, len(metrics_to_plot['precision']['transfer']) + len(metrics_to_plot['precision']['finetune']) + 1),\n","         metrics_to_plot['precision']['finetune'], 'r-', label='Precisión (Fine-Tune)')\n","plt.plot(range(1, len(metrics_to_plot['recall']['transfer']) + 1), metrics_to_plot['recall']['transfer'], 'b--', label='Recall (Transfer)')\n","plt.plot(np.arange(len(metrics_to_plot['recall']['transfer']) + 1, len(metrics_to_plot['recall']['transfer']) + len(metrics_to_plot['recall']['finetune']) + 1),\n","         metrics_to_plot['recall']['finetune'], 'r--', label='Recall (Fine-Tune)')\n","plt.axvline(x=len(metrics_to_plot['precision']['transfer']), color='k', linestyle='--')\n","plt.xlabel('Época')\n","plt.ylabel('Valor')\n","plt.title('Evolución de Precisión y Recall')\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","\n","plt.tight_layout()\n","plt.savefig(f\"{RESULTS_PATH}/learning_curves_combined.png\")\n","plt.show()\n","\n","# Loggear curvas de aprendizaje\n","# Ensure the epoch ranges match the data\n","epochs_transfer = len(metrics_to_plot['loss']['transfer'])\n","epochs_finetune = len(metrics_to_plot['loss']['finetune'])\n","\n","wandb.log({\"combined_loss\": wandb.plot.line_series(\n","    xs=list(range(1, epochs_transfer + epochs_finetune + 1)),\n","    ys=[list(metrics_to_plot['loss']['transfer']) + list(metrics_to_plot['loss']['finetune'])],\n","    keys=[\"Pérdida\"],\n","    title=\"Evolución de la Pérdida\"\n",")})\n","\n","wandb.log({\"combined_map50\": wandb.plot.line_series(\n","    xs=list(range(1, epochs_transfer + epochs_finetune + 1)),\n","    ys=[list(metrics_to_plot['map50']['transfer']) + list(metrics_to_plot['map50']['finetune'])],\n","    keys=[\"mAP@0.5\"],\n","    title=\"Evolución del mAP@0.5\"\n",")})\n","\n","wandb.log({\"combined_map50-95\": wandb.plot.line_series(\n","    xs=list(range(1, epochs_transfer + epochs_finetune + 1)),\n","    ys=[list(metrics_to_plot['map50-95']['transfer']) + list(metrics_to_plot['map50-95']['finetune'])],\n","    keys=[\"mAP@0.5:0.95\"],\n","    title=\"Evolución del mAP@0.5:0.95\"\n",")})\n","\n","wandb.log({\"combined_precision\": wandb.plot.line_series(\n","    xs=list(range(1, epochs_transfer + epochs_finetune + 1)),\n","    ys=[list(metrics_to_plot['precision']['transfer']) + list(metrics_to_plot['precision']['finetune'])],\n","    keys=[\"Precisión\"],\n","    title=\"Evolución de la Precisión\"\n",")})\n","\n","wandb.log({\"combined_recall\": wandb.plot.line_series(\n","    xs=list(range(1, epochs_transfer + epochs_finetune + 1)),\n","    ys=[list(metrics_to_plot['recall']['transfer']) + list(metrics_to_plot['recall']['finetune'])],\n","    keys=[\"Recall\"],\n","    title=\"Evolución del Recall\"\n",")})"],"metadata":{"id":"V-C9BSH5GBI9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== ANÁLISIS DE FALSOS POSITIVOS Y FALSOS NEGATIVOS =====================\n","def analyze_error_cases(model, test_dir, test_labels_dir, conf_threshold=0.25, iou_threshold=0.5, max_samples=5):\n","    false_positives, false_negatives = [], []\n","    image_files = [f for f in os.listdir(test_dir) if f.endswith('.jpg')]\n","    np.random.shuffle(image_files)\n","    for img_file in tqdm(image_files, desc=\"Analizando errores\"):\n","        img_path = os.path.join(test_dir, img_file)\n","        label_path = os.path.join(test_labels_dir, img_file.replace('.jpg', '.txt'))\n","        img = cv2.imread(img_path)\n","        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n","        h, w, _ = img.shape\n","        gt_boxes = []\n","        if os.path.exists(label_path):\n","            with open(label_path, 'r') as f:\n","                for line in f.readlines():\n","                    values = line.strip().split()\n","                    if len(values) == 5:\n","                        _, x, y, width, height = map(float, values)\n","                        x1 = int((x - width / 2) * w)\n","                        y1 = int((y - height / 2) * h)\n","                        x2 = int((x + width / 2) * w)\n","                        y2 = int((y + height / 2) * h)\n","                        gt_boxes.append({'box': [x1, y1, x2, y2], 'matched': False})\n","        with torch.no_grad():\n","            results = model.predict(img_path, conf=conf_threshold, verbose=False)[0]\n","        pred_boxes = []\n","        for box in results.boxes:\n","            xyxy = box.xyxy[0].cpu().numpy().astype(int)\n","            conf = float(box.conf[0].cpu().numpy())\n","            pred_boxes.append({'box': xyxy, 'conf': conf, 'matched': False})\n","        for gt_idx, gt in enumerate(gt_boxes):\n","            best_iou = 0\n","            best_match = None\n","            for pred_idx, pred in enumerate(pred_boxes):\n","                if pred['matched']:\n","                    continue\n","                box1 = gt['box']\n","                box2 = pred['box']\n","                x_left = max(box1[0], box2[0])\n","                y_top = max(box1[1], box2[1])\n","                x_right = min(box1[2], box2[2])\n","                y_bottom = min(box1[3], box2[3])\n","                if x_right < x_left or y_bottom < y_top:\n","                    continue\n","                intersection_area = (x_right - x_left) * (y_bottom - y_top)\n","                box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n","                box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n","                iou = intersection_area / float(box1_area + box2_area - intersection_area)\n","                if iou > best_iou and iou >= iou_threshold:\n","                    best_iou = iou\n","                    best_match = pred_idx\n","            if best_match is not None:\n","                gt_boxes[gt_idx]['matched'] = True\n","                pred_boxes[best_match]['matched'] = True\n","            else:\n","                if len(false_negatives) < max_samples:\n","                    fn_img = img.copy()\n","                    cv2.rectangle(fn_img, (gt['box'][0], gt['box'][1]), (gt['box'][2], gt['box'][3]), (255, 0, 0), 2)\n","                    false_negatives.append({'image': fn_img, 'filename': img_file, 'box': gt['box']})\n","        for pred in pred_boxes:\n","            if not pred['matched']:\n","                if len(false_positives) < max_samples:\n","                    fp_img = img.copy()\n","                    cv2.rectangle(fp_img, (pred['box'][0], pred['box'][1]), (pred['box'][2], pred['box'][3]), (0, 0, 255), 2)\n","                    false_positives.append({'image': fp_img, 'filename': img_file, 'box': pred['box'], 'confidence': pred['conf']})\n","        if len(false_positives) >= max_samples and len(false_negatives) >= max_samples:\n","            break\n","    return false_positives, false_negatives\n","\n","print(\"\\n=== ANÁLISIS DE FALSOS POSITIVOS Y FALSOS NEGATIVOS ===\")\n","false_positives, false_negatives = analyze_error_cases(ft_model, images_test, labels_test)\n","if false_positives:\n","    plt.figure(figsize=(15, 5 * min(len(false_positives), 5)))\n","    for i, fp in enumerate(false_positives[:5]):\n","        plt.subplot(min(len(false_positives), 5), 1, i+1)\n","        plt.imshow(fp['image'])\n","        plt.title(f\"Falso Positivo en {fp['filename']} (Confianza: {fp['confidence']:.2f})\")\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.savefig(f\"{RESULTS_PATH}/false_positives.png\")\n","    plt.show()\n","else:\n","    print(\"No se encontraron falsos positivos en la muestra analizada.\")\n","if false_negatives:\n","    plt.figure(figsize=(15, 5 * min(len(false_negatives), 5)))\n","    for i, fn in enumerate(false_negatives[:5]):\n","        plt.subplot(min(len(false_negatives), 5), 1, i+1)\n","        plt.imshow(fn['image'])\n","        plt.title(f\"Falso Negativo en {fn['filename']}\")\n","        plt.axis('off')\n","    plt.tight_layout()\n","    plt.savefig(f\"{RESULTS_PATH}/false_negatives.png\")\n","    plt.show()\n","else:\n","    print(\"No se encontraron falsos negativos en la muestra analizada.\")\n","\n","wandb.log({\"false_positives_count\": len(false_positives), \"false_negatives_count\": len(false_negatives)})"],"metadata":{"id":"RXCTQg24GD98"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== COMPARACIÓN DE CONFIGURACIONES =====================\n","print(\"\\n=== COMPARACIÓN DE CONFIGURACIONES ===\")\n","transfer_df = pd.read_csv(f\"{OUTPUT_PATH}/transfer_learning/results.csv\")\n","finetune_df = pd.read_csv(f\"{OUTPUT_PATH}/fine_tuning/results.csv\")\n","config_summary = {\n","    'Transfer Learning': {\n","        'model': CONFIG['model_type'],\n","        'batch_size': CONFIG['batch_size'],\n","        'learning_rate': CONFIG['learning_rate_transfer'],\n","        'optimizer': 'SGD + Momentum',\n","        'epochs': CONFIG['epochs_transfer'],\n","        'final_map50': transfer_df['       metrics/mAP50(B)'].iloc[-1],\n","        'final_map50_95': transfer_df['    metrics/mAP50-95(B)'].iloc[-1],\n","        'final_precision': transfer_df['       metrics/precision(B)'].iloc[-1],\n","        'final_recall': transfer_df['          metrics/recall(B)'].iloc[-1]\n","    },\n","    'Fine-Tuning': {\n","        'model': CONFIG['model_type'],\n","        'batch_size': CONFIG['batch_size']//2,\n","        'learning_rate': CONFIG['learning_rate_finetune'],\n","        'optimizer': 'AdamW',\n","        'epochs': CONFIG['epochs_finetune'],\n","        'final_map50': finetune_df['       metrics/mAP50(B)'].iloc[-1],\n","        'final_map50_95': finetune_df['    metrics/mAP50-95(B)'].iloc[-1],\n","        'final_precision': finetune_df['       metrics/precision(B)'].iloc[-1],\n","        'final_recall': finetune_df['          metrics/recall(B)'].iloc[-1]\n","    }\n","}\n","\n","print(\"\\nComparación de configuraciones y resultados:\")\n","df_config = pd.DataFrame(config_summary).T\n","print(df_config)\n","\n","metrics_to_compare = ['final_map50', 'final_map50_95', 'final_precision', 'final_recall']\n","metrics_labels = {'final_map50': 'mAP@0.5', 'final_map50_95': 'mAP@0.5:0.95', 'final_precision': 'Precisión', 'final_recall': 'Recall'}\n","plt.figure(figsize=(12, 6))\n","x = np.arange(len(metrics_to_compare))\n","width = 0.35\n","transfer_values = [config_summary['Transfer Learning'][m] for m in metrics_to_compare]\n","finetune_values = [config_summary['Fine-Tuning'][m] for m in metrics_to_compare]\n","plt.bar(x - width/2, transfer_values, width, label='Transfer Learning')\n","plt.bar(x + width/2, finetune_values, width, label='Fine-Tuning')\n","plt.xlabel('Métrica')\n","plt.ylabel('Valor')\n","plt.title('Comparación de Métricas entre Transfer Learning y Fine-Tuning')\n","plt.xticks(x, [metrics_labels[m] for m in metrics_to_compare])\n","plt.legend()\n","plt.grid(True, alpha=0.3)\n","for i, v in enumerate(transfer_values):\n","    plt.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center')\n","for i, v in enumerate(finetune_values):\n","    plt.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center')\n","plt.tight_layout()\n","plt.savefig(f\"{RESULTS_PATH}/metrics_comparison.png\")\n","plt.show()\n","\n","wandb.log({\"config_comparison\": wandb.Table(dataframe=df_config)})"],"metadata":{"id":"XktSle2MGGGb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== ESTIMACIÓN DE LA CAPACIDAD DE GENERALIZACIÓN =====================\n","print(\"\\n=== ESTIMACIÓN DE LA CAPACIDAD DE GENERALIZACIÓN ===\")\n","def evaluate_generalization():\n","    val_results = ft_model.val(data=custom_yaml_path, split='val')\n","    test_results = ft_model.val(data=custom_yaml_path, split='test')\n","    val_metrics = val_results.results_dict\n","    test_metrics = test_results.results_dict\n","    metrics_to_compare = [\n","        ('metrics/mAP50(B)', 'mAP@0.5'),\n","        ('metrics/mAP50-95(B)', 'mAP@0.5:0.95'),\n","        ('metrics/precision(B)', 'Precisión'),\n","        ('metrics/recall(B)', 'Recall')\n","    ]\n","    differences, names, val_values, test_values = [], [], [], []\n","    for metric_key, metric_name in metrics_to_compare:\n","        val_value = val_metrics[metric_key]\n","        test_value = test_metrics[metric_key]\n","        rel_diff = (val_value - test_value) / (val_value + 1e-6) * 100\n","        differences.append(rel_diff)\n","        names.append(metric_name)\n","        val_values.append(val_value)\n","        test_values.append(test_value)\n","    return names, val_values, test_values, differences\n","\n","try:\n","    metric_names, val_values, test_values, differences = evaluate_generalization()\n","    gen_data = {\n","        'Métrica': metric_names,\n","        'Validación': val_values,\n","        'Prueba': test_values,\n","        'Diferencia (%)': differences\n","    }\n","    gen_df = pd.DataFrame(gen_data)\n","    print(\"\\nEvaluación de generalización:\")\n","    print(gen_df)\n","\n","    plt.figure(figsize=(10, 6))\n","    x = np.arange(len(metric_names))\n","    width = 0.35\n","    plt.bar(x - width/2, val_values, width, label='Validación')\n","    plt.bar(x + width/2, test_values, width, label='Prueba')\n","    plt.xlabel('Métrica')\n","    plt.ylabel('Valor')\n","    plt.title('Comparación entre Validación y Prueba')\n","    plt.xticks(x, metric_names)\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    for i, v in enumerate(val_values):\n","        plt.text(i - width/2, v + 0.01, f'{v:.3f}', ha='center')\n","    for i, v in enumerate(test_values):\n","        plt.text(i + width/2, v + 0.01, f'{v:.3f}', ha='center')\n","    plt.tight_layout()\n","    plt.savefig(f\"{RESULTS_PATH}/validation_test_comparison.png\")\n","    plt.show()\n","\n","    plt.figure(figsize=(10, 6))\n","    colors = ['green' if d < 5 else 'orange' if d < 10 else 'red' for d in np.abs(differences)]\n","    plt.bar(metric_names, np.abs(differences), color=colors)\n","    plt.axhline(y=5, color='r', linestyle='--', label='Umbral 5%')\n","    plt.xlabel('Métrica')\n","    plt.ylabel('Diferencia Absoluta (%)')\n","    plt.title('Gap entre Validación y Prueba')\n","    plt.grid(True, alpha=0.3)\n","    plt.legend()\n","    for i, v in enumerate(differences):\n","        plt.text(i, abs(v) + 0.5, f'{abs(v):.2f}%', ha='center')\n","    plt.tight_layout()\n","    plt.savefig(f\"{RESULTS_PATH}/generalization_gap.png\")\n","    plt.show()\n","\n","    avg_diff = np.mean(np.abs(differences))\n","    print(\"\\nInterpretación de la capacidad de generalización:\")\n","    if avg_diff < 5:\n","        print(\"✅ EXCELENTE GENERALIZACIÓN: Diferencia promedio < 5%.\")\n","    elif avg_diff < 10:\n","        print(\"⚠️ BUENA GENERALIZACIÓN: Diferencia promedio entre 5-10%.\")\n","    else:\n","        print(\"❌ POSIBLE SOBREAJUSTE: Diferencia promedio > 10%.\")\n","except Exception as e:\n","    print(f\"No se pudo realizar la evaluación de generalización: {e}\")\n","\n","wandb.log({\"generalization_metrics\": wandb.Table(dataframe=gen_df)})"],"metadata":{"id":"dwB1f43BGH4D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== EXPORTACIÓN DEL MODELO FINAL =====================\n","print(\"\\n=== EXPORTACIÓN DEL MODELO FINAL ===\")\n","\n","# Definir formatos de exportación (excluyendo 'pt')\n","formats = ['torchscript', 'onnx', 'openvino', 'tflite']\n","export_paths = {}\n","model_path = f\"{OUTPUT_PATH}/fine_tuning/weights/best.pt\"  # Ruta del modelo entrenado\n","ft_model = YOLO(model_path)\n","\n","# Asegurar que la GPU se use si está disponible\n","device = 0 if torch.cuda.is_available() else 'cpu'\n","print(f\"Usando dispositivo para exportación: {'GPU' if device == 0 else 'CPU'}\")\n","\n","for fmt in formats:\n","    try:\n","        # Definir la ruta esperada según el formato\n","        if fmt in ['torchscript', 'onnx']:\n","            expected_path = f\"{OUTPUT_PATH}/fine_tuning/weights/best.{fmt}\"\n","        elif fmt == 'openvino':\n","            expected_path = f\"{OUTPUT_PATH}/fine_tuning/weights/best_openvino_model\"\n","        elif fmt == 'tflite':\n","            expected_path = f\"{OUTPUT_PATH}/fine_tuning/weights/best.tflite\"\n","        else:\n","            expected_path = f\"{OUTPUT_PATH}/final_model_{fmt}.{fmt}\"\n","\n","        # Exportar el modelo\n","        ft_model.export(\n","            format=fmt,\n","            imgsz=CONFIG['img_size'],\n","            device=device,\n","            optimize=False if fmt in ['onnx', 'openvino'] else True,\n","            dynamic=False,\n","            half=False,  # Evitar FP16 en exportación para compatibilidad\n","            workspace=4.0 if fmt == 'tflite' else None  # Optimización para TFLite\n","        )\n","\n","        # Verificar que el archivo o directorio se creó\n","        if fmt == 'openvino':\n","            # OpenVINO crea un directorio\n","            if os.path.isdir(expected_path):\n","                export_paths[fmt] = expected_path\n","                print(f\"Modelo exportado exitosamente en formato {fmt}: {expected_path}\")\n","            else:\n","                print(f\"Advertencia: No se encontró el directorio exportado para {fmt}\")\n","        else:\n","            # Otros formatos crean un archivo\n","            if os.path.isfile(expected_path):\n","                export_paths[fmt] = expected_path\n","                print(f\"Modelo exportado exitosamente en formato {fmt}: {expected_path}\")\n","            else:\n","                print(f\"Advertencia: No se encontró el archivo exportado para {fmt}\")\n","\n","    except Exception as e:\n","        print(f\"Error al exportar en formato {fmt}: {str(e)}\")\n","        export_paths[fmt] = None\n","\n","# Loggear rutas de exportación en W&B\n","wandb.log({\"exported_model_paths\": export_paths})\n","\n","# Verificar que al menos un formato se exportó correctamente\n","if not any(export_paths.values()):\n","    print(\"⚠️ Advertencia: No se exportó correctamente ningún formato del modelo.\")\n","else:\n","    print(\"Exportación completada. Rutas de los modelos exportados:\")\n","    for fmt, path in export_paths.items():\n","        if path:\n","            print(f\"- {fmt}: {path}\")\n","\n","# Liberar memoria\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"N0Fr834AGLN0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ===================== INFORME FINAL =====================\n","print(\"\\n=== INFORME FINAL ===\")\n","report = f\"\"\"\n","REPORTE DE ENTRENAMIENTO - DETECCIÓN DE PERSONAS\n","Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n","Duración total del entrenamiento: {(time.time() - wandb.run.start_time) / 3600:.2f} horas\n","\n","1. CONFIGURACIÓN\n","- Modelo: {CONFIG['model_type']}\n","- Tamaño de imagen: {CONFIG['img_size']}x{CONFIG['img_size']}\n","- Transfer Learning: {CONFIG['epochs_transfer']} épocas, LR={CONFIG['learning_rate_transfer']}, Batch={CONFIG['batch_size']}\n","- Fine-Tuning: {CONFIG['epochs_finetune']} épocas, LR={CONFIG['learning_rate_finetune']}, Batch={CONFIG['batch_size']//2}\n","- Dataset: {train_images_count} imágenes (train), {val_images_count} (val), {test_images_count} (test)\n","\n","2. MÉTRICAS FINALES (TEST)\n","- Precisión: {val_metrics['metrics/precision(B)']:.4f}\n","- Recall: {val_metrics['metrics/recall(B)']:.4f}\n","- F1-Score: {f1_score:.4f}\n","- mAP@0.5: {val_metrics['metrics/mAP50(B)']:.4f}\n","- mAP@0.5:0.95: {val_metrics['metrics/mAP50-95(B)']:.4f}\n","- TPR: {tpr:.4f}\n","- FPR: {fpr:.4f}\n","\n","3. ANÁLISIS DE ERRORES\n","- Falsos Positivos: {len(false_positives)}\n","- Falsos Negativos: {len(false_negatives)}\n","\n","4. RENDIMIENTO POR TAMAÑO\n","{chr(10).join([f\"- {size.capitalize()}: Precisión={metrics['precision']:.4f}, Recall={metrics['recall']:.4f}, F1={metrics['f1']:.4f}\" for size, metrics in size_metrics.items()])}\n","\n","5. VELOCIDAD DE INFERENCIA\n","{chr(10).join([f\"- Batch {bs}: {metrics['fps']:.2f} FPS\" for bs, metrics in speed_results.items()])}\n","\n","6. MODELOS EXPORTADOS\n","{chr(10).join([f\"- {fmt.upper()}: {path if path else 'No exportado'}\" for fmt, path in export_paths.items()])}\n","\n","7. CAPACIDAD DE GENERALIZACIÓN\n","- Diferencia promedio: {avg_diff:.2f}% {'(Excelente)' if avg_diff < 5 else '(Buena)' if avg_diff < 10 else '(Revisar posible sobreajuste)'}\n","\n","Resultados y gráficos guardados en: {RESULTS_PATH}\n","Modelos guardados en: {OUTPUT_PATH}\n","\"\"\"\n","\n","# Imprimir y guardar informe\n","print(report)\n","report_path = f\"{RESULTS_PATH}/training_report.txt\"\n","with open(report_path, 'w') as f:\n","    f.write(report)\n","print(f\"Informe guardado en: {report_path}\")\n","\n","# Subir informe a W&B\n","wandb.save(report_path)\n","\n","# ===================== LIMPIEZA Y FINALIZACIÓN =====================\n","print(\"\\n=== FINALIZANDO ===\")\n","# Liberar memoria\n","torch.cuda.empty_cache()\n","gc.collect()\n","\n","# Cerrar W&B\n","wandb.finish()\n","print(\"Entrenamiento completado exitosamente. Todos los recursos han sido liberados.\")\n","\n","# Verificar espacio en Drive\n","import subprocess\n","space_output = subprocess.check_output(f\"du -sh {OUTPUT_PATH}\", shell=True).decode().split()[0]\n","print(f\"Espacio utilizado en {OUTPUT_PATH}: {space_output}\")"],"metadata":{"id":"WzXse6R1GPVz"},"execution_count":null,"outputs":[]}]}