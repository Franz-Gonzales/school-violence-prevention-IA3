{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["# Instalaci√≥n de dependencias\n","!pip install transformers\n","!pip install datasets\n","!pip install decord\n","!pip install scikit-learn\n","!pip install matplotlib\n","!pip install seaborn\n","!pip install pandas\n","!pip install tqdm\n","!pip install scipy\n","!pip install torchmetrics\n","!pip install timm\n","!pip install av\n","!pip install einops\n","!pip install evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KlMSrGEwj-Wh","outputId":"129f7f48-7b7a-45ed-e1de-4c303137325d","executionInfo":{"status":"ok","timestamp":1747642969518,"user_tz":240,"elapsed":34963,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: decord in /usr/local/lib/python3.11/dist-packages (0.6.0)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord) (2.0.2)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n","Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n","Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.11/dist-packages (1.7.1)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n","Requirement already satisfied: lightning-utilities>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (0.14.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n","Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n","Requirement already satisfied: av in /usr/local/lib/python3.11/dist-packages (14.4.0)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n","Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n"]}]},{"cell_type":"code","source":["# Importar bibliotecas necesarias\n","import os\n","import random\n","import math\n","import time\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","from datetime import datetime\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR\n","from torchvision import transforms\n","from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score, BinarySpecificity\n","from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from transformers import TimesformerForVideoClassification, TimesformerConfig, AutoImageProcessor\n","from transformers import get_cosine_schedule_with_warmup\n","import decord\n","from decord import VideoReader, cpu\n","import av\n","import gc\n","import warnings\n","import random\n","import io\n","import zipfile\n","import logging\n","import json\n","from pathlib import Path\n"],"metadata":{"id":"Afo36OBfkCKI","executionInfo":{"status":"ok","timestamp":1747642988876,"user_tz":240,"elapsed":17203,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Montar Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3iG0bNlkIfx","outputId":"6456831f-a0a7-4ebe-b5ae-8fe195778df6","executionInfo":{"status":"ok","timestamp":1747642991608,"user_tz":240,"elapsed":1585,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}}},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# Configurar advertencias\n","warnings.filterwarnings('ignore')\n","\n","# Configurar logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","\n","# Verificar disponibilidad de GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Usando dispositivo: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_MnaOy73kHBy","outputId":"96fe1c38-3ff2-4e9e-c643-2915a5f6d220","executionInfo":{"status":"ok","timestamp":1747642993844,"user_tz":240,"elapsed":37,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Usando dispositivo: cuda\n"]}]},{"cell_type":"code","source":["# ============================== CONFIGURACI√ìN DE HIPERPAR√ÅMETROS ==============================\n","\n","# Hiperpar√°metros generales\n","CONFIG = {\n","    # Rutas y nombres\n","    \"dataset_path\": \"/content/drive/MyDrive/dataset_violencia\",  # Ajustar seg√∫n la ubicaci√≥n real\n","    \"output_dir\": \"/content/drive/MyDrive/Carpeta_Proyecto_IA3/modelo_timesformer2\",\n","    \"model_name\": \"timesformer_violence_detector\",\n","\n","    # Par√°metros del modelo\n","    \"pretrained_model\": \"facebook/timesformer-base-finetuned-k400\",\n","    \"num_frames\": 8,              # N√∫mero de frames a procesar\n","    \"image_size\": 224,             # Tama√±o de los frames (224x224)\n","    \"num_classes\": 2,              # Violencia / No violencia\n","\n","    # Par√°metros de entrenamiento - Transfer Learning\n","    \"tl_batch_size\": 8,            # Tama√±o del batch\n","    \"tl_num_epochs\": 10,           # N√∫mero de √©pocas\n","    \"tl_learning_rate\": 5e-5,      # Learning rate inicial\n","    \"tl_weight_decay\": 1e-4,       # Regularizaci√≥n L2\n","    \"tl_dropout\": 0.2,             # Tasa de dropout\n","    \"tl_warmup_ratio\": 0.1,        # Proporci√≥n de steps para warmup\n","\n","    # Par√°metros de entrenamiento - Fine-Tuning\n","    \"ft_batch_size\": 4,            # Tama√±o del batch (m√°s peque√±o para fine-tuning)\n","    \"ft_num_epochs\": 5,            # N√∫mero de √©pocas adicionales\n","    \"ft_learning_rate\": 1e-5,      # Learning rate m√°s bajo para fine-tuning\n","    \"ft_weight_decay\": 5e-5,       # Regularizaci√≥n L2 suave\n","\n","    # Umbral de clasificaci√≥n\n","    \"threshold\": 0.6,              # Umbral de decisi√≥n para la clasificaci√≥n\n","\n","    # Configuraci√≥n de checkpoints\n","    \"save_steps\": 100,             # Guardar cada X pasos\n","    \"save_total_limit\": 3,         # M√°ximo n√∫mero de checkpoints a mantener\n","    \"save_best_only\": True,        # Guardar solo el mejor modelo\n","\n","    # M√©tricas y evaluaci√≥n\n","    \"eval_steps\": 50,              # Evaluar cada X pasos\n","    \"logging_steps\": 10,           # Mostrar m√©tricas cada X pasos\n","\n","    # Otros par√°metros\n","    \"seed\": 42,                    # Semilla para reproducibilidad\n","    \"mixed_precision\": True,       # Usar precisi√≥n mixta para acelerar entrenamiento\n","}\n","\n","# Crear directorio de salida si no existe\n","os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n","\n","# Guardar configuraci√≥n\n","with open(os.path.join(CONFIG[\"output_dir\"], \"config.json\"), 'w') as f:\n","    json.dump(CONFIG, f, indent=4)\n","\n","# Configurar reproducibilidad\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(CONFIG[\"seed\"])"],"metadata":{"id":"0Z7ad0SlnuLJ","executionInfo":{"status":"ok","timestamp":1747642995968,"user_tz":240,"elapsed":11,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ============================== CLASES PARA EL DATASET Y PROCESAMIENTO ==============================\n","\n","# Clase para procesar y cargar los videos\n","class ViolenceVideoDataset(Dataset):\n","    def __init__(self, root_dir, split='train', transform=None, num_frames=16, image_size=224, max_videos=None):\n","        \"\"\"\n","        Dataset para clasificaci√≥n de violencia en videos\n","\n","        Args:\n","            root_dir: Directorio ra√≠z del dataset\n","            split: 'train', 'val' o 'test'\n","            transform: Transformaciones a aplicar\n","            num_frames: N√∫mero de frames a extraer de cada video\n","            image_size: Tama√±o de los frames\n","            max_videos: Limitar n√∫mero de videos (para pruebas r√°pidas)\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.split = split\n","        self.transform = transform\n","        self.num_frames = num_frames\n","        self.image_size = image_size\n","\n","        self.processor = AutoImageProcessor.from_pretrained(CONFIG[\"pretrained_model\"])\n","\n","        # Obtener las rutas de videos y etiquetas\n","        violence_dir = os.path.join(root_dir, split, 'violence')\n","        no_violence_dir = os.path.join(root_dir, split, 'no_violence')\n","\n","        # Verificar que los directorios existan\n","        if not os.path.exists(violence_dir) or not os.path.exists(no_violence_dir):\n","            raise ValueError(f\"No se encontraron los directorios del dataset en {root_dir}/{split}\")\n","\n","        violence_videos = glob.glob(os.path.join(violence_dir, '*.mp4'))\n","        no_violence_videos = glob.glob(os.path.join(no_violence_dir, '*.mp4'))\n","\n","        if len(violence_videos) == 0 or len(no_violence_videos) == 0:\n","            raise ValueError(f\"No se encontraron videos en {violence_dir} o {no_violence_dir}\")\n","\n","        # Limitar videos si es necesario\n","        if max_videos is not None:\n","            max_per_class = max_videos // 2\n","            violence_videos = violence_videos[:max_per_class]\n","            no_violence_videos = no_violence_videos[:max_per_class]\n","\n","        self.video_paths = violence_videos + no_violence_videos\n","        self.labels = [1] * len(violence_videos) + [0] * len(no_violence_videos)\n","\n","        # Mezclar los datos manteniendo correspondencia entre paths y labels\n","        combined = list(zip(self.video_paths, self.labels))\n","        random.shuffle(combined)\n","        self.video_paths, self.labels = zip(*combined)\n","\n","        # Convertir a lista\n","        self.video_paths = list(self.video_paths)\n","        self.labels = list(self.labels)\n","\n","        print(f\"Cargados {len(self.video_paths)} videos para split '{split}'\")\n","        print(f\"Violencia: {len(violence_videos)}, No Violencia: {len(no_violence_videos)}\")\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def sample_frames_from_video(self, video_path):\n","        \"\"\"Extrae frames uniformemente espaciados del video\"\"\"\n","        try:\n","            # Usar decord para cargar el video eficientemente\n","            video_reader = VideoReader(video_path, ctx=cpu(0))\n","            total_frames = len(video_reader)\n","\n","            if total_frames == 0:\n","                raise ValueError(f\"Video vac√≠o o corrupto: {video_path}\")\n","\n","            # Seleccionar frames uniformemente\n","            indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n","            frames = video_reader.get_batch(indices).asnumpy()  # (num_frames, H, W, C)\n","\n","            # Aplicar resize y normalizaci√≥n\n","            processed_frames = []\n","            for frame in frames:\n","                # Redimensionar\n","                frame = transforms.functional.resize(\n","                    transforms.functional.to_tensor(frame),\n","                    (self.image_size, self.image_size)\n","                )\n","                processed_frames.append(frame)\n","\n","            # Apilar frames\n","            frames_tensor = torch.stack(processed_frames)  # (T, C, H, W)\n","\n","            # Mover dimensiones para coincidir con lo que espera el modelo (B, C, T, H, W)\n","            frames_tensor = frames_tensor.permute(1, 0, 2, 3).unsqueeze(0)\n","\n","            return frames_tensor\n","\n","        except Exception as e:\n","            logger.error(f\"Error al procesar video {video_path}: {str(e)}\")\n","            # Retornar un tensor de ceros en caso de error\n","            return torch.zeros((1, 3, self.num_frames, self.image_size, self.image_size))\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Obtiene un item por su √≠ndice\"\"\"\n","        video_path = self.video_paths[idx]\n","        label = self.labels[idx]\n","\n","        # Extraer frames\n","        frames = self.sample_frames_from_video(video_path)\n","\n","        # Preprocesar frames usando el procesador de TimeSformer\n","        try:\n","            frames_list = list(frames.squeeze(0).permute(1, 0, 2, 3))  # Convertir a lista de tensores (T, C, H, W)\n","            inputs = self.processor(frames_list, return_tensors=\"pt\", do_rescale=False)\n","            pixel_values = inputs['pixel_values'].squeeze(0)  # Eliminar dim de batch\n","        except Exception as e:\n","            logger.error(f\"Error al procesar frames del video {video_path}: {str(e)}\")\n","            # Crear input vac√≠o de tama√±o correcto en caso de error\n","            pixel_values = torch.zeros((3, self.num_frames, self.image_size, self.image_size))\n","\n","        return {\n","            'pixel_values': pixel_values,\n","            'labels': torch.tensor(label, dtype=torch.long),\n","            'video_path': video_path\n","        }\n","\n","# ============================== FUNCIONES DE ENTRENAMIENTO Y EVALUACI√ìN ==============================\n","\n","def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch, config):\n","    \"\"\"Entrena el modelo durante una √©poca completa\"\"\"\n","    model.train()\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    epoch_precision = 0\n","    epoch_recall = 0\n","    epoch_f1 = 0\n","\n","    # M√©tricas\n","    accuracy_metric = BinaryAccuracy().to(device)\n","    precision_metric = BinaryPrecision().to(device)\n","    recall_metric = BinaryRecall().to(device)\n","    f1_metric = BinaryF1Score().to(device)\n","\n","    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"√âpoca {epoch+1}\")\n","\n","    for step, batch in progress_bar:\n","        try:\n","            # Mover datos al dispositivo\n","            pixel_values = batch['pixel_values'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            # Forward pass\n","            outputs = model(pixel_values=pixel_values, labels=labels)\n","            loss = outputs.loss\n","\n","            # Backward pass\n","            optimizer.zero_grad()\n","            loss.backward()\n","\n","            # Clip gradient norm para estabilidad\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            if scheduler is not None:\n","                scheduler.step()\n","\n","            # Calcular m√©tricas\n","            logits = outputs.logits\n","            preds = torch.sigmoid(logits[:, 1])  # Solo necesitamos la probabilidad de 'violencia'\n","\n","            accuracy = accuracy_metric(preds, labels)\n","            precision = precision_metric(preds, labels)\n","            recall = recall_metric(preds, labels)\n","            f1 = f1_metric(preds, labels)\n","\n","            # Acumular m√©tricas\n","            epoch_loss += loss.item()\n","            epoch_acc += accuracy.item()\n","            epoch_precision += precision.item()\n","            epoch_recall += recall.item()\n","            epoch_f1 += f1.item()\n","\n","            # Actualizar progreso\n","            progress_bar.set_postfix({\n","                'loss': loss.item(),\n","                'acc': accuracy.item(),\n","                'prec': precision.item(),\n","                'rec': recall.item(),\n","                'f1': f1.item()\n","            })\n","\n","            # Liberar memoria expl√≠citamente\n","            del pixel_values, labels, outputs, loss, logits, preds\n","            torch.cuda.empty_cache()\n","\n","            # Guardar checkpoint cada ciertos pasos\n","            if (step + 1) % config[\"save_steps\"] == 0:\n","                checkpoint_path = os.path.join(\n","                    config[\"output_dir\"],\n","                    f\"checkpoint_epoch{epoch+1}_step{step+1}.pt\"\n","                )\n","                torch.save({\n","                    'epoch': epoch,\n","                    'step': step,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","                    'loss': loss.item(),\n","                }, checkpoint_path)\n","                logger.info(f\"Guardado checkpoint en {checkpoint_path}\")\n","\n","        except Exception as e:\n","            logger.error(f\"Error en paso {step}, √©poca {epoch+1}: {str(e)}\")\n","            # Intentar liberar memoria y continuar\n","            torch.cuda.empty_cache()\n","            continue\n","\n","    # Calcular m√©tricas promedio\n","    num_batches = len(dataloader)\n","    epoch_loss /= num_batches\n","    epoch_acc /= num_batches\n","    epoch_precision /= num_batches\n","    epoch_recall /= num_batches\n","    epoch_f1 /= num_batches\n","\n","    return {\n","        'loss': epoch_loss,\n","        'accuracy': epoch_acc,\n","        'precision': epoch_precision,\n","        'recall': epoch_recall,\n","        'f1': epoch_f1\n","    }\n","\n","def evaluate(model, dataloader, criterion, device, config):\n","    \"\"\"Eval√∫a el modelo en un conjunto de datos\"\"\"\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    val_loss = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Evaluando\"):\n","            try:\n","                # Mover datos al dispositivo\n","                pixel_values = batch['pixel_values'].to(device)\n","                labels = batch['labels'].to(device)\n","\n","                # Forward pass\n","                outputs = model(pixel_values=pixel_values, labels=labels)\n","                loss = outputs.loss\n","                val_loss += loss.item()\n","\n","                # Obtener predicciones\n","                logits = outputs.logits\n","                preds = torch.sigmoid(logits[:, 1])  # Solo la probabilidad de 'violencia'\n","\n","                # Guardar predicciones y etiquetas\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","                # Liberar memoria\n","                del pixel_values, labels, outputs, loss, logits, preds\n","                torch.cuda.empty_cache()\n","\n","            except Exception as e:\n","                logger.error(f\"Error al evaluar batch: {str(e)}\")\n","                continue\n","\n","    # Convertir a arrays numpy\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","\n","    if len(all_preds) == 0 or len(all_labels) == 0:\n","        logger.error(\"No se pudieron obtener predicciones o etiquetas durante la evaluaci√≥n\")\n","        return {\n","            'loss': float('inf'),\n","            'accuracy': 0,\n","            'precision': 0,\n","            'recall': 0,\n","            'specificity': 0,\n","            'f1': 0,\n","            'roc_auc': 0,\n","            'confusion_matrix': np.zeros((2, 2)),\n","            'fpr': np.array([0, 1]),\n","            'tpr': np.array([0, 0]),\n","            'predictions': np.array([]),\n","            'labels': np.array([])\n","        }\n","\n","    # Calcular m√©tricas\n","    binary_preds = (all_preds >= config[\"threshold\"]).astype(int)\n","\n","    accuracy = accuracy_score(all_labels, binary_preds)\n","    precision = precision_score(all_labels, binary_preds, zero_division=0)\n","    recall = recall_score(all_labels, binary_preds, zero_division=0)\n","    f1 = f1_score(all_labels, binary_preds, zero_division=0)\n","\n","    # Calcular especificidad (TN / (TN + FP))\n","    tn, fp, fn, tp = confusion_matrix(all_labels, binary_preds, labels=[0, 1]).ravel()\n","    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n","\n","    # Calcular m√©tricas de curva ROC\n","    try:\n","        fpr, tpr, _ = roc_curve(all_labels, all_preds)\n","        roc_auc = auc(fpr, tpr)\n","    except Exception as e:\n","        logger.error(f\"Error al calcular curva ROC: {str(e)}\")\n","        fpr, tpr = np.array([0, 1]), np.array([0, 0])\n","        roc_auc = 0\n","\n","    # Matriz de confusi√≥n\n","    cm = confusion_matrix(all_labels, binary_preds, labels=[0, 1])\n","\n","    # P√©rdida promedio\n","    val_loss /= len(dataloader)\n","\n","    # Crear informe de evaluaci√≥n\n","    eval_results = {\n","        'loss': val_loss,\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,  # Sensibilidad\n","        'specificity': specificity,\n","        'f1': f1,\n","        'roc_auc': roc_auc,\n","        'confusion_matrix': cm,\n","        'fpr': fpr,\n","        'tpr': tpr,\n","        'predictions': all_preds,\n","        'labels': all_labels\n","    }\n","\n","    return eval_results\n","\n","def plot_metrics(train_metrics, val_metrics, config):\n","    \"\"\"Genera gr√°ficos de m√©tricas de entrenamiento\"\"\"\n","    metrics_to_plot = ['loss', 'accuracy', 'precision', 'recall', 'f1']\n","    epochs = range(1, len(train_metrics['loss']) + 1)\n","\n","    plt.figure(figsize=(20, 15))\n","\n","    for i, metric in enumerate(metrics_to_plot):\n","        plt.subplot(3, 2, i+1)\n","        plt.plot(epochs, train_metrics[metric], 'b-', label=f'Training {metric}')\n","        plt.plot(epochs, val_metrics[metric], 'r-', label=f'Validation {metric}')\n","        plt.title(f'{metric.capitalize()} vs. Epochs')\n","        plt.xlabel('Epochs')\n","        plt.ylabel(metric.capitalize())\n","        plt.legend()\n","        plt.grid(True)\n","\n","    # Guardar figura\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config[\"output_dir\"], \"training_metrics.png\"))\n","    plt.close()\n","\n","def plot_confusion_matrix(cm, config, phase='transfer_learning'):\n","    \"\"\"Visualiza la matriz de confusi√≥n\"\"\"\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=['No Violencia', 'Violencia'],\n","                yticklabels=['No Violencia', 'Violencia'])\n","    plt.xlabel('Predicci√≥n')\n","    plt.ylabel('Real')\n","    plt.title('Matriz de Confusi√≥n')\n","\n","    # Guardar figura\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config[\"output_dir\"], f\"confusion_matrix_{phase}.png\"))\n","    plt.close()\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, config, phase='transfer_learning'):\n","    \"\"\"Visualiza la curva ROC\"\"\"\n","    plt.figure(figsize=(10, 8))\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC)')\n","    plt.legend(loc=\"lower right\")\n","\n","    # Guardar figura\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config[\"output_dir\"], f\"roc_curve_{phase}.png\"))\n","    plt.close()\n","\n","def save_evaluation_report(eval_results, config, phase='transfer_learning'):\n","    \"\"\"Guarda un informe detallado de la evaluaci√≥n\"\"\"\n","    report = {\n","        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n","        'phase': phase,\n","        'metrics': {\n","            'loss': float(eval_results['loss']),\n","            'accuracy': float(eval_results['accuracy']),\n","            'precision': float(eval_results['precision']),\n","            'recall': float(eval_results['recall']),\n","            'specificity': float(eval_results['specificity']),\n","            'f1_score': float(eval_results['f1']),\n","            'roc_auc': float(eval_results['roc_auc']),\n","        },\n","        'confusion_matrix': eval_results['confusion_matrix'].tolist(),\n","    }\n","\n","    # Guardar informe en formato JSON\n","    with open(os.path.join(config[\"output_dir\"], f\"evaluation_report_{phase}.json\"), 'w') as f:\n","        json.dump(report, f, indent=4)\n","\n","    # Tambi√©n guardar en formato de texto para mejor legibilidad\n","    with open(os.path.join(config[\"output_dir\"], f\"evaluation_report_{phase}.txt\"), 'w') as f:\n","        f.write(f\"Evaluaci√≥n del Modelo - Fase: {phase}\\n\")\n","        f.write(f\"Fecha: {report['timestamp']}\\n\")\n","        f.write(\"\\n=== M√©tricas ===\\n\")\n","        f.write(f\"Loss: {report['metrics']['loss']:.4f}\\n\")\n","        f.write(f\"Accuracy: {report['metrics']['accuracy']:.4f}\\n\")\n","        f.write(f\"Precision: {report['metrics']['precision']:.4f}\\n\")\n","        f.write(f\"Recall (Sensibilidad): {report['metrics']['recall']:.4f}\\n\")\n","        f.write(f\"Specificity: {report['metrics']['specificity']:.4f}\\n\")\n","        f.write(f\"F1-Score: {report['metrics']['f1_score']:.4f}\\n\")\n","        f.write(f\"ROC AUC: {report['metrics']['roc_auc']:.4f}\\n\")\n","        f.write(\"\\n=== Matriz de Confusi√≥n ===\\n\")\n","        f.write(\"                Pred: No Violencia  Pred: Violencia\\n\")\n","        f.write(f\"Real: No Violencia    {eval_results['confusion_matrix'][0][0]}               {eval_results['confusion_matrix'][0][1]}\\n\")\n","        f.write(f\"Real: Violencia       {eval_results['confusion_matrix'][1][0]}               {eval_results['confusion_matrix'][1][1]}\\n\")"],"metadata":{"id":"A_RQ45EhnyXZ","executionInfo":{"status":"ok","timestamp":1747643002739,"user_tz":240,"elapsed":90,"user":{"displayName":"Franz Reinaldo Gonzales S.","userId":"11091101958395281034"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# ============================== ENTRENAMIENTO CON TRANSFER LEARNING ==============================\n","\n","logger.info(\"Iniciando fase de Transfer Learning\")\n","print(\"Iniciando fase de Transfer Learning\")\n","\n","# 1. Cargar modelo pre-entrenado\n","model = TimesformerForVideoClassification.from_pretrained(\n","    CONFIG[\"pretrained_model\"],\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"],\n","    num_labels=CONFIG[\"num_classes\"],  # A√±adir esto para configurar 2 clases desde el inicio\n","    ignore_mismatched_sizes=True\n",")\n","\n","# 2. Asegurarnos de que la clasificaci√≥n final tiene el n√∫mero correcto de salidas\n","if hasattr(model, 'classifier'):\n","    if hasattr(model.classifier, 'out_features') and model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        # Guardar dimensi√≥n de entrada\n","        in_features = model.classifier.in_features\n","\n","        # Reemplazar completamente el clasificador\n","        model.classifier = nn.Linear(in_features, CONFIG[\"num_classes\"])\n","\n","        logger.info(f\"Reemplazada capa de clasificaci√≥n: {in_features} -> {CONFIG['num_classes']}\")\n","    elif isinstance(model.classifier, nn.Sequential):\n","        # Si ya es una secuencia, asegurarnos que la √∫ltima capa tenga la salida correcta\n","        last_layer = model.classifier[-1]\n","        if hasattr(last_layer, 'out_features') and last_layer.out_features != CONFIG[\"num_classes\"]:\n","            in_features = last_layer.in_features\n","            model.classifier[-1] = nn.Linear(in_features, CONFIG[\"num_classes\"])\n","            logger.info(f\"Reemplazada √∫ltima capa de clasificaci√≥n: {in_features} -> {CONFIG['num_classes']}\")\n","\n","\n","# 3. Congelar los par√°metros del modelo base (excepto los de la capa de clasificaci√≥n)\n","for name, param in model.named_parameters():\n","    if 'classifier' not in name:  # Congelar todos los par√°metros excepto los del clasificador\n","        param.requires_grad = False\n","\n","# Verificar par√°metros entrenables vs congelados\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in model.parameters())\n","logger.info(f\"Par√°metros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n","print(f\"Par√°metros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n","\n","# Mover modelo a GPU\n","model.to(device)\n","\n","# 4. Preparar datasets y dataloaders\n","train_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='train',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","val_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='val',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=CONFIG[\"tl_batch_size\"],\n","    shuffle=True,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=CONFIG[\"tl_batch_size\"],\n","    shuffle=False,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","# 5. Configurar optimizador y scheduler\n","optimizer = optim.AdamW(\n","    filter(lambda p: p.requires_grad, model.parameters()),\n","    lr=CONFIG[\"tl_learning_rate\"],\n","    weight_decay=CONFIG[\"tl_weight_decay\"]\n",")\n","\n","# Calcular pasos totales para schedulers\n","num_training_steps = len(train_dataloader) * CONFIG[\"tl_num_epochs\"]\n","num_warmup_steps = int(num_training_steps * CONFIG[\"tl_warmup_ratio\"])\n","\n","scheduler = get_cosine_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=num_warmup_steps,\n","    num_training_steps=num_training_steps\n",")\n","\n","# 6. Criterio de p√©rdida (ya incluido en el modelo)\n","criterion = nn.CrossEntropyLoss()\n","\n","# 7. Inicializar tracking de m√©tricas\n","best_val_f1 = 0.0\n","train_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n","val_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n","\n","# 8. Entrenamiento por √©pocas\n","for epoch in range(CONFIG[\"tl_num_epochs\"]):\n","    logger.info(f\"Iniciando √©poca {epoch+1}/{CONFIG['tl_num_epochs']}\")\n","    print(f\"Iniciando √©poca {epoch+1}/{CONFIG['tl_num_epochs']}\")\n","\n","    # Entrenamiento\n","    train_results = train_epoch(\n","        model=model,\n","        dataloader=train_dataloader,\n","        optimizer=optimizer,\n","        scheduler=scheduler,\n","        criterion=criterion,\n","        device=device,\n","        epoch=epoch,\n","        config=CONFIG\n","    )\n","\n","    # Evaluaci√≥n\n","    eval_results = evaluate(\n","        model=model,\n","        dataloader=val_dataloader,\n","        criterion=criterion,\n","        device=device,\n","        config=CONFIG\n","    )\n","\n","    # Registrar m√©tricas\n","    for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1']:\n","        train_metrics[metric].append(train_results[metric])\n","        val_metrics[metric].append(eval_results[metric])\n","\n","    # Mostrar resultados\n","    logger.info(f\"Epoch {epoch+1}/{CONFIG['tl_num_epochs']} - \"\n","               f\"Train Loss: {train_results['loss']:.4f}, \"\n","               f\"Val Loss: {eval_results['loss']:.4f}, \"\n","               f\"Train Acc: {train_results['accuracy']:.4f}, \"\n","               f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n","               f\"Val F1: {eval_results['f1']:.4f}\")\n","    print(f\"Epoch {epoch+1}/{CONFIG['tl_num_epochs']} - \"\n","          f\"Train Loss: {train_results['loss']:.4f}, \"\n","          f\"Val Loss: {eval_results['loss']:.4f}, \"\n","          f\"Train Acc: {train_results['accuracy']:.4f}, \"\n","          f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n","          f\"Val F1: {eval_results['f1']:.4f}\")\n","\n","    # Guardar mejor modelo\n","    if eval_results['f1'] > best_val_f1:\n","        best_val_f1 = eval_results['f1']\n","\n","        # Guardar modelo\n","        model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_tl.pt\")\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'val_f1': best_val_f1,\n","            'config': CONFIG,\n","        }, model_path)\n","\n","        logger.info(f\"Guardado mejor modelo con F1: {best_val_f1:.4f} en {model_path}\")\n","        print(f\"Guardado mejor modelo con F1: {best_val_f1:.4f} en {model_path}\")\n","\n","    # Guardar checkpoint al final de cada √©poca\n","    checkpoint_path = os.path.join(CONFIG[\"output_dir\"], f\"checkpoint_epoch{epoch+1}.pt\")\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'train_metrics': train_metrics,\n","        'val_metrics': val_metrics,\n","    }, checkpoint_path)\n","\n","    logger.info(f\"Guardado checkpoint de √©poca {epoch+1} en {checkpoint_path}\")\n","    print(f\"Guardado checkpoint de √©poca {epoch+1} en {checkpoint_path}\")\n","\n","# 9. Visualizar y guardar m√©tricas\n","plot_metrics(train_metrics, val_metrics, CONFIG)\n","\n","# 10. Evaluaci√≥n final del mejor modelo\n","# Cargar el mejor modelo\n","best_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_tl.pt\")\n","checkpoint = torch.load(best_model_path)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","logger.info(f\"Evaluando mejor modelo de Transfer Learning (F1: {checkpoint['val_f1']:.4f})\")\n","print(f\"Evaluando mejor modelo de Transfer Learning (F1: {checkpoint['val_f1']:.4f})\")\n","\n","final_eval_results = evaluate(\n","    model=model,\n","    dataloader=val_dataloader,\n","    criterion=criterion,\n","    device=device,\n","    config=CONFIG\n",")\n","\n","# Visualizar matriz de confusi√≥n\n","plot_confusion_matrix(final_eval_results['confusion_matrix'], CONFIG, phase='transfer_learning')\n","\n","# Visualizar curva ROC\n","plot_roc_curve(\n","    final_eval_results['fpr'],\n","    final_eval_results['tpr'],\n","    final_eval_results['roc_auc'],\n","    CONFIG,\n","    phase='transfer_learning'\n",")\n","\n","# Guardar informe detallado\n","save_evaluation_report(final_eval_results, CONFIG, phase='transfer_learning')\n","\n","logger.info(\"Completada fase de Transfer Learning\")\n","print(\"Completada fase de Transfer Learning\")\n","\n","# Guardar resultados para usarlos en etapas posteriores\n","tl_results = final_eval_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cs5N5PgCn7vg","outputId":"1e26a842-58c2-414c-fa06-f0d3d5c521bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iniciando fase de Transfer Learning\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Par√°metros entrenables: 1,538 / 121,260,290 (0.00%)\n"]},{"output_type":"stream","name":"stderr","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"]},{"output_type":"stream","name":"stdout","text":["Cargados 8000 videos para split 'train'\n","Violencia: 4000, No Violencia: 4000\n","Cargados 1500 videos para split 'val'\n","Violencia: 750, No Violencia: 750\n","Iniciando √©poca 1/10\n"]},{"output_type":"stream","name":"stderr","text":["√âpoca 1:  10%|‚ñâ         | 99/1000 [07:10<36:44,  2.45s/it, loss=0.976, acc=0.25, prec=0.4, rec=0.4, f1=0.4]   ERROR:__main__:Error en paso 99, √©poca 1: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 1:  20%|‚ñà‚ñâ        | 199/1000 [11:00<31:48,  2.38s/it, loss=0.712, acc=0.375, prec=0.2, rec=0.5, f1=0.286]ERROR:__main__:Error en paso 199, √©poca 1: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 1:  30%|‚ñà‚ñà‚ñâ       | 299/1000 [15:01<16:33,  1.42s/it, loss=0.955, acc=0.375, prec=0, rec=0, f1=0]     ERROR:__main__:Error en paso 299, √©poca 1: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 399/1000 [19:02<19:55,  1.99s/it, loss=0.926, acc=0.5, prec=0.667, rec=0.4, f1=0.5] ERROR:__main__:Error en paso 399, √©poca 1: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 499/1000 [23:06<22:06,  2.65s/it, loss=0.565, acc=0.375, prec=0.5, rec=0.4, f1=0.444] ERROR:__main__:Error en paso 499, √©poca 1: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 599/1000 [27:02<14:10,  2.12s/it, loss=0.621, acc=0.5, prec=0.667, rec=0.4, f1=0.5]     ERROR:__main__:Error en paso 599, √©poca 1: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 699/1000 [31:01<10:40,  2.13s/it, loss=0.541, acc=0.625, prec=1, rec=0.4, f1=0.571]  ERROR:__main__:Error en paso 699, √©poca 1: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 799/1000 [34:53<05:35,  1.67s/it, loss=0.431, acc=0.5, prec=1, rec=0.333, f1=0.5]  ERROR:__main__:Error en paso 799, √©poca 1: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 899/1000 [40:00<04:53,  2.90s/it, loss=0.331, acc=0.75, prec=0.8, rec=0.8, f1=0.8]  ERROR:__main__:Error en paso 899, √©poca 1: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 999/1000 [44:06<00:02,  2.66s/it, loss=0.249, acc=0.875, prec=1, rec=0.75, f1=0.857] ERROR:__main__:Error en paso 999, √©poca 1: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [44:06<00:00,  2.65s/it, loss=0.249, acc=0.875, prec=1, rec=0.75, f1=0.857]\n","Evaluando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 188/188 [13:38<00:00,  4.35s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/10 - Train Loss: 0.6173, Val Loss: 0.3113, Train Acc: 0.5200, Val Acc: 0.7760, Val F1: 0.7214\n","Guardado mejor modelo con F1: 0.7214 en /content/drive/MyDrive/Carpeta_Proyecto_IA3/modelo_timesformer2/timesformer_violence_detector_best_tl.pt\n","Guardado checkpoint de √©poca 1 en /content/drive/MyDrive/Carpeta_Proyecto_IA3/modelo_timesformer2/checkpoint_epoch1.pt\n","Iniciando √©poca 2/10\n"]},{"output_type":"stream","name":"stderr","text":["√âpoca 2:  10%|‚ñâ         | 99/1000 [01:44<15:34,  1.04s/it, loss=0.256, acc=1, prec=1, rec=1, f1=1]               ERROR:__main__:Error en paso 99, √©poca 2: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 2:  20%|‚ñà‚ñâ        | 199/1000 [03:30<14:26,  1.08s/it, loss=0.37, acc=0.75, prec=1, rec=0.667, f1=0.8]ERROR:__main__:Error en paso 199, √©poca 2: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 2:  30%|‚ñà‚ñà‚ñâ       | 299/1000 [05:20<12:55,  1.11s/it, loss=0.18, acc=1, prec=1, rec=1, f1=1]          ERROR:__main__:Error en paso 299, √©poca 2: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 399/1000 [07:10<11:09,  1.11s/it, loss=0.15, acc=0.875, prec=1, rec=0.667, f1=0.8]     ERROR:__main__:Error en paso 399, √©poca 2: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 499/1000 [09:01<09:05,  1.09s/it, loss=0.166, acc=0.75, prec=1, rec=0.6, f1=0.75]ERROR:__main__:Error en paso 499, √©poca 2: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 599/1000 [10:51<07:24,  1.11s/it, loss=0.368, acc=0.75, prec=0.75, rec=0.75, f1=0.75]ERROR:__main__:Error en paso 599, √©poca 2: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 699/1000 [12:42<05:38,  1.12s/it, loss=0.0975, acc=1, prec=1, rec=1, f1=1]           ERROR:__main__:Error en paso 699, √©poca 2: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 799/1000 [14:33<03:40,  1.10s/it, loss=0.261, acc=0.875, prec=0.8, rec=1, f1=0.889]ERROR:__main__:Error en paso 799, √©poca 2: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 899/1000 [16:24<01:51,  1.10s/it, loss=0.249, acc=0.875, prec=1, rec=0.8, f1=0.889]ERROR:__main__:Error en paso 899, √©poca 2: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 999/1000 [18:15<00:01,  1.10s/it, loss=0.121, acc=0.875, prec=0.857, rec=1, f1=0.923]ERROR:__main__:Error en paso 999, √©poca 2: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [18:15<00:00,  1.10s/it, loss=0.121, acc=0.875, prec=0.857, rec=1, f1=0.923]\n","Evaluando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 188/188 [03:26<00:00,  1.10s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/10 - Train Loss: 0.2418, Val Loss: 0.1893, Train Acc: 0.8858, Val Acc: 0.8973, Val F1: 0.8882\n","Guardado mejor modelo con F1: 0.8882 en /content/drive/MyDrive/Carpeta_Proyecto_IA3/modelo_timesformer2/timesformer_violence_detector_best_tl.pt\n","Guardado checkpoint de √©poca 2 en /content/drive/MyDrive/Carpeta_Proyecto_IA3/modelo_timesformer2/checkpoint_epoch2.pt\n","Iniciando √©poca 3/10\n"]},{"output_type":"stream","name":"stderr","text":["√âpoca 3:  10%|‚ñâ         | 99/1000 [01:55<16:34,  1.10s/it, loss=0.064, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 99, √©poca 3: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 3:  20%|‚ñà‚ñâ        | 199/1000 [03:46<14:51,  1.11s/it, loss=0.255, acc=0.875, prec=0.857, rec=1, f1=0.923]ERROR:__main__:Error en paso 199, √©poca 3: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 3:  30%|‚ñà‚ñà‚ñâ       | 299/1000 [05:37<12:57,  1.11s/it, loss=0.167, acc=1, prec=1, rec=1, f1=1]           ERROR:__main__:Error en paso 299, √©poca 3: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 3:  40%|‚ñà‚ñà‚ñà‚ñâ      | 399/1000 [07:27<11:00,  1.10s/it, loss=0.13, acc=0.875, prec=1, rec=0.833, f1=0.909]ERROR:__main__:Error en paso 399, √©poca 3: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 499/1000 [09:18<09:15,  1.11s/it, loss=0.304, acc=0.875, prec=0.75, rec=1, f1=0.857]ERROR:__main__:Error en paso 499, √©poca 3: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 599/1000 [11:09<07:28,  1.12s/it, loss=0.142, acc=0.875, prec=1, rec=0.8, f1=0.889] ERROR:__main__:Error en paso 599, √©poca 3: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 699/1000 [12:59<05:30,  1.10s/it, loss=0.0611, acc=0.75, prec=1, rec=0.6, f1=0.75]ERROR:__main__:Error en paso 699, √©poca 3: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 799/1000 [14:50<03:41,  1.10s/it, loss=0.241, acc=0.875, prec=0.667, rec=1, f1=0.8]ERROR:__main__:Error en paso 799, √©poca 3: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 3:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 899/1000 [16:40<01:52,  1.11s/it, loss=0.0804, acc=1, prec=1, rec=1, f1=1]         ERROR:__main__:Error en paso 899, √©poca 3: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 999/1000 [18:31<00:01,  1.10s/it, loss=0.238, acc=1, prec=1, rec=1, f1=1]           ERROR:__main__:Error en paso 999, √©poca 3: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [18:31<00:00,  1.11s/it, loss=0.238, acc=1, prec=1, rec=1, f1=1]\n","Evaluando: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 188/188 [03:26<00:00,  1.10s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/10 - Train Loss: 0.1828, Val Loss: 0.1600, Train Acc: 0.9176, Val Acc: 0.9167, Val F1: 0.9113\n","Guardado mejor modelo con F1: 0.9113 en /content/drive/MyDrive/Carpeta_Proyecto_IA3/modelo_timesformer2/timesformer_violence_detector_best_tl.pt\n","Guardado checkpoint de √©poca 3 en /content/drive/MyDrive/Carpeta_Proyecto_IA3/modelo_timesformer2/checkpoint_epoch3.pt\n","Iniciando √©poca 4/10\n"]},{"output_type":"stream","name":"stderr","text":["√âpoca 4:  10%|‚ñâ         | 99/1000 [01:56<16:35,  1.11s/it, loss=0.297, acc=0.875, prec=0.75, rec=1, f1=0.857]ERROR:__main__:Error en paso 99, √©poca 4: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 4:  20%|‚ñà‚ñâ        | 199/1000 [03:47<14:46,  1.11s/it, loss=0.503, acc=0.875, prec=0.833, rec=1, f1=0.909]  ERROR:__main__:Error en paso 199, √©poca 4: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 4:  30%|‚ñà‚ñà‚ñâ       | 299/1000 [05:37<13:04,  1.12s/it, loss=0.317, acc=0.75, prec=0.667, rec=1, f1=0.8] ERROR:__main__:Error en paso 299, √©poca 4: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 4:  40%|‚ñà‚ñà‚ñà‚ñâ      | 399/1000 [07:28<10:58,  1.10s/it, loss=0.00767, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 399, √©poca 4: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 499/1000 [09:18<09:13,  1.10s/it, loss=0.11, acc=0.875, prec=0.75, rec=1, f1=0.857] ERROR:__main__:Error en paso 499, √©poca 4: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 4:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 599/1000 [11:10<07:24,  1.11s/it, loss=0.0319, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 599, √©poca 4: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 4:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 699/1000 [13:01<05:38,  1.12s/it, loss=0.11, acc=0.875, prec=0.833, rec=1, f1=0.909]ERROR:__main__:Error en paso 699, √©poca 4: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 4:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 799/1000 [14:51<03:41,  1.10s/it, loss=0.0214, acc=1, prec=1, rec=1, f1=1]ERROR:__main__:Error en paso 799, √©poca 4: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 4:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 899/1000 [16:42<01:51,  1.10s/it, loss=0.172, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 899, √©poca 4: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 999/1000 [18:33<00:01,  1.10s/it, loss=0.0139, acc=1, prec=1, rec=1, f1=1]         ERROR:__main__:Error en paso 999, √©poca 4: cannot access local variable 'loss' where it is not associated with a value\n","√âpoca 4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [18:33<00:00,  1.11s/it, loss=0.0139, acc=1, prec=1, rec=1, f1=1]\n","Evaluando:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 101/188 [01:52<01:36,  1.10s/it]"]}]},{"cell_type":"code","source":["# ============================== FINE TUNING ==============================\n","\n","logger.info(\"Iniciando fase de Fine-Tuning\")\n","\n","# Cargar el mejor modelo de Transfer Learning\n","best_tl_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_tl.pt\")\n","checkpoint = torch.load(best_tl_model_path)\n","\n","# Comprobar si ya tenemos el modelo cargado (de la celda anterior) o necesitamos cargarlo\n","try:\n","    # Intentar acceder al modelo, si no est√° definido, lanzar√° una excepci√≥n\n","    model\n","    logger.info(\"Usando modelo ya cargado de celda anterior\")\n","    # Cargar estado del modelo desde checkpoint\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","except NameError:\n","    # Si el modelo no est√° definido, crear uno nuevo y cargarlo\n","    logger.info(\"Cargando modelo desde checkpoint\")\n","    model = TimesformerForVideoClassification.from_pretrained(\n","        CONFIG[\"pretrained_model\"],\n","        num_frames=CONFIG[\"num_frames\"],\n","        image_size=CONFIG[\"image_size\"],\n","    )\n","\n","    # Adaptarlo a nuestra tarea si es necesario\n","    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(CONFIG[\"tl_dropout\"]),\n","            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n","        )\n","\n","    # Cargar estado del modelo desde checkpoint\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# 1. Descongelar todos los par√°metros del modelo\n","for param in model.parameters():\n","    param.requires_grad = True\n","\n","# Verificar par√°metros entrenables\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in model.parameters())\n","logger.info(f\"Par√°metros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n","\n","# Asegurar que el modelo est√° en el dispositivo correcto\n","model.to(device)\n","\n","# 2. Preparar datasets y dataloaders (mismos que antes)\n","train_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='train',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","val_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='val',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=CONFIG[\"ft_batch_size\"],  # Tama√±o de batch m√°s peque√±o para fine-tuning\n","    shuffle=True,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=CONFIG[\"ft_batch_size\"],\n","    shuffle=False,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","# 3. Configurar optimizador con learning rate diferenciado\n","# Usar learning rates m√°s peque√±os para capas base y m√°s grandes para capas superiores\n","param_groups = [\n","    {\n","        'params': [p for n, p in model.named_parameters() if 'classifier' not in n],\n","        'lr': CONFIG[\"ft_learning_rate\"] * 0.1  # LR m√°s bajo para el backbone\n","    },\n","    {\n","        'params': [p for n, p in model.named_parameters() if 'classifier' in n],\n","        'lr': CONFIG[\"ft_learning_rate\"]  # LR normal para el clasificador\n","    }\n","]\n","\n","optimizer = optim.AdamW(\n","    param_groups,\n","    weight_decay=CONFIG[\"ft_weight_decay\"]\n",")\n","\n","# Calcular pasos totales para scheduler\n","num_training_steps = len(train_dataloader) * CONFIG[\"ft_num_epochs\"]\n","\n","# Scheduler con cosine annealing\n","scheduler = CosineAnnealingLR(\n","    optimizer,\n","    T_max=num_training_steps,\n","    eta_min=1e-6\n",")\n","\n","# 4. Criterio de p√©rdida (ya incluido en el modelo)\n","criterion = nn.CrossEntropyLoss()\n","\n","# 5. Inicializar tracking de m√©tricas\n","best_val_f1 = 0.0\n","train_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n","val_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n","\n","# 6. Entrenamiento por √©pocas\n","for epoch in range(CONFIG[\"ft_num_epochs\"]):\n","    logger.info(f\"Iniciando √©poca {epoch+1}/{CONFIG['ft_num_epochs']} (fine-tuning)\")\n","\n","    # Entrenamiento\n","    train_results = train_epoch(\n","        model=model,\n","        dataloader=train_dataloader,\n","        optimizer=optimizer,\n","        scheduler=scheduler,\n","        criterion=criterion,\n","        device=device,\n","        epoch=epoch,\n","        config=CONFIG\n","    )\n","\n","    # Evaluaci√≥n\n","    eval_results = evaluate(\n","        model=model,\n","        dataloader=val_dataloader,\n","        criterion=criterion,\n","        device=device,\n","        config=CONFIG\n","    )\n","\n","    # Registrar m√©tricas\n","    for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1']:\n","        train_metrics[metric].append(train_results[metric])\n","        val_metrics[metric].append(eval_results[metric])\n","\n","    # Mostrar resultados\n","    logger.info(f\"Epoch {epoch+1}/{CONFIG['ft_num_epochs']} (FT) - \"\n","               f\"Train Loss: {train_results['loss']:.4f}, \"\n","               f\"Val Loss: {eval_results['loss']:.4f}, \"\n","               f\"Train Acc: {train_results['accuracy']:.4f}, \"\n","               f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n","               f\"Val F1: {eval_results['f1']:.4f}\")\n","\n","    # Guardar mejor modelo\n","    if eval_results['f1'] > best_val_f1:\n","        best_val_f1 = eval_results['f1']\n","\n","        # Guardar modelo\n","        model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'val_f1': best_val_f1,\n","            'config': CONFIG,\n","        }, model_path)\n","\n","        logger.info(f\"Guardado mejor modelo (FT) con F1: {best_val_f1:.4f} en {model_path}\")\n","\n","    # Guardar checkpoint al final de cada √©poca\n","    checkpoint_path = os.path.join(CONFIG[\"output_dir\"], f\"checkpoint_ft_epoch{epoch+1}.pt\")\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'train_metrics': train_metrics,\n","        'val_metrics': val_metrics,\n","    }, checkpoint_path)\n","\n","    logger.info(f\"Guardado checkpoint de fine-tuning √©poca {epoch+1} en {checkpoint_path}\")\n","\n","# 7. Visualizar y guardar m√©tricas\n","# Crear nuevas visualizaciones para fine-tuning\n","plt.figure(figsize=(20, 15))\n","\n","metrics_to_plot = ['loss', 'accuracy', 'precision', 'recall', 'f1']\n","epochs = range(1, len(train_metrics['loss']) + 1)\n","\n","for i, metric in enumerate(metrics_to_plot):\n","    plt.subplot(3, 2, i+1)\n","    plt.plot(epochs, train_metrics[metric], 'b-', label=f'Training {metric}')\n","    plt.plot(epochs, val_metrics[metric], 'r-', label=f'Validation {metric}')\n","    plt.title(f'{metric.capitalize()} vs. Epochs (Fine-Tuning)')\n","    plt.xlabel('Epochs')\n","    plt.ylabel(metric.capitalize())\n","    plt.legend()\n","    plt.grid(True)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"fine_tuning_metrics.png\"))\n","plt.close()\n","\n","# 8. Evaluaci√≥n final del mejor modelo\n","# Cargar el mejor modelo\n","best_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","checkpoint = torch.load(best_model_path)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","logger.info(f\"Evaluando mejor modelo de Fine-Tuning (F1: {checkpoint['val_f1']:.4f})\")\n","\n","final_eval_results = evaluate(\n","    model=model,\n","    dataloader=val_dataloader,\n","    criterion=criterion,\n","    device=device,\n","    config=CONFIG\n",")\n","\n","# Visualizar matriz de confusi√≥n\n","plot_confusion_matrix(final_eval_results['confusion_matrix'], CONFIG, phase='fine_tuning')\n","\n","# Visualizar curva ROC\n","plot_roc_curve(\n","    final_eval_results['fpr'],\n","    final_eval_results['tpr'],\n","    final_eval_results['roc_auc'],\n","    CONFIG,\n","    phase='fine_tuning'\n",")\n","\n","# Guardar informe detallado\n","save_evaluation_report(final_eval_results, CONFIG, phase='fine_tuning')\n","\n","logger.info(\"Completada fase de Fine-Tuning\")\n","\n","# Guardar resultados para usarlos en etapas posteriores\n","ft_results = final_eval_results"],"metadata":{"id":"D9RikOs4oJZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== EVALUACI√ìN EN CONJUNTO DE PRUEBA ==============================\n","\n","logger.info(\"Evaluando modelo en conjunto de prueba\")\n","\n","# Cargar el mejor modelo de Fine-Tuning\n","best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","\n","# Comprobar si ya tenemos el modelo cargado\n","try:\n","    # Intentar acceder al modelo\n","    model\n","    logger.info(\"Usando modelo ya cargado de celda anterior\")\n","    # Cargar estado del mejor modelo de fine-tuning\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","except NameError:\n","    # Si el modelo no est√° definido, crear uno nuevo y cargarlo\n","    logger.info(\"Cargando modelo desde checkpoint\")\n","    model = TimesformerForVideoClassification.from_pretrained(\n","        CONFIG[\"pretrained_model\"],\n","        num_frames=CONFIG[\"num_frames\"],\n","        image_size=CONFIG[\"image_size\"],\n","    )\n","\n","    # Adaptarlo a nuestra tarea\n","    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(CONFIG[\"tl_dropout\"]),\n","            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n","        )\n","\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# Asegurar que el modelo est√° en el dispositivo correcto\n","model.to(device)\n","model.eval()\n","\n","# Cargar dataset de prueba\n","test_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='test',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=CONFIG[\"ft_batch_size\"],\n","    shuffle=False,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","# Evaluar\n","criterion = nn.CrossEntropyLoss()\n","test_results = evaluate(\n","    model=model,\n","    dataloader=test_dataloader,\n","    criterion=criterion,\n","    device=device,\n","    config=CONFIG\n",")\n","\n","# Visualizar matriz de confusi√≥n\n","plot_confusion_matrix(test_results['confusion_matrix'], CONFIG, phase='test')\n","\n","# Visualizar curva ROC\n","plot_roc_curve(\n","    test_results['fpr'],\n","    test_results['tpr'],\n","    test_results['roc_auc'],\n","    CONFIG,\n","    phase='test'\n",")\n","\n","# Generar y guardar reporte detallado\n","save_evaluation_report(test_results, CONFIG, phase='test')\n","\n","# M√©tricas adicionales: Precision-Recall curve\n","precision, recall, _ = precision_recall_curve(\n","    test_results['labels'],\n","    test_results['predictions']\n",")\n","pr_auc = average_precision_score(test_results['labels'], test_results['predictions'])\n","\n","# Graficar curva Precision-Recall\n","plt.figure(figsize=(10, 8))\n","plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend(loc=\"lower left\")\n","plt.grid(True)\n","plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"precision_recall_curve_test.png\"))\n","plt.close()\n","\n","# An√°lisis de mejores umbrales\n","# Calcular m√©tricas para diferentes umbrales\n","thresholds = np.linspace(0.1, 0.9, 9)\n","threshold_metrics = []\n","\n","for threshold in thresholds:\n","    binary_preds = (test_results['predictions'] >= threshold).astype(int)\n","\n","    acc = accuracy_score(test_results['labels'], binary_preds)\n","    prec = precision_score(test_results['labels'], binary_preds, zero_division=0)\n","    rec = recall_score(test_results['labels'], binary_preds, zero_division=0)\n","    f1 = f1_score(test_results['labels'], binary_preds, zero_division=0)\n","\n","    threshold_metrics.append({\n","        'threshold': threshold,\n","        'accuracy': acc,\n","        'precision': prec,\n","        'recall': rec,\n","        'f1': f1\n","    })\n","\n","# Convertir a DataFrame para mejor visualizaci√≥n\n","threshold_df = pd.DataFrame(threshold_metrics)\n","\n","# Graficar m√©tricas vs umbral\n","plt.figure(figsize=(12, 8))\n","for metric in ['accuracy', 'precision', 'recall', 'f1']:\n","    plt.plot(threshold_df['threshold'], threshold_df[metric], marker='o', label=metric)\n","\n","plt.xlabel('Umbral de decisi√≥n')\n","plt.ylabel('Valor de m√©trica')\n","plt.title('M√©tricas vs Umbral de decisi√≥n')\n","plt.legend()\n","plt.grid(True)\n","plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"threshold_analysis.png\"))\n","plt.close()\n","\n","# Encontrar mejor umbral seg√∫n F1\n","best_threshold_idx = threshold_df['f1'].idxmax()\n","best_threshold = threshold_df.loc[best_threshold_idx, 'threshold']\n","\n","logger.info(f\"Mejor umbral encontrado: {best_threshold:.2f} con F1: {threshold_df.loc[best_threshold_idx, 'f1']:.4f}\")\n","\n","# Guardar an√°lisis de umbrales\n","threshold_df.to_csv(os.path.join(CONFIG[\"output_dir\"], \"threshold_analysis.csv\"), index=False)\n","\n","# Actualizar el umbral en la configuraci√≥n\n","CONFIG[\"threshold\"] = float(best_threshold)\n","with open(os.path.join(CONFIG[\"output_dir\"], \"config.json\"), 'w') as f:\n","    json.dump(CONFIG, f, indent=4)\n","\n","# Calcular y visualizar curva ROC detallada con punto √≥ptimo\n","fpr, tpr, thresholds_roc = roc_curve(test_results['labels'], test_results['predictions'])\n","roc_auc = auc(fpr, tpr)\n","\n","# Calcular distancia al punto √≥ptimo (0,1)\n","distances = np.sqrt((1-tpr)**2 + fpr**2)\n","optimal_idx = np.argmin(distances)\n","optimal_threshold = thresholds_roc[optimal_idx]\n","\n","plt.figure(figsize=(10, 8))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red',\n","            label=f'Punto √≥ptimo (umbral={optimal_threshold:.2f})')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Tasa de Falsos Positivos')\n","plt.ylabel('Tasa de Verdaderos Positivos')\n","plt.title('Curva ROC con punto √≥ptimo')\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"roc_curve_optimal_test.png\"))\n","plt.close()\n","\n","logger.info(f\"Umbral √≥ptimo seg√∫n distancia a punto ideal en ROC: {optimal_threshold:.4f}\")\n","\n","# Guardar este umbral tambi√©n\n","with open(os.path.join(CONFIG[\"output_dir\"], \"optimal_thresholds.json\"), 'w') as f:\n","    json.dump({\n","        'f1_optimal': float(best_threshold),\n","        'roc_optimal': float(optimal_threshold)\n","    }, f, indent=4)\n","\n","# Mostrar resumen de resultados\n","logger.info(f\"Resumen de evaluaci√≥n en conjunto de prueba:\")\n","logger.info(f\"Accuracy: {test_results['accuracy']:.4f}\")\n","logger.info(f\"Precision: {test_results['precision']:.4f}\")\n","logger.info(f\"Recall (Sensibilidad): {test_results['recall']:.4f}\")\n","logger.info(f\"Specificity: {test_results['specificity']:.4f}\")\n","logger.info(f\"F1-Score: {test_results['f1']:.4f}\")\n","logger.info(f\"ROC AUC: {test_results['roc_auc']:.4f}\")\n","logger.info(f\"PR AUC: {pr_auc:.4f}\")"],"metadata":{"id":"_FiIInTroO9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== EXPORTACI√ìN DEL MODELO ==============================\n","\n","logger.info(\"Exportando modelo para inferencia\")\n","\n","# Comprobar si ya tenemos el modelo cargado\n","try:\n","    # Intentar acceder al modelo\n","    model\n","    # Asegurarse de que tiene cargado el mejor modelo de fine-tuning\n","    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","except NameError:\n","    # Si el modelo no est√° definido, crear uno nuevo y cargarlo\n","    logger.info(\"Cargando modelo desde checkpoint\")\n","    model = TimesformerForVideoClassification.from_pretrained(\n","        CONFIG[\"pretrained_model\"],\n","        num_frames=CONFIG[\"num_frames\"],\n","        image_size=CONFIG[\"image_size\"],\n","    )\n","\n","    # Adaptarlo a nuestra tarea\n","    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(CONFIG[\"tl_dropout\"]),\n","            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n","        )\n","\n","    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# Asegurar que el modelo est√° en el dispositivo correcto\n","model.to(device)\n","model.eval()\n","\n","# 1. Guardar modelo en formato PyTorch\n","model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_final.pt\")\n","torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'config': CONFIG,\n","}, model_path)\n","\n","logger.info(f\"Modelo guardado en formato PyTorch: {model_path}\")\n","\n","# 2. Crear modelo para inferencia sin calcular p√©rdidas (m√°s eficiente)\n","class TimesformerInference(nn.Module):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.timesformer = model\n","\n","    def forward(self, pixel_values):\n","        outputs = self.timesformer(pixel_values=pixel_values)\n","        logits = outputs.logits\n","        # Aplicar sigmoid para obtener probabilidades para la clase 'violencia'\n","        probs = torch.sigmoid(logits[:, 1])\n","        return probs\n","\n","inference_model = TimesformerInference(model)\n","inference_model.eval()\n","\n","# Guardar modelo de inferencia\n","inference_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_inference.pt\")\n","torch.save(inference_model, inference_model_path)\n","\n","logger.info(f\"Modelo de inferencia guardado: {inference_model_path}\")\n","\n","# 3. Guardar tambi√©n usando save_pretrained de Hugging Face\n","save_dir = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_hf\")\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# Guardar modelo y procesador\n","try:\n","    model.save_pretrained(save_dir)\n","    processor = AutoImageProcessor.from_pretrained(CONFIG[\"pretrained_model\"])\n","    processor.save_pretrained(save_dir)\n","    logger.info(f\"Modelo y procesador guardados en formato Hugging Face: {save_dir}\")\n","except Exception as e:\n","    logger.warning(f\"Error al guardar modelo con save_pretrained: {str(e)}\")\n","    logger.info(\"Utilizando m√©todo alternativo para guardar en formato HF\")\n","\n","    # M√©todo alternativo\n","    if not os.path.exists(os.path.join(save_dir, \"config.json\")):\n","        config_obj = TimesformerConfig.from_pretrained(CONFIG[\"pretrained_model\"])\n","        config_obj.num_frames = CONFIG[\"num_frames\"]\n","        config_obj.image_size = CONFIG[\"image_size\"]\n","        config_obj.num_labels = CONFIG[\"num_classes\"]\n","        config_obj.save_pretrained(save_dir)\n","\n","    if not os.path.exists(os.path.join(save_dir, \"pytorch_model.bin\")):\n","        torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n","\n","    # Guardar procesador\n","    processor = AutoImageProcessor.from_pretrained(CONFIG[\"pretrained_model\"])\n","    if not os.path.exists(os.path.join(save_dir, \"preprocessor_config.json\")):\n","        processor.save_pretrained(save_dir)\n","\n","    logger.info(f\"Modelo y procesador guardados en formato alternativo: {save_dir}\")\n","\n","# 4. Crear script de ejemplo para inferencia\n","inference_script = \"\"\"\n","import torch\n","import torch.nn as nn\n","from transformers import TimesformerForVideoClassification, AutoImageProcessor\n","from decord import VideoReader, cpu\n","import numpy as np\n","from torchvision import transforms\n","import os\n","import json\n","\n","def load_model(model_path, config_path=None):\n","    \"\"\"Carga el modelo de detecci√≥n de violencia\"\"\"\n","    # Cargar configuraci√≥n si se proporciona\n","    if config_path:\n","        with open(config_path, 'r') as f:\n","            config = json.load(f)\n","    else:\n","        # Intentar encontrar config.json en el mismo directorio\n","        config_dir = os.path.dirname(model_path)\n","        config_path = os.path.join(config_dir, \"config.json\")\n","        if os.path.exists(config_path):\n","            with open(config_path, 'r') as f:\n","                config = json.load(f)\n","        else:\n","            # Valores por defecto\n","            config = {\n","                \"num_frames\": 16,\n","                \"image_size\": 224,\n","                \"threshold\": 0.5\n","            }\n","\n","    # Opci√≥n 1: Cargar modelo guardado con HF save_pretrained\n","    if os.path.isdir(model_path):\n","        model = TimesformerForVideoClassification.from_pretrained(model_path)\n","        processor = AutoImageProcessor.from_pretrained(model_path)\n","        return model, processor, config\n","\n","    # Opci√≥n 2: Cargar modelo guardado con torch.save\n","    checkpoint = torch.load(model_path, map_location=\"cpu\")\n","\n","    if isinstance(checkpoint, torch.nn.Module):\n","        # Es un modelo entero guardado con torch.save(model)\n","        return checkpoint, None, config\n","\n","    # Es un diccionario con state_dict\n","    if \"model_state_dict\" in checkpoint:\n","        model = TimesformerForVideoClassification.from_pretrained(\n","            \"facebook/timesformer-base-finetuned-k400\",\n","            num_frames=config[\"num_frames\"],\n","            image_size=config[\"image_size\"],\n","        )\n","        model.load_state_dict(checkpoint[\"model_state_dict\"])\n","        processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n","        return model, processor, config\n","\n","    raise ValueError(\"Formato de modelo no reconocido\")\n","\n","def process_video(video_path, model, processor=None, config=None):\n","    \"\"\"Procesa un video y detecta violencia\"\"\"\n","    if config is None:\n","        config = {\n","            \"num_frames\": 16,\n","            \"image_size\": 224,\n","            \"threshold\": 0.5\n","        }\n","\n","    # Cargar video con decord\n","    video_reader = VideoReader(video_path, ctx=cpu(0))\n","    total_frames = len(video_reader)\n","\n","    # Seleccionar frames uniformemente\n","    indices = np.linspace(0, total_frames - 1, config[\"num_frames\"], dtype=int)\n","    frames = video_reader.get_batch(indices).asnumpy()  # (num_frames, H, W, C)\n","\n","    # Preprocesar frames\n","    processed_frames = []\n","    for frame in frames:\n","        # Redimensionar\n","        frame = transforms.functional.resize(\n","            transforms.functional.to_tensor(frame),\n","            (config[\"image_size\"], config[\"image_size\"])\n","        )\n","        processed_frames.append(frame)\n","\n","    # Apilar frames\n","    frames_tensor = torch.stack(processed_frames)  # (T, C, H, W)\n","\n","    # Preprocesar con el procesador si est√° disponible\n","    if processor:\n","        inputs = processor(\n","            list(frames_tensor),  # Lista de tensores (T, C, H, W)\n","            return_tensors=\"pt\"\n","        )\n","        pixel_values = inputs['pixel_values']\n","    else:\n","        # Formato alternativo si no hay procesador\n","        frames_tensor = frames_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # (1, C, T, H, W)\n","        pixel_values = frames_tensor\n","\n","    # Inferencia\n","    model.eval()\n","    with torch.no_grad():\n","        # Comprobar si es modelo de inferencia personalizado\n","        if isinstance(model, nn.Module) and hasattr(model, 'timesformer'):\n","            # Modelo de inferencia que devuelve directamente probabilidades\n","            violence_prob = model(pixel_values)\n","        else:\n","            # Modelo TimesformerForVideoClassification est√°ndar\n","            outputs = model(pixel_values=pixel_values)\n","            # Continuaci√≥n del script de inferencia\n","            logits = outputs.logits\n","            violence_prob = torch.sigmoid(logits[:, 1])\n","\n","    # Determinar predicci√≥n seg√∫n umbral\n","    is_violent = violence_prob.item() >= config[\"threshold\"]\n","\n","    return {\n","        \"is_violent\": is_violent,\n","        \"violence_probability\": violence_prob.item(),\n","        \"threshold\": config[\"threshold\"]\n","    }\n","\n","# Ejemplo de uso\n","if __name__ == \"__main__\":\n","    # Ruta al modelo guardado\n","    model_path = \"ruta/al/modelo_timesformer_inference.pt\"\n","    config_path = \"ruta/al/config.json\"\n","\n","    # Cargar modelo\n","    model, processor, config = load_model(model_path, config_path)\n","\n","    # Ruta al video a procesar\n","    video_path = \"ruta/al/video_ejemplo.mp4\"\n","\n","    # Procesar video\n","    result = process_video(video_path, model, processor, config)\n","\n","    # Mostrar resultado\n","    print(f\"¬øDetectada violencia?: {'S√≠' if result['is_violent'] else 'No'}\")\n","    print(f\"Probabilidad de violencia: {result['violence_probability']:.4f}\")\n","\"\"\"\n","\n","# Guardar script de ejemplo\n","with open(os.path.join(CONFIG[\"output_dir\"], \"inference_example.py\"), 'w') as f:\n","    f.write(inference_script)\n","\n","logger.info(f\"Script de ejemplo para inferencia guardado en: {os.path.join(CONFIG['output_dir'], 'inference_example.py')}\")\n","\n","# 5. Exportar modelo en formato ONNX para inferencia m√°s r√°pida\n","try:\n","    # Generar un input de ejemplo\n","    dummy_input = torch.randn(1, 3, CONFIG[\"num_frames\"], CONFIG[\"image_size\"], CONFIG[\"image_size\"]).to(device)\n","\n","    # Configurar rutas\n","    onnx_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}.onnx\")\n","\n","    # Exportar a ONNX\n","    torch.onnx.export(\n","        inference_model,\n","        dummy_input,\n","        onnx_path,\n","        export_params=True,\n","        opset_version=12,\n","        input_names=['input'],\n","        output_names=['output'],\n","        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n","    )\n","\n","    logger.info(f\"Modelo exportado en formato ONNX: {onnx_path}\")\n","except Exception as e:\n","    logger.warning(f\"No se pudo exportar a ONNX: {str(e)}\")\n","    logger.info(\"Puede ser debido a compatibilidad con la arquitectura. Esto no afectar√° al uso del modelo en PyTorch.\")\n","\n","# Mostrar rutas de los modelos exportados\n","export_paths = {\n","    'pytorch_model': model_path,\n","    'inference_model': inference_model_path,\n","    'huggingface_model': save_dir,\n","    'onnx_model': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}.onnx\"),\n","    'config': os.path.join(CONFIG[\"output_dir\"], \"config.json\"),\n","    'inference_script': os.path.join(CONFIG[\"output_dir\"], \"inference_example.py\")\n","}\n","\n","logger.info(\"Rutas de los modelos exportados:\")\n","for key, path in export_paths.items():\n","    logger.info(f\"  - {key}: {path}\")"],"metadata":{"id":"7P-3ujPEoSLR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== PRUEBA DE INFERENCIA ==============================\n","\n","logger.info(\"Realizando pruebas de inferencia en muestras\")\n","\n","# Comprobar si ya tenemos el modelo cargado\n","try:\n","    # Intentar acceder al modelo\n","    model\n","    # Asegurarse de que tiene cargado el mejor modelo de fine-tuning\n","    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","except NameError:\n","    # Si el modelo no est√° definido, crear uno nuevo y cargarlo\n","    logger.info(\"Cargando modelo desde checkpoint\")\n","    model = TimesformerForVideoClassification.from_pretrained(\n","        CONFIG[\"pretrained_model\"],\n","        num_frames=CONFIG[\"num_frames\"],\n","        image_size=CONFIG[\"image_size\"],\n","    )\n","\n","    # Adaptarlo a nuestra tarea\n","    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(CONFIG[\"tl_dropout\"]),\n","            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n","        )\n","\n","    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# Asegurar que el modelo est√° en el dispositivo correcto\n","model.to(device)\n","model.eval()\n","\n","# Cargar dataset de prueba\n","test_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='test',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","# Seleccionar algunas muestras aleatorias\n","num_samples = min(5, len(test_dataset))\n","sample_indices = random.sample(range(len(test_dataset)), num_samples)\n","\n","# Resultados\n","results = []\n","\n","# Crear figura para visualizaci√≥n\n","fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4*num_samples))\n","if num_samples == 1:\n","    axes = axes.reshape(1, 2)\n","\n","for i, idx in enumerate(sample_indices):\n","    try:\n","        # Obtener muestra\n","        sample = test_dataset[idx]\n","        pixel_values = sample['pixel_values'].unsqueeze(0).to(device)  # A√±adir dimensi√≥n de batch\n","        label = sample['labels'].item()\n","        video_path = sample['video_path']\n","\n","        # Inferencia\n","        with torch.no_grad():\n","            outputs = model(pixel_values=pixel_values)\n","            logits = outputs.logits\n","            probs = torch.softmax(logits, dim=1)\n","            violence_prob = probs[0, 1].item()\n","            prediction = violence_prob >= CONFIG[\"threshold\"]\n","\n","        # Extraer un frame para visualizaci√≥n\n","        video_reader = VideoReader(video_path, ctx=cpu(0))\n","        mid_frame_idx = len(video_reader) // 2\n","        frame = video_reader[mid_frame_idx].asnumpy()\n","\n","        # Guardar resultado\n","        results.append({\n","            'video_path': video_path,\n","            'true_label': label,\n","            'violence_prob': violence_prob,\n","            'prediction': prediction,\n","            'correct': (prediction == label)\n","        })\n","\n","        # Visualizar\n","        axes[i, 0].imshow(frame)\n","        axes[i, 0].set_title(f\"Video: {os.path.basename(video_path)}\")\n","        axes[i, 0].axis('off')\n","\n","        # Graficar probabilidad\n","        bar_colors = ['green', 'red']\n","        class_names = ['No Violencia', 'Violencia']\n","        class_probs = [1 - violence_prob, violence_prob]\n","\n","        axes[i, 1].barh(class_names, class_probs, color=bar_colors)\n","        axes[i, 1].set_xlim(0, 1)\n","        axes[i, 1].set_title(f\"Predicci√≥n: {'Violencia' if prediction else 'No Violencia'} \" +\n","                          f\"(Real: {'Violencia' if label else 'No Violencia'})\")\n","        axes[i, 1].axvline(x=CONFIG[\"threshold\"], color='black', linestyle='--',\n","                      label=f'Umbral: {CONFIG[\"threshold\"]:.2f}')\n","        axes[i, 1].legend()\n","\n","    except Exception as e:\n","        logger.error(f\"Error al procesar muestra {idx}: {str(e)}\")\n","        # En caso de error, dejar la posici√≥n vac√≠a\n","        axes[i, 0].axis('off')\n","        axes[i, 1].axis('off')\n","        continue\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"inference_samples.png\"))\n","plt.close()\n","\n","# Guardar resultados\n","if results:\n","    results_df = pd.DataFrame(results)\n","    results_df.to_csv(os.path.join(CONFIG[\"output_dir\"], \"inference_samples_results.csv\"), index=False)\n","\n","    # Mostrar resumen\n","    correct_count = sum(1 for r in results if r['correct'])\n","    logger.info(f\"Precisi√≥n en muestras de prueba: {correct_count}/{len(results)} ({100 * correct_count / len(results):.1f}%)\")\n","else:\n","    logger.warning(\"No se pudieron procesar muestras para pruebas de inferencia\")"],"metadata":{"id":"4LaslpW1oc-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== MEDICI√ìN DE RENDIMIENTO ==============================\n","\n","logger.info(\"Realizando benchmark de velocidad de inferencia\")\n","\n","# Asegurarse de que el modelo est√° cargado\n","try:\n","    # Intentar acceder al modelo\n","    model\n","    # Configurar para evaluaci√≥n\n","    model.eval()\n","except NameError:\n","    # Si el modelo no est√° definido, cargar el mejor de fine-tuning\n","    logger.info(\"Cargando modelo desde checkpoint\")\n","    model = TimesformerForVideoClassification.from_pretrained(\n","        CONFIG[\"pretrained_model\"],\n","        num_frames=CONFIG[\"num_frames\"],\n","        image_size=CONFIG[\"image_size\"],\n","    )\n","\n","    # Adaptarlo a nuestra tarea\n","    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(CONFIG[\"tl_dropout\"]),\n","            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n","        )\n","\n","    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","# Asegurar que el modelo est√° en el dispositivo correcto\n","model.to(device)\n","\n","# Crear datos de ejemplo\n","dummy_input = torch.randn(1, 3, CONFIG[\"num_frames\"], CONFIG[\"image_size\"], CONFIG[\"image_size\"]).to(device)\n","\n","# Calentar la GPU\n","logger.info(\"Calentando GPU...\")\n","with torch.no_grad():\n","    for _ in range(10):\n","        _ = model(pixel_values=dummy_input)\n","\n","# Medir tiempo de inferencia\n","num_runs = 100\n","logger.info(f\"Midiendo tiempo para {num_runs} ejecuciones...\")\n","\n","start_time = time.time()\n","with torch.no_grad():\n","    for _ in range(num_runs):\n","        _ = model(pixel_values=dummy_input)\n","end_time = time.time()\n","\n","# Calcular m√©tricas\n","total_time = end_time - start_time\n","avg_time_per_inference = total_time / num_runs\n","fps = num_runs / total_time\n","\n","logger.info(f\"Tiempo total para {num_runs} ejecuciones: {total_time:.4f} segundos\")\n","logger.info(f\"Tiempo promedio por inferencia: {avg_time_per_inference*1000:.2f} ms\")\n","logger.info(f\"FPS (frames por segundo): {fps:.2f}\")\n","\n","# Guardar resultados\n","benchmark_results = {\n","    'total_time': total_time,\n","    'num_runs': num_runs,\n","    'avg_time_per_inference_ms': avg_time_per_inference * 1000,\n","    'fps': fps,\n","    'batch_size': 1,\n","    'num_frames': CONFIG[\"num_frames\"],\n","    'image_size': CONFIG[\"image_size\"],\n","    'device': str(device)\n","}\n","\n","with open(os.path.join(CONFIG[\"output_dir\"], \"benchmark_results.json\"), 'w') as f:\n","    json.dump(benchmark_results, f, indent=4)\n","\n","logger.info(\"Benchmark completado y resultados guardados.\")"],"metadata":{"id":"8YYbU2Fdofjp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== RESUMEN FINAL DE M√âTRICAS ==============================\n","\n","logger.info(\"=== RESUMEN FINAL DE M√âTRICAS ===\")\n","\n","# Intentar cargar informes de evaluaci√≥n\n","try:\n","    # Transfer Learning\n","    tl_report_path = os.path.join(CONFIG[\"output_dir\"], \"evaluation_report_transfer_learning.json\")\n","    with open(tl_report_path, 'r') as f:\n","        tl_report = json.load(f)\n","\n","    # Fine-Tuning\n","    ft_report_path = os.path.join(CONFIG[\"output_dir\"], \"evaluation_report_fine_tuning.json\")\n","    with open(ft_report_path, 'r') as f:\n","        ft_report = json.load(f)\n","\n","    # Test\n","    test_report_path = os.path.join(CONFIG[\"output_dir\"], \"evaluation_report_test.json\")\n","    with open(test_report_path, 'r') as f:\n","        test_report = json.load(f)\n","\n","    # Mostrar m√©tricas\n","    logger.info(\"M√©tricas en Transfer Learning (validaci√≥n):\")\n","    logger.info(f\"  - Accuracy: {tl_report['metrics']['accuracy']:.4f}\")\n","    logger.info(f\"  - Precision: {tl_report['metrics']['precision']:.4f}\")\n","    logger.info(f\"  - Recall (Sensibilidad): {tl_report['metrics']['recall']:.4f}\")\n","    logger.info(f\"  - Specificity: {tl_report['metrics']['specificity']:.4f}\")\n","    logger.info(f\"  - F1-Score: {tl_report['metrics']['f1_score']:.4f}\")\n","    logger.info(f\"  - ROC AUC: {tl_report['metrics']['roc_auc']:.4f}\")\n","\n","    logger.info(\"M√©tricas en Fine-Tuning (validaci√≥n):\")\n","    logger.info(f\"  - Accuracy: {ft_report['metrics']['accuracy']:.4f}\")\n","    logger.info(f\"  - Precision: {ft_report['metrics']['precision']:.4f}\")\n","    logger.info(f\"  - Recall (Sensibilidad): {ft_report['metrics']['recall']:.4f}\")\n","    logger.info(f\"  - Specificity: {ft_report['metrics']['specificity']:.4f}\")\n","    logger.info(f\"  - F1-Score: {ft_report['metrics']['f1_score']:.4f}\")\n","    logger.info(f\"  - ROC AUC: {ft_report['metrics']['roc_auc']:.4f}\")\n","\n","    logger.info(\"M√©tricas en Test (final):\")\n","    logger.info(f\"  - Accuracy: {test_report['metrics']['accuracy']:.4f}\")\n","    logger.info(f\"  - Precision: {test_report['metrics']['precision']:.4f}\")\n","    logger.info(f\"  - Recall (Sensibilidad): {test_report['metrics']['recall']:.4f}\")\n","    logger.info(f\"  - Specificity: {test_report['metrics']['specificity']:.4f}\")\n","    logger.info(f\"  - F1-Score: {test_report['metrics']['f1_score']:.4f}\")\n","    logger.info(f\"  - ROC AUC: {test_report['metrics']['roc_auc']:.4f}\")\n","\n","    # Crear tabla comparativa\n","    metrics = ['accuracy', 'precision', 'recall', 'specificity', 'f1_score', 'roc_auc']\n","    data = {\n","        'M√©trica': metrics,\n","        'Transfer Learning': [tl_report['metrics'][m] for m in metrics],\n","        'Fine-Tuning': [ft_report['metrics'][m] for m in metrics],\n","        'Test': [test_report['metrics'][m] for m in metrics]\n","    }\n","\n","    df = pd.DataFrame(data)\n","\n","    # Formatear para mostrar resultados\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 120)\n","    pd.set_option('display.precision', 4)\n","\n","    print(\"\\n=== TABLA COMPARATIVA DE M√âTRICAS ===\")\n","    print(df)\n","\n","    # Guardar tabla\n","    df.to_csv(os.path.join(CONFIG[\"output_dir\"], \"metrics_comparison.csv\"), index=False)\n","\n","except Exception as e:\n","    logger.error(f\"Error al cargar informes de evaluaci√≥n: {str(e)}\")\n","    logger.info(\"Aseg√∫rate de que las fases de Transfer Learning, Fine-Tuning y Test ya se han ejecutado.\")\n","\n","# Mostrar informaci√≥n sobre el modelo final\n","try:\n","    # Cargar informaci√≥n del benchmark\n","    benchmark_path = os.path.join(CONFIG[\"output_dir\"], \"benchmark_results.json\")\n","    with open(benchmark_path, 'r') as f:\n","        benchmark = json.load(f)\n","\n","    logger.info(\"\\nRendimiento del modelo:\")\n","    logger.info(f\"  - Tiempo por inferencia: {benchmark['avg_time_per_inference_ms']:.2f} ms\")\n","    logger.info(f\"  - Frames por segundo: {benchmark['fps']:.2f} FPS\")\n","\n","    # Cargar umbrales √≥ptimos\n","    thresholds_path = os.path.join(CONFIG[\"output_dir\"], \"optimal_thresholds.json\")\n","    with open(thresholds_path, 'r') as f:\n","        thresholds = json.load(f)\n","\n","    logger.info(\"\\nUmbrales √≥ptimos:\")\n","    logger.info(f\"  - Umbral √≥ptimo seg√∫n F1: {thresholds['f1_optimal']:.4f}\")\n","    logger.info(f\"  - Umbral √≥ptimo seg√∫n ROC: {thresholds['roc_optimal']:.4f}\")\n","\n","except Exception as e:\n","    logger.error(f\"Error al cargar informaci√≥n de rendimiento: {str(e)}\")\n","\n","# Mostrar rutas de los modelos exportados\n","try:\n","    model_paths = {\n","        'Modelo PyTorch': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_final.pt\"),\n","        'Modelo de Inferencia': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_inference.pt\"),\n","        'Modelo Hugging Face': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_hf\"),\n","        'Modelo ONNX': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}.onnx\"),\n","        'Script de Inferencia': os.path.join(CONFIG[\"output_dir\"], \"inference_example.py\")\n","    }\n","\n","    logger.info(\"\\nModelos exportados:\")\n","    for name, path in model_paths.items():\n","        exists = \"‚úì\" if os.path.exists(path) else \"‚úó\"\n","        logger.info(f\"  - {name}: {path} {exists}\")\n","\n","except Exception as e:\n","    logger.error(f\"Error al verificar rutas de modelos: {str(e)}\")\n","\n","logger.info(\"\\n¬°Entrenamiento y evaluaci√≥n del modelo TimeSformer para detecci√≥n de violencia completados!\")"],"metadata":{"id":"GcqFvvp4ohU5"},"execution_count":null,"outputs":[]}]}