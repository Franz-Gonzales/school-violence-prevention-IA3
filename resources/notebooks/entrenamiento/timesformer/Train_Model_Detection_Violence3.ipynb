{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMJNZ00OyuCFHrxbS6WSxbs"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Entrenamiento del Modelo TimeSformer para Detección de Violencia Escolar"],"metadata":{"id":"R3SPE4RyiTY7"}},{"cell_type":"markdown","source":["## Sección 1: Configuración del Entorno"],"metadata":{"id":"VBQGAHoXtO5c"}},{"cell_type":"code","source":["# Instalación de dependencias necesarias\n","!pip install -q transformers==4.35.0\n","!pip install -q torch==2.0.1 torchvision==0.15.2\n","!pip install -q pytorch-lightning==2.0.9\n","!pip install -q wandb==0.15.12\n","!pip install -q timm==0.9.2\n","!pip install -q scikit-learn==1.3.0\n","!pip install -q matplotlib==3.7.2 seaborn==0.12.2\n","!pip install -q opencv-python==4.8.0.76\n","!pip install -q einops==0.6.1\n","!pip install -q av==10.0.0\n","!pip install -q torchmetrics==1.0.3\n","!pip install -q tensorboardX==2.6.2"],"metadata":{"id":"0wGPzLMVtQFU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Importaciones básicas\n","import os\n","import sys\n","import random\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm.notebook import tqdm\n","import time\n","import json\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","import torchvision.transforms as transforms\n","from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n","from torchmetrics import Accuracy, Precision, Recall, F1Score, ConfusionMatrix, AUROC, Specificity\n","\n","import pytorch_lightning as pl\n","from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, LearningRateMonitor\n","from pytorch_lightning.loggers import TensorBoardLogger, WandbLogger\n","\n","from sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix\n","from sklearn.model_selection import train_test_split\n","\n","import cv2\n","import av\n","from einops import rearrange\n","\n","from transformers import (\n","    TimesformerForVideoClassification,\n","    TimesformerConfig,\n","    AutoImageProcessor,\n","    get_cosine_schedule_with_warmup\n",")\n","\n","# Verificar disponibilidad de GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","if device.type == 'cuda':\n","    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n","    print(f\"Memory available: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"],"metadata":{"id":"hvLvdrkrtWnb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configurar semillas para reproducibilidad\n","SEED = 42\n","random.seed(SEED)\n","np.random.seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","os.environ['PYTHONHASHSEED'] = str(SEED)"],"metadata":{"id":"G6Cy38uWtY9i"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Configuración de WandB (opcional pero recomendado para seguimiento de experimentos)\n","import wandb\n","try:\n","    wandb.login()\n","    wandb_available = True\n","except:\n","    print(\"WandB no disponible. Continuando sin tracking...\")\n","    wandb_available = False"],"metadata":{"id":"0aKXEVddta-i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 2: Montar Google Drive y Configurar Rutas"],"metadata":{"id":"4VkE-L0itcrL"}},{"cell_type":"code","source":["# Montar Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"VZNsW8uqxbPc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Configurar rutas al dataset\n","BASE_PATH = \"/content/drive/MyDrive/dataset_violencia\"\n","TRAIN_PATH = os.path.join(BASE_PATH, \"train\")\n","VAL_PATH = os.path.join(BASE_PATH, \"val\")\n","TEST_PATH = os.path.join(BASE_PATH, \"test\")\n","\n","# Rutas para guardar los modelos y checkpoints\n","MODEL_SAVE_PATH = \"/content/drive/MyDrive/violence_detection_models\"\n","CHECKPOINT_PATH = os.path.join(MODEL_SAVE_PATH, \"checkpoints\")\n","EXPORT_PATH = os.path.join(MODEL_SAVE_PATH, \"export\")\n","\n","# Crear directorios si no existen\n","os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n","os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n","os.makedirs(EXPORT_PATH, exist_ok=True)\n","\n","# Verificar la estructura del dataset\n","if not os.path.exists(TRAIN_PATH):\n","    raise FileNotFoundError(f\"El directorio de training {TRAIN_PATH} no existe\")\n","\n","# Verificar las clases\n","classes = os.listdir(TRAIN_PATH)\n","num_classes = len(classes)\n","print(f\"Clases detectadas: {classes} (Total: {num_classes})\")\n","\n","# Contar muestras en cada partición\n","def count_samples(path):\n","    return sum([len(os.listdir(os.path.join(path, c))) for c in os.listdir(path) if os.path.isdir(os.path.join(path, c))])\n","\n","train_samples = count_samples(TRAIN_PATH)\n","val_samples = count_samples(VAL_PATH)\n","test_samples = count_samples(TEST_PATH)\n","\n","print(f\"Muestras en Training: {train_samples}\")\n","print(f\"Muestras en Validation: {val_samples}\")\n","print(f\"Muestras en Test: {test_samples}\")"],"metadata":{"id":"yMDWmGSjteb_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Verificar estructura del dataset\n","for path in [TRAIN_PATH, VAL_PATH, TEST_PATH]:\n","    if not os.path.exists(path):\n","        raise FileNotFoundError(f\"El directorio {path} no existe\")\n","    for class_name in os.listdir(path):\n","        class_dir = os.path.join(path, class_name)\n","        if not os.path.isdir(class_dir):\n","            continue\n","        num_samples = len([f for f in os.listdir(class_dir) if f.endswith(('.mp4', '.avi'))])\n","        print(f\"Clase '{class_name}' en {os.path.basename(path)}: {num_samples} videos\")"],"metadata":{"id":"t9a27jAExecD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 3: Crear Dataset y DataLoader para Videos"],"metadata":{"id":"0FNNHtd_ti2R"}},{"cell_type":"code","source":["class VideoDataset(Dataset):\n","    \"\"\"\n","    Dataset para videos de clasificación de violencia\n","    \"\"\"\n","    def __init__(self, root_dir, processor, num_frames=8, max_duration=6, transform=None,\n","                 frame_sample_rate=2, clip_start=0, debug=False):\n","        self.root_dir = root_dir\n","        self.processor = processor\n","        self.num_frames = num_frames\n","        self.max_duration = max_duration\n","        self.transform = transform\n","        self.frame_sample_rate = frame_sample_rate\n","        self.clip_start = clip_start\n","        self.debug = debug\n","\n","        self.classes = sorted(os.listdir(root_dir))\n","        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n","\n","        self.samples = []\n","        for class_name in self.classes:\n","            class_dir = os.path.join(root_dir, class_name)\n","            for video_name in os.listdir(class_dir):\n","                if video_name.endswith(('.mp4', '.avi')):\n","                    self.samples.append((os.path.join(class_dir, video_name), self.class_to_idx[class_name]))\n","\n","        if debug:\n","            # Limitar cantidad de muestras para debug\n","            self.samples = self.samples[:100]\n","\n","    def __len__(self):\n","        return len(self.samples)\n","\n","    def load_video(self, video_path):\n","        \"\"\"\n","        Carga un video y extrae los frames necesarios\n","        \"\"\"\n","        try:\n","            container = av.open(video_path)\n","            indices = self._sample_frame_indices(container)\n","\n","            video_frames = []\n","            container.seek(0)\n","            for i, frame in enumerate(container.decode(video=0)):\n","                if i in indices:\n","                    img = frame.to_ndarray(format=\"rgb\")\n","                    video_frames.append(img)\n","                if len(video_frames) == self.num_frames:\n","                    break\n","\n","            # Si no se obtuvieron suficientes frames, repetir el último\n","            if len(video_frames) < self.num_frames:\n","                last_frame = video_frames[-1] if video_frames else np.zeros((224, 224, 3), dtype=np.uint8)\n","                while len(video_frames) < self.num_frames:\n","                    video_frames.append(last_frame)\n","\n","            return video_frames\n","        except Exception as e:\n","            print(f\"Error cargando video {video_path}: {e}\")\n","            # Crear frames negros como fallback\n","            return [np.zeros((224, 224, 3), dtype=np.uint8) for _ in range(self.num_frames)]\n","\n","    def _sample_frame_indices(self, container):\n","        \"\"\"\n","        Muestrea índices de frames uniformemente distribuidos\n","        \"\"\"\n","        # Obtener información más segura sobre el video\n","        try:\n","            video_stream = container.streams.video[0]\n","            total_frames = video_stream.frames\n","\n","            if total_frames <= 0 or total_frames is None:\n","                # Estimar frames basado en la duración y fps\n","                fps = video_stream.average_rate\n","                duration = video_stream.duration\n","                if fps and duration:\n","                    total_frames = int(duration * float(fps) / 1e6)  # duration en microsegundos\n","        except Exception as e:\n","            print(f\"Error estimando frames: {e}\")\n","            total_frames = 0\n","\n","        if total_frames <= 0:\n","            # Fallback: asumir 30 fps y una duración razonable\n","            total_frames = min(300, int(self.max_duration * 30))\n","\n","        # Calcular frames a muestrear\n","        start_idx = min(self.clip_start, max(0, total_frames - self.num_frames * self.frame_sample_rate))\n","\n","        # Usar np.linspace para asegurar distribución uniforme\n","        if total_frames < self.num_frames:\n","            # Si hay menos frames que los requeridos, repetir índices\n","            indices = np.array(list(range(total_frames)) * (self.num_frames // total_frames + 1))[:self.num_frames]\n","        else:\n","            indices = np.linspace(start_idx, min(total_frames - 1, start_idx + (self.num_frames - 1) * self.frame_sample_rate),\n","                              self.num_frames, dtype=int)\n","\n","        return indices\n","\n","    def __getitem__(self, idx):\n","        video_path, label = self.samples[idx]\n","\n","        try:\n","            frames = self.load_video(video_path)\n","\n","            # Aplicar transformaciones adicionales\n","            if self.transform:\n","                frames = [self.transform(frame) for frame in frames]\n","\n","            # Procesar frames para TimeSformer\n","            inputs = self.processor(frames, return_tensors=\"pt\")\n","            # Quitar la dimensión de batch para que el DataLoader la pueda manejar\n","            pixel_values = inputs.pixel_values.squeeze(0)\n","\n","            return {\n","                \"pixel_values\": pixel_values,\n","                \"labels\": torch.tensor(label, dtype=torch.long),\n","                \"video_path\": video_path\n","            }\n","\n","        except Exception as e:\n","            print(f\"Error procesando video {video_path}: {e}\")\n","            # Manejar error devolviendo un tensor de ceros\n","            h, w = 224, 224\n","            dummy_frames = torch.zeros((self.num_frames, 3, h, w))\n","            return {\n","                \"pixel_values\": dummy_frames,\n","                \"labels\": torch.tensor(label, dtype=torch.long),\n","                \"video_path\": video_path\n","            }\n","\n","# Función de colación personalizada para batch processing\n","def collate_fn(batch):\n","    pixel_values = torch.stack([item['pixel_values'] for item in batch])\n","    labels = torch.stack([item['labels'] for item in batch])\n","    video_paths = [item['video_path'] for item in batch]\n","\n","    return {\n","        'pixel_values': pixel_values,\n","        'labels': labels,\n","        'video_paths': video_paths\n","    }"],"metadata":{"id":"WA9doPA7tkCc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 4: Configuración del Modelo y Procesador"],"metadata":{"id":"EZjG7ZdstnGy"}},{"cell_type":"code","source":["# Cargar el procesador de imágenes para TimeSformer\n","# Cargar el procesador de imágenes para TimeSformer\n","MODEL_CHECKPOINT = \"facebook/timesformer-base-finetuned-k400\"  # Modelo pre-entrenado en Kinetics-400\n","\n","try:\n","    processor = AutoImageProcessor.from_pretrained(MODEL_CHECKPOINT)\n","    print(f\"Procesador cargado correctamente desde {MODEL_CHECKPOINT}\")\n","except Exception as e:\n","    print(f\"Error cargando procesador desde {MODEL_CHECKPOINT}: {e}\")\n","    # Alternativa: usar otro checkpoint\n","    BACKUP_CHECKPOINT = \"facebook/timesformer-base-finetuned-ssv2\"\n","    print(f\"Intentando con checkpoint alternativo: {BACKUP_CHECKPOINT}\")\n","    try:\n","        processor = AutoImageProcessor.from_pretrained(BACKUP_CHECKPOINT)\n","        MODEL_CHECKPOINT = BACKUP_CHECKPOINT\n","        print(f\"Procesador cargado correctamente desde {MODEL_CHECKPOINT}\")\n","    except Exception as e2:\n","        raise RuntimeError(f\"No se pudo cargar el procesador: {e2}\")\n","\n","# Definir transforms adicionales (aumentación de datos)\n","train_transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n","    transforms.RandomHorizontalFlip(),\n","    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n","    transforms.ToTensor(),\n","    # La normalización la hace el procesador\n","])\n","\n","valid_transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","    # La normalización la hace el procesador\n","])\n","\n","# Crear datasets para cada partición\n","def create_datasets(num_frames=8, frame_sample_rate=2):\n","    train_dataset = VideoDataset(\n","        root_dir=TRAIN_PATH,\n","        processor=processor,\n","        num_frames=num_frames,\n","        transform=train_transform,\n","        frame_sample_rate=frame_sample_rate\n","    )\n","\n","    val_dataset = VideoDataset(\n","        root_dir=VAL_PATH,\n","        processor=processor,\n","        num_frames=num_frames,\n","        transform=valid_transform,\n","        frame_sample_rate=frame_sample_rate\n","    )\n","\n","    test_dataset = VideoDataset(\n","        root_dir=TEST_PATH,\n","        processor=processor,\n","        num_frames=num_frames,\n","        transform=valid_transform,\n","        frame_sample_rate=frame_sample_rate\n","    )\n","\n","    return train_dataset, val_dataset, test_dataset\n","\n","# Definir función para crear dataloaders\n","def create_dataloaders(train_dataset, val_dataset, test_dataset, batch_size=4, num_workers=2):\n","    train_loader = DataLoader(\n","        train_dataset,\n","        batch_size=batch_size,\n","        shuffle=True,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        collate_fn=collate_fn\n","    )\n","\n","    val_loader = DataLoader(\n","        val_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        collate_fn=collate_fn\n","    )\n","\n","    test_loader = DataLoader(\n","        test_dataset,\n","        batch_size=batch_size,\n","        shuffle=False,\n","        num_workers=num_workers,\n","        pin_memory=True,\n","        collate_fn=collate_fn\n","    )\n","\n","    return train_loader, val_loader, test_loader"],"metadata":{"id":"1S5RiApktoN8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 5: Definir el Modelo en PyTorch Lightning"],"metadata":{"id":"RmGqgi1Ctrnx"}},{"cell_type":"code","source":["class TimeSformerLightningModule(pl.LightningModule):\n","    def __init__(\n","        self,\n","        num_classes=2,\n","        num_frames=8,\n","        learning_rate=2e-5,\n","        weight_decay=0.05,\n","        warmup_steps=100,\n","        total_steps=2000,\n","        freeze_backbone=True,\n","        unfreeze_layers=3,\n","        model_checkpoint=\"facebook/timesformer-base-finetuned-k400\"\n","    ):\n","        super().__init__()\n","        self.save_hyperparameters()\n","\n","        # Cargar configuración del modelo\n","        self.config = TimesformerConfig.from_pretrained(\n","            model_checkpoint,\n","            num_frames=num_frames,\n","            num_labels=num_classes\n","        )\n","\n","        # Cargar modelo pre-entrenado\n","        self.model = TimesformerForVideoClassification.from_pretrained(\n","            model_checkpoint,\n","            config=self.config,\n","            ignore_mismatched_sizes=True  # Permitir cambiar num_classes\n","        )\n","\n","        # Configurar congelamiento de capas para transfer learning\n","        # Configurar congelamiento de capas para transfer learning\n","        if freeze_backbone:\n","            # Congelar todos los parámetros del modelo base\n","            for param in self.model.parameters():\n","                param.requires_grad = False\n","\n","            # Descongelar la cabeza de clasificación\n","            for param in self.model.classifier.parameters():\n","                param.requires_grad = True\n","\n","            # Descongelar las últimas N capas del transformer (para fine-tuning parcial)\n","            if unfreeze_layers > 0:\n","                num_encoder_layers = len(list(self.model.timesformer.encoder.layer))\n","                print(f\"Número total de capas encoder: {num_encoder_layers}\")\n","\n","                for i in range(1, min(unfreeze_layers + 1, num_encoder_layers + 1)):\n","                    layer_idx = num_encoder_layers - i\n","                    layer = self.model.timesformer.encoder.layer[layer_idx]\n","                    print(f\"Descongelando capa encoder: {layer_idx}\")\n","                    for param in layer.parameters():\n","                        param.requires_grad = True\n","\n","        # Inicializar métricas\n","        self.accuracy = Accuracy(task=\"multiclass\", num_classes=num_classes)\n","        self.precision_metric = Precision(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n","        self.recall_metric = Recall(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n","        self.f1_metric = F1Score(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n","        self.specificity_metric = Specificity(task=\"multiclass\", num_classes=num_classes, average=\"macro\")\n","        self.auroc = AUROC(task=\"multiclass\", num_classes=num_classes)\n","        self.confusion_matrix = ConfusionMatrix(task=\"multiclass\", num_classes=num_classes)\n","\n","        # Almacenar predicciones para métricas finales\n","        self.test_preds = []\n","        self.test_targets = []\n","        self.test_probs = []\n","\n","    def forward(self, pixel_values):\n","        return self.model(pixel_values=pixel_values)\n","\n","    def training_step(self, batch, batch_idx):\n","        pixel_values = batch[\"pixel_values\"]\n","        labels = batch[\"labels\"]\n","\n","        outputs = self.model(pixel_values=pixel_values, labels=labels)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","        # Calcular métricas\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.accuracy(preds, labels)\n","        prec = self.precision_metric(preds, labels)\n","        rec = self.recall_metric(preds, labels)\n","        f1 = self.f1_metric(preds, labels)\n","\n","        # Registrar métricas en el logger\n","        self.log(\"train_loss\", loss, prog_bar=True, on_step=True, on_epoch=True, sync_dist=True)\n","        self.log(\"train_acc\", acc, prog_bar=True, on_step=False, on_epoch=True, sync_dist=True)\n","        self.log(\"train_precision\", prec, on_step=False, on_epoch=True, sync_dist=True)\n","        self.log(\"train_recall\", rec, on_step=False, on_epoch=True, sync_dist=True)\n","        self.log(\"train_f1\", f1, on_step=False, on_epoch=True, sync_dist=True)\n","\n","        return loss\n","\n","    def validation_step(self, batch, batch_idx):\n","        pixel_values = batch[\"pixel_values\"]\n","        labels = batch[\"labels\"]\n","\n","        outputs = self.model(pixel_values=pixel_values, labels=labels)\n","        loss = outputs.loss\n","        logits = outputs.logits\n","\n","        # Calcular métricas\n","        preds = torch.argmax(logits, dim=1)\n","        acc = self.accuracy(preds, labels)\n","        prec = self.precision_metric(preds, labels)\n","        rec = self.recall_metric(preds, labels)\n","        f1 = self.f1_metric(preds, labels)\n","        spec = self.specificity_metric(preds, labels)\n","\n","        # Registrar métricas en el logger\n","        self.log(\"val_loss\", loss, prog_bar=True, sync_dist=True)\n","        self.log(\"val_acc\", acc, prog_bar=True, sync_dist=True)\n","        self.log(\"val_precision\", prec, sync_dist=True)\n","        self.log(\"val_recall\", rec, sync_dist=True)\n","        self.log(\"val_f1\", f1, sync_dist=True)\n","        self.log(\"val_specificity\", spec, sync_dist=True)\n","\n","        return loss\n","\n","    def test_step(self, batch, batch_idx):\n","        pixel_values = batch[\"pixel_values\"]\n","        labels = batch[\"labels\"]\n","\n","        outputs = self.model(pixel_values=pixel_values)\n","        logits = outputs.logits\n","\n","        # Guardar predicciones, targets y probabilidades para análisis posterior\n","        preds = torch.argmax(logits, dim=1)\n","        probs = F.softmax(logits, dim=1)\n","\n","        self.test_preds.append(preds.cpu())\n","        self.test_targets.append(labels.cpu())\n","        self.test_probs.append(probs.cpu())\n","\n","        # Calcular métricas\n","        acc = self.accuracy(preds, labels)\n","        self.log(\"test_acc\", acc, sync_dist=True)\n","\n","        return {\"test_loss\": 0.0, \"preds\": preds, \"targets\": labels}\n","\n","    def on_test_epoch_end(self):\n","        # Concatenar todas las predicciones y targets\n","        preds = torch.cat(self.test_preds, dim=0).numpy()\n","        targets = torch.cat(self.test_targets, dim=0).numpy()\n","        probs = torch.cat(self.test_probs, dim=0).numpy()\n","\n","        # Calcular y mostrar todas las métricas\n","        report = classification_report(targets, preds, target_names=self.trainer.datamodule.classes)\n","        conf_matrix = confusion_matrix(targets, preds)\n","\n","        # Calcular métricas específicas\n","        tn, fp, fn, tp = conf_matrix.ravel() if conf_matrix.shape == (2, 2) else (0, 0, 0, 0)\n","        accuracy = (tp + tn) / (tp + tn + fp + fn) if (tp + tn + fp + fn) > 0 else 0\n","        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n","        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n","        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n","        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n","\n","        # True Positive Rate (TPR) = Recall\n","        tpr = recall\n","        # False Positive Rate (FPR)\n","        fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n","\n","        # Guardar resultados en el logger\n","        self.logger.experiment.add_text(\"Classification Report\", report)\n","\n","        # Imprimir resultados\n","        print(f\"\\nClassification Report:\\n{report}\")\n","        print(f\"\\nConfusion Matrix:\\n{conf_matrix}\")\n","        print(f\"\\nMétricas clave:\")\n","        print(f\"Accuracy: {accuracy:.4f}\")\n","        print(f\"Precision: {precision:.4f}\")\n","        print(f\"Recall (TPR): {recall:.4f}\")\n","        print(f\"Specificity: {specificity:.4f}\")\n","        print(f\"F1-Score: {f1:.4f}\")\n","        print(f\"False Positive Rate: {fpr:.4f}\")\n","\n","        # Limpiar acumuladores para futuras evaluaciones\n","        self.test_preds.clear()\n","        self.test_targets.clear()\n","        self.test_probs.clear()\n","\n","    def configure_optimizers(self):\n","        # Crear un optimizador con weight decay que no afecte a bias y normalization\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters()\n","                          if not any(nd in n for nd in no_decay) and p.requires_grad],\n","                \"weight_decay\": self.hparams.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in self.model.named_parameters()\n","                          if any(nd in n for nd in no_decay) and p.requires_grad],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","\n","        optimizer = optim.AdamW(optimizer_grouped_parameters, lr=self.hparams.learning_rate)\n","\n","        # Configurar scheduler\n","        scheduler = get_cosine_schedule_with_warmup(\n","            optimizer,\n","            num_warmup_steps=self.hparams.warmup_steps,\n","            num_training_steps=self.hparams.total_steps\n","        )\n","\n","        return {\n","            \"optimizer\": optimizer,\n","            \"lr_scheduler\": {\n","                \"scheduler\": scheduler,\n","                \"interval\": \"step\",\n","            },\n","        }"],"metadata":{"id":"pjvajgg6ttFZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 6: Configurar LightningDataModule"],"metadata":{"id":"tm6D-lQJty7j"}},{"cell_type":"code","source":["class VideoDataModule(pl.LightningDataModule):\n","    def __init__(\n","        self,\n","        train_path,\n","        val_path,\n","        test_path,\n","        processor,\n","        num_frames=8,\n","        frame_sample_rate=2,\n","        batch_size=4,\n","        num_workers=2\n","    ):\n","        super().__init__()\n","        self.train_path = train_path\n","        self.val_path = val_path\n","        self.test_path = test_path\n","        self.processor = processor\n","        self.num_frames = num_frames\n","        self.frame_sample_rate = frame_sample_rate\n","        self.batch_size = batch_size\n","        self.num_workers = num_workers\n","\n","        # Definir clases\n","        self.classes = sorted(os.listdir(train_path))\n","\n","    def setup(self, stage=None):\n","        # Definir datasets en diferentes etapas\n","        if stage == 'fit' or stage is None:\n","            self.train_dataset = VideoDataset(\n","                root_dir=self.train_path,\n","                processor=self.processor,\n","                num_frames=self.num_frames,\n","                transform=train_transform,\n","                frame_sample_rate=self.frame_sample_rate\n","            )\n","\n","            self.val_dataset = VideoDataset(\n","                root_dir=self.val_path,\n","                processor=self.processor,\n","                num_frames=self.num_frames,\n","                transform=valid_transform,\n","                frame_sample_rate=self.frame_sample_rate\n","            )\n","\n","        if stage == 'test' or stage is None:\n","            self.test_dataset = VideoDataset(\n","                root_dir=self.test_path,\n","                processor=self.processor,\n","                num_frames=self.num_frames,\n","                transform=valid_transform,\n","                frame_sample_rate=self.frame_sample_rate\n","            )\n","\n","    def train_dataloader(self):\n","        return DataLoader(\n","            self.train_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=True,\n","            num_workers=self.num_workers,\n","            pin_memory=True,\n","            collate_fn=collate_fn\n","        )\n","\n","    def val_dataloader(self):\n","        return DataLoader(\n","            self.val_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.num_workers,\n","            pin_memory=True,\n","            collate_fn=collate_fn\n","        )\n","\n","    def test_dataloader(self):\n","        return DataLoader(\n","            self.test_dataset,\n","            batch_size=self.batch_size,\n","            shuffle=False,\n","            num_workers=self.num_workers,\n","            pin_memory=True,\n","            collate_fn=collate_fn\n","        )"],"metadata":{"id":"ZKurdysVtz5M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 7: Transfer Learning - Primera Fase"],"metadata":{"id":"cApl3fYjt3UB"}},{"cell_type":"code","source":["# Configurar parámetros del experimento para transfer learning\n","experiment_name = \"violence_detection_transfer_learning\"\n","NUM_FRAMES = 8\n","BATCH_SIZE = 4\n","LEARNING_RATE = 2e-5\n","WEIGHT_DECAY = 0.05\n","MAX_EPOCHS = 20\n","NUM_WORKERS = 2  # Ajustar según disponibilidad en Colab\n","\n","# Calcular steps para scheduler\n","num_devices = 1  # En Colab normalmente es 1 GPU\n","data_module = VideoDataModule(\n","    train_path=TRAIN_PATH,\n","    val_path=VAL_PATH,\n","    test_path=TEST_PATH,\n","    processor=processor,\n","    num_frames=NUM_FRAMES,\n","    batch_size=BATCH_SIZE,\n","    num_workers=NUM_WORKERS\n",")\n","data_module.setup()\n","\n","steps_per_epoch = len(data_module.train_dataloader())\n","total_steps = steps_per_epoch * MAX_EPOCHS\n","warmup_steps = min(100, int(0.1 * total_steps))\n","\n","print(f\"Steps por epoch: {steps_per_epoch}\")\n","print(f\"Total steps: {total_steps}\")\n","print(f\"Warmup steps: {warmup_steps}\")\n","\n","# Configurar callbacks\n","checkpoint_callback = ModelCheckpoint(\n","    dirpath=CHECKPOINT_PATH,\n","    filename=\"timesformer-violence-{epoch:02d}-{val_f1:.4f}\",\n","    monitor=\"val_f1\",\n","    mode=\"max\",\n","    save_top_k=3,\n","    save_last=True,\n","    verbose=True\n",")\n","\n","early_stopping_callback = EarlyStopping(\n","    monitor=\"val_f1\",\n","    patience=5,\n","    mode=\"max\",\n","    verbose=True\n",")\n","\n","lr_monitor = LearningRateMonitor(logging_interval=\"step\")\n","\n","# Configurar loggers\n","tb_logger = TensorBoardLogger(\"lightning_logs\", name=experiment_name)\n","\n","# Configurar WandB logger si está disponible\n","loggers = [tb_logger]\n","if wandb_available:\n","    wandb_logger = WandbLogger(\n","        project=\"violence_detection\",\n","        name=experiment_name,\n","        log_model=True\n","    )\n","    loggers.append(wandb_logger)\n","\n","# Crear modelo con capas congeladas para transfer learning\n","model = TimeSformerLightningModule(\n","    num_classes=len(data_module.classes),\n","    num_frames=NUM_FRAMES,\n","    learning_rate=LEARNING_RATE,\n","    weight_decay=WEIGHT_DECAY,\n","    warmup_steps=warmup_steps,\n","    total_steps=total_steps,\n","    freeze_backbone=True,  # Congelar la mayoría del modelo\n","    unfreeze_layers=1      # Solo descongelar la última capa transformer\n",")\n","\n","# Mostrar capas congeladas y descongeladas\n","unfrozen_params = [name for name, param in model.named_parameters() if param.requires_grad]\n","frozen_params = [name for name, param in model.named_parameters() if not param.requires_grad]\n","\n","print(f\"Parámetros descongelados: {len(unfrozen_params)}\")\n","print(f\"Algunos parámetros descongelados: {unfrozen_params[:5]}\")\n","print(f\"Parámetros congelados: {len(frozen_params)}\")\n","print(f\"Algunos parámetros congelados: {frozen_params[:5]}\")\n","\n","# Iniciar entrenamiento\n","trainer = pl.Trainer(\n","    max_epochs=MAX_EPOCHS,\n","    callbacks=[checkpoint_callback, early_stopping_callback, lr_monitor],\n","    logger=loggers,\n","    log_every_n_steps=10,\n","    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n","    precision=\"16-mixed\" if torch.cuda.is_available() else \"32\",  # Usar precisión mixta para ahorrar memoria\n",")\n","\n","# Entrenar modelo\n","trainer.fit(model, datamodule=data_module)\n","\n","# Evaluar en conjunto de validación\n","val_results = trainer.validate(model, datamodule=data_module)\n","print(f\"Resultados de validación: {val_results}\")\n","\n","# Guardar el mejor modelo de transfer learning\n","best_model_path = checkpoint_callback.best_model_path\n","if best_model_path:\n","    print(f\"Mejor modelo guardado en: {best_model_path}\")\n","\n","    # Cargar el mejor modelo\n","    best_model = TimeSformerLightningModule.load_from_checkpoint(best_model_path)\n","\n","    # Guardar el modelo completo para uso posterior\n","    transfer_model_path = os.path.join(MODEL_SAVE_PATH, \"transfer_learning_model\")\n","    os.makedirs(transfer_model_path, exist_ok=True)\n","    best_model.model.save_pretrained(transfer_model_path)\n","    processor.save_pretrained(transfer_model_path)\n","\n","    print(f\"Modelo de transfer learning guardado en: {transfer_model_path}\")\n","else:\n","    print(\"No se encontró un mejor modelo. Usando el último entrenado.\")\n","    transfer_model_path = os.path.join(MODEL_SAVE_PATH, \"transfer_learning_model\")\n","    os.makedirs(transfer_model_path, exist_ok=True)\n","    model.model.save_pretrained(transfer_model_path)\n","    processor.save_pretrained(transfer_model_path)"],"metadata":{"id":"X-o5O6ait4XL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 8: Evaluación Detallada del Modelo de Transfer Learning"],"metadata":{"id":"wMk5mbyjuAAi"}},{"cell_type":"code","source":["# Evaluar el mejor modelo en el conjunto de prueba\n","print(\"Evaluando modelo de transfer learning en el conjunto de prueba...\")\n","test_results = trainer.test(best_model if best_model_path else model, datamodule=data_module)\n","\n","# Función para generar gráficos de evaluación\n","def plot_confusion_matrix(cm, classes, title='Matriz de Confusión', cmap=plt.cm.Blues):\n","    plt.figure(figsize=(8, 8))\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    # Añadir valores numéricos\n","    thresh = cm.max() / 2.\n","    for i in range(cm.shape[0]):\n","        for j in range(cm.shape[1]):\n","            plt.text(j, i, format(cm[i, j], 'd'),\n","                     horizontalalignment=\"center\",\n","                     color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('Etiqueta Real')\n","    plt.xlabel('Etiqueta Predicha')\n","    return plt\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, title='Curva ROC'):\n","    plt.figure(figsize=(8, 8))\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('Tasa de Falsos Positivos')\n","    plt.ylabel('Tasa de Verdaderos Positivos')\n","    plt.title(title)\n","    plt.legend(loc=\"lower right\")\n","    return plt\n","\n","# Evaluar modelo con predicciones detalladas\n","def evaluate_detailed(model, data_loader, classes):\n","    model.eval()\n","    model.to(device)\n","\n","    all_preds = []\n","    all_labels = []\n","    all_probs = []\n","\n","    with torch.no_grad():\n","        for batch in tqdm(data_loader, desc=\"Evaluando\"):\n","            pixel_values = batch[\"pixel_values\"].to(device)\n","            labels = batch[\"labels\"].to(device)\n","\n","            outputs = model.model(pixel_values=pixel_values)\n","            logits = outputs.logits\n","\n","            preds = torch.argmax(logits, dim=1)\n","            probs = F.softmax(logits, dim=1)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","            all_probs.extend(probs.cpu().numpy())\n","\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","    all_probs = np.array(all_probs)\n","\n","    # Calcular métricas\n","    report = classification_report(all_labels, all_preds, target_names=classes, output_dict=True)\n","    conf_mat = confusion_matrix(all_labels, all_preds)\n","\n","    # Para caso binario (violencia / no_violencia)\n","    if len(classes) == 2:\n","        # Calcular TPR/FPR para curva ROC\n","        fpr, tpr, _ = roc_curve(all_labels, all_probs[:, 1])\n","        roc_auc = auc(fpr, tpr)\n","\n","        # Extraer métricas específicas\n","        accuracy = (all_preds == all_labels).mean()\n","\n","        # Para clase positiva (violencia = 1)\n","        precision = report['1']['precision'] if '1' in report else report[classes[1]]['precision']\n","        recall = report['1']['recall'] if '1' in report else report[classes[1]]['recall']\n","        f1 = report['1']['f1-score'] if '1' in report else report[classes[1]]['f1-score']\n","\n","        # Calcular especificidad (TNR)\n","        tn, fp, fn, tp = conf_mat.ravel()\n","        specificity = tn / (tn + fp)\n","\n","        # Tasa de falsos positivos\n","        false_positive_rate = fp / (fp + tn)\n","\n","        metrics = {\n","            'accuracy': accuracy,\n","            'precision': precision,\n","            'recall': recall,\n","            'specificity': specificity,\n","            'f1': f1,\n","            'tpr': recall,  # TPR = recall\n","            'fpr': false_positive_rate,\n","            'roc_auc': roc_auc\n","        }\n","\n","        # Generar gráficos\n","        cm_plot = plot_confusion_matrix(conf_mat, classes, title='Matriz de Confusión - Transfer Learning')\n","        roc_plot = plot_roc_curve(fpr, tpr, roc_auc, title='Curva ROC - Transfer Learning')\n","\n","        # Guardar gráficos\n","        cm_plot.savefig(os.path.join(MODEL_SAVE_PATH, 'transfer_learning_confusion_matrix.png'))\n","        roc_plot.savefig(os.path.join(MODEL_SAVE_PATH, 'transfer_learning_roc_curve.png'))\n","\n","        plt.close('all')\n","\n","        # Mostrar métricas\n","        print(\"\\n=== Métricas Detalladas - Transfer Learning ===\")\n","        print(f\"Accuracy: {accuracy:.4f}\")\n","        print(f\"Precision: {precision:.4f}\")\n","        print(f\"Recall (TPR): {recall:.4f}\")\n","        print(f\"Specificity (TNR): {specificity:.4f}\")\n","        print(f\"F1-Score: {f1:.4f}\")\n","        print(f\"False Positive Rate: {false_positive_rate:.4f}\")\n","        print(f\"ROC AUC: {roc_auc:.4f}\")\n","\n","        # Guardar métricas\n","        with open(os.path.join(MODEL_SAVE_PATH, 'transfer_learning_metrics.json'), 'w') as f:\n","            json.dump(metrics, f, indent=4)\n","\n","        return metrics, conf_mat, (fpr, tpr, roc_auc)\n","\n","    # Para caso multiclase (si se amplía a más categorías)\n","    else:\n","        # Calcular accuracy general\n","        accuracy = (all_preds == all_labels).mean()\n","\n","        # Calcular macro promedio de métricas\n","        precision = np.mean([report[cls]['precision'] for cls in classes])\n","        recall = np.mean([report[cls]['recall'] for cls in classes])\n","        f1 = np.mean([report[cls]['f1-score'] for cls in classes])\n","\n","        # Para multiclase, calcular One-vs-Rest ROC AUC\n","        roc_auc = {}\n","        for i, cls in enumerate(classes):\n","            fpr, tpr, _ = roc_curve((all_labels == i).astype(int), all_probs[:, i])\n","            roc_auc[cls] = auc(fpr, tpr)\n","\n","        metrics = {\n","            'accuracy': accuracy,\n","            'precision': precision,\n","            'recall': recall,\n","            'f1': f1,\n","            'roc_auc': roc_auc\n","        }\n","\n","        # Generar gráfico de confusión\n","        cm_plot = plot_confusion_matrix(conf_mat, classes, title='Matriz de Confusión - Multiclase')\n","        cm_plot.savefig(os.path.join(MODEL_SAVE_PATH, 'transfer_learning_confusion_matrix.png'))\n","        plt.close('all')\n","\n","        # Mostrar métricas\n","        print(\"\\n=== Métricas Detalladas - Transfer Learning (Multiclase) ===\")\n","        print(f\"Accuracy: {accuracy:.4f}\")\n","        print(f\"Precision (macro): {precision:.4f}\")\n","        print(f\"Recall (macro): {recall:.4f}\")\n","        print(f\"F1-Score (macro): {f1:.4f}\")\n","        for cls in classes:\n","            print(f\"ROC AUC ({cls}): {roc_auc[cls]:.4f}\")\n","\n","        # Guardar métricas\n","        with open(os.path.join(MODEL_SAVE_PATH, 'transfer_learning_metrics.json'), 'w') as f:\n","            json.dump(metrics, f, indent=4)\n","\n","        return metrics, conf_mat, roc_auc\n","\n","# Ejecutar evaluación detallada en el conjunto de prueba\n","loaded_model = best_model if best_model_path else model\n","test_metrics, conf_mat, roc_data = evaluate_detailed(\n","    loaded_model,\n","    data_module.test_dataloader(),\n","    data_module.classes\n",")"],"metadata":{"id":"lw6TwNDYuBaE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 9: Fine-Tuning - Segunda Fase"],"metadata":{"id":"wDdZ0N_euG6K"}},{"cell_type":"code","source":["# Cargar el modelo pre-entrenado con transfer learning\n","print(\"Iniciando fase de fine-tuning...\")\n","pretrained_model_path = transfer_model_path\n","\n","# Configurar parámetros para fine-tuning\n","experiment_name = \"violence_detection_fine_tuning\"\n","NUM_FRAMES = 8  # Mantener el mismo número de frames\n","BATCH_SIZE = 2  # Reducir batch size para fine-tuning\n","LEARNING_RATE = 5e-6  # Reducir el learning rate para fine-tuning\n","WEIGHT_DECAY = 0.01\n","MAX_EPOCHS = 15\n","NUM_WORKERS = 2\n","\n","# Crear modelo para fine-tuning, descongelando más capas\n","model_ft = TimeSformerLightningModule(\n","    num_classes=len(data_module.classes),\n","    num_frames=NUM_FRAMES,\n","    learning_rate=LEARNING_RATE,\n","    weight_decay=WEIGHT_DECAY,\n","    warmup_steps=warmup_steps,\n","    total_steps=total_steps,\n","    freeze_backbone=False,  # Descongelar todo el modelo\n","    model_checkpoint=pretrained_model_path\n",")\n","\n","# Configurar callbacks para fine-tuning\n","checkpoint_callback_ft = ModelCheckpoint(\n","    dirpath=CHECKPOINT_PATH,\n","    filename=\"timesformer-violence-ft-{epoch:02d}-{val_f1:.4f}\",\n","    monitor=\"val_f1\",\n","    mode=\"max\",\n","    save_top_k=3,\n","    save_last=True,\n","    verbose=True\n",")\n","\n","early_stopping_callback_ft = EarlyStopping(\n","    monitor=\"val_f1\",\n","    patience=7,\n","    mode=\"max\",\n","    verbose=True\n",")\n","\n","lr_monitor_ft = LearningRateMonitor(logging_interval=\"step\")\n","\n","# Configurar loggers para fine-tuning\n","tb_logger_ft = TensorBoardLogger(\"lightning_logs\", name=experiment_name)\n","\n","loggers_ft = [tb_logger_ft]\n","if wandb_available:\n","    wandb_logger_ft = WandbLogger(\n","        project=\"violence_detection\",\n","        name=experiment_name,\n","        log_model=True\n","    )\n","    loggers_ft.append(wandb_logger_ft)\n","\n","# Iniciar entrenamiento de fine-tuning\n","trainer_ft = pl.Trainer(\n","    max_epochs=MAX_EPOCHS,\n","    callbacks=[checkpoint_callback_ft, early_stopping_callback_ft, lr_monitor_ft],\n","    logger=loggers_ft,\n","    log_every_n_steps=10,\n","    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n","    precision=\"16-mixed\" if torch.cuda.is_available() else \"32\",\n",")\n","\n","# Entrenar modelo con fine-tuning\n","trainer_ft.fit(model_ft, datamodule=data_module)\n","\n","# Evaluar en conjunto de validación\n","val_results_ft = trainer_ft.validate(model_ft, datamodule=data_module)\n","print(f\"Resultados de validación con fine-tuning: {val_results_ft}\")\n","\n","# Guardar el mejor modelo de fine-tuning\n","best_model_path_ft = checkpoint_callback_ft.best_model_path\n","if best_model_path_ft:\n","    print(f\"Mejor modelo de fine-tuning guardado en: {best_model_path_ft}\")\n","\n","    # Cargar el mejor modelo\n","    best_model_ft = TimeSformerLightningModule.load_from_checkpoint(best_model_path_ft)\n","\n","    # Guardar el modelo completo para uso posterior\n","    fine_tuning_model_path = os.path.join(MODEL_SAVE_PATH, \"fine_tuning_model\")\n","    os.makedirs(fine_tuning_model_path, exist_ok=True)\n","    best_model_ft.model.save_pretrained(fine_tuning_model_path)\n","    processor.save_pretrained(fine_tuning_model_path)\n","\n","    print(f\"Modelo de fine-tuning guardado en: {fine_tuning_model_path}\")\n","else:\n","    print(\"No se encontró un mejor modelo de fine-tuning. Usando el último entrenado.\")\n","    fine_tuning_model_path = os.path.join(MODEL_SAVE_PATH, \"fine_tuning_model\")\n","    os.makedirs(fine_tuning_model_path, exist_ok=True)\n","    model_ft.model.save_pretrained(fine_tuning_model_path)\n","    processor.save_pretrained(fine_tuning_model_path)"],"metadata":{"id":"by1hfZGMuIJ_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 10: Evaluación Detallada del Modelo Fine-Tuned"],"metadata":{"id":"xBF0ymKEuMOq"}},{"cell_type":"code","source":["# Evaluar el mejor modelo de fine-tuning en el conjunto de prueba\n","print(\"Evaluando modelo de fine-tuning en el conjunto de prueba...\")\n","test_results_ft = trainer_ft.test(best_model_ft if best_model_path_ft else model_ft, datamodule=data_module)\n","\n","# Ejecutar evaluación detallada en el conjunto de prueba\n","loaded_model_ft = best_model_ft if best_model_path_ft else model_ft\n","test_metrics_ft, conf_mat_ft, roc_data_ft = evaluate_detailed(\n","    loaded_model_ft,\n","    data_module.test_dataloader(),\n","    data_module.classes\n",")\n","\n","# Comparar métricas entre transfer learning y fine-tuning\n","print(\"\\n=== Comparación de Modelos ===\")\n","metrics_comparison = {\n","    \"Métrica\": [\"Accuracy\", \"Precision\", \"Recall (TPR)\", \"Specificity\", \"F1-Score\", \"ROC AUC\"],\n","    \"Transfer Learning\": [\n","        f\"{test_metrics['accuracy']:.4f}\",\n","        f\"{test_metrics['precision']:.4f}\",\n","        f\"{test_metrics['recall']:.4f}\",\n","        f\"{test_metrics['specificity']:.4f}\",\n","        f\"{test_metrics['f1']:.4f}\",\n","        f\"{test_metrics['roc_auc']:.4f}\"\n","    ],\n","    \"Fine-Tuning\": [\n","        f\"{test_metrics_ft['accuracy']:.4f}\",\n","        f\"{test_metrics_ft['precision']:.4f}\",\n","        f\"{test_metrics_ft['recall']:.4f}\",\n","        f\"{test_metrics_ft['specificity']:.4f}\",\n","        f\"{test_metrics_ft['f1']:.4f}\",\n","        f\"{test_metrics_ft['roc_auc']:.4f}\"\n","    ]\n","}\n","\n","# Crear DataFrame y mostrar tabla de comparación\n","comparison_df = pd.DataFrame(metrics_comparison)\n","print(comparison_df.to_string(index=False))\n","\n","# Guardar tabla de comparación\n","comparison_df.to_csv(os.path.join(MODEL_SAVE_PATH, 'model_comparison.csv'), index=False)\n","\n","# Generar gráfico de comparación\n","plt.figure(figsize=(12, 8))\n","bar_width = 0.35\n","x = np.arange(len(metrics_comparison[\"Métrica\"]))\n","\n","# Convertir valores a float para gráfico\n","tl_values = [float(val) for val in metrics_comparison[\"Transfer Learning\"]]\n","ft_values = [float(val) for val in metrics_comparison[\"Fine-Tuning\"]]\n","\n","plt.bar(x - bar_width/2, tl_values, bar_width, label='Transfer Learning')\n","plt.bar(x + bar_width/2, ft_values, bar_width, label='Fine-Tuning')\n","\n","plt.xlabel('Métricas')\n","plt.ylabel('Valores')\n","plt.title('Comparación de Modelos: Transfer Learning vs Fine-Tuning')\n","plt.xticks(x, metrics_comparison[\"Métrica\"])\n","plt.legend()\n","plt.tight_layout()\n","plt.savefig(os.path.join(MODEL_SAVE_PATH, 'models_comparison.png'))\n","plt.close()\n","\n","# Comparar curvas ROC si es un problema binario\n","if len(data_module.classes) == 2:\n","    plt.figure(figsize=(10, 8))\n","\n","    # Extraer datos ROC\n","    fpr_tl, tpr_tl, roc_auc_tl = roc_data\n","    fpr_ft, tpr_ft, roc_auc_ft = roc_data_ft\n","\n","    plt.plot(fpr_tl, tpr_tl, color='blue', lw=2,\n","             label=f'Transfer Learning (AUC = {roc_auc_tl:.2f})')\n","    plt.plot(fpr_ft, tpr_ft, color='red', lw=2,\n","             label=f'Fine-Tuning (AUC = {roc_auc_ft:.2f})')\n","\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('Tasa de Falsos Positivos')\n","    plt.ylabel('Tasa de Verdaderos Positivos')\n","    plt.title('Comparación de Curvas ROC')\n","    plt.legend(loc=\"lower right\")\n","    plt.savefig(os.path.join(MODEL_SAVE_PATH, 'roc_curves_comparison.png'))\n","    plt.close()"],"metadata":{"id":"7QqvIglyuO5a"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 11: Exportación del Modelo Final para Despliegue"],"metadata":{"id":"s3LH1b1yuVdD"}},{"cell_type":"code","source":["# Seleccionar el mejor modelo para exportar (generalmente será el fine-tuned)\n","if test_metrics_ft['f1'] >= test_metrics['f1']:\n","    export_model_path = fine_tuning_model_path\n","    best_final_model = loaded_model_ft\n","    print(\"El modelo de fine-tuning tiene mejor rendimiento y será exportado como modelo final.\")\n","else:\n","    export_model_path = transfer_model_path\n","    best_final_model = loaded_model\n","    print(\"El modelo de transfer learning tiene mejor rendimiento y será exportado como modelo final.\")\n","\n","# Exportar el modelo para despliegue\n","final_export_path = os.path.join(EXPORT_PATH, \"final_model\")\n","os.makedirs(final_export_path, exist_ok=True)\n","\n","# Guardar el modelo en formato de Hugging Face Transformers\n","best_final_model.model.save_pretrained(final_export_path)\n","processor.save_pretrained(final_export_path)\n","\n","# Guardar configuración adicional para facilitar el despliegue\n","config = {\n","    \"model_type\": \"TimesformerForVideoClassification\",\n","    \"num_frames\": NUM_FRAMES,\n","    \"frame_sample_rate\": data_module.frame_sample_rate,\n","    \"classes\": data_module.classes,\n","    \"input_size\": [224, 224],\n","    \"created_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n","    \"metrics\": test_metrics_ft if test_metrics_ft['f1'] >= test_metrics['f1'] else test_metrics\n","}\n","\n","with open(os.path.join(final_export_path, \"config.json\"), \"w\") as f:\n","    json.dump(config, f, indent=4)\n","\n","print(f\"Modelo final exportado a: {final_export_path}\")\n","\n","# Exportar también en formato ONNX para inferencia rápida (opcional)\n","try:\n","    import onnx\n","    import onnxruntime\n","    from transformers.onnx import export\n","\n","    # Definir ruta para modelo ONNX\n","    onnx_path = os.path.join(EXPORT_PATH, \"onnx_model\")\n","    os.makedirs(onnx_path, exist_ok=True)\n","\n","    # Exportar a ONNX\n","    onnx_model_path = os.path.join(onnx_path, \"model.onnx\")\n","\n","    # Crear un input dummy\n","    dummy_inputs = {\n","        \"pixel_values\": torch.randn(1, NUM_FRAMES, 3, 224, 224).to(device)\n","    }\n","\n","    # Exportar modelo\n","    torch.onnx.export(\n","        best_final_model.model,\n","        (dummy_inputs[\"pixel_values\"],),\n","        onnx_model_path,\n","        export_params=True,\n","        opset_version=12,\n","        input_names=[\"pixel_values\"],\n","        output_names=[\"logits\"],\n","        dynamic_axes={\n","            \"pixel_values\": {0: \"batch_size\"},\n","            \"logits\": {0: \"batch_size\"}\n","        }\n","    )\n","\n","    # Verificar modelo ONNX\n","    onnx_model = onnx.load(onnx_model_path)\n","    onnx.checker.check_model(onnx_model)\n","\n","    # Guardar config con el modelo ONNX\n","    with open(os.path.join(onnx_path, \"config.json\"), \"w\") as f:\n","        config[\"onnx\"] = True\n","        json.dump(config, f, indent=4)\n","\n","    # Guardar el procesador\n","    processor.save_pretrained(onnx_path)\n","\n","    print(f\"Modelo ONNX exportado a: {onnx_path}\")\n","except Exception as e:\n","    print(f\"No se pudo exportar a ONNX: {e}\")\n","    print(\"Continuando sin exportación ONNX\")"],"metadata":{"id":"aN0_NN6juWi9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 12: Prueba del Modelo con Inferencia en Tiempo Real"],"metadata":{"id":"Jo0VywnUua1q"}},{"cell_type":"code","source":["# Función para procesar un video y hacer inferencia\n","def predict_violence(video_path, model, processor, num_frames=8, frame_sample_rate=2):\n","    # Cargar video\n","    container = av.open(video_path)\n","\n","    # Estimar total de frames\n","    total_frames = container.streams.video[0].frames\n","    if total_frames <= 0:\n","        # Estimar frames basado en la duración y fps\n","        fps = container.streams.video[0].average_rate\n","        total_duration = container.streams.video[0].duration\n","        if fps:\n","            total_frames = int(total_duration * float(fps) / 1e6)  # duration en microsegundos\n","\n","    if total_frames <= 0:\n","        # Fallback: asumir 30 fps y 5 segundos\n","        total_frames = 150\n","\n","    # Muestrear frames\n","    indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)\n","\n","    # Extraer frames\n","    frames = []\n","    container.seek(0)\n","    for i, frame in enumerate(container.decode(video=0)):\n","        if i in indices:\n","            img = frame.to_ndarray(format=\"rgb\")\n","            frames.append(img)\n","        if len(frames) == num_frames:\n","            break\n","\n","    # Si no hay suficientes frames, repetir el último\n","    if len(frames) < num_frames:\n","        last_frame = frames[-1] if frames else np.zeros((224, 224, 3), dtype=np.uint8)\n","        while len(frames) < num_frames:\n","            frames.append(last_frame)\n","\n","    # Preprocesar frames\n","    transform = transforms.Compose([\n","        transforms.ToPILImage(),\n","        transforms.Resize(256),\n","        transforms.CenterCrop(224),\n","        transforms.ToTensor(),\n","    ])\n","\n","    frames_processed = [transform(frame) for frame in frames]\n","\n","    # Procesar con el procesador de TimeSformer\n","    inputs = processor(frames_processed, return_tensors=\"pt\")\n","    pixel_values = inputs.pixel_values.to(device)\n","\n","    # Realizar inferencia\n","    model.eval()\n","    with torch.no_grad():\n","        outputs = model.model(pixel_values=pixel_values)\n","        logits = outputs.logits\n","        probs = F.softmax(logits, dim=1)\n","        predicted_class = torch.argmax(logits, dim=1).item()\n","\n","    return {\n","        \"predicted_class\": predicted_class,\n","        \"class_name\": data_module.classes[predicted_class],\n","        \"confidence\": probs[0][predicted_class].item(),\n","        \"probabilities\": {data_module.classes[i]: prob.item() for i, prob in enumerate(probs[0])}\n","    }\n","\n","# Probar el modelo con algunos videos del conjunto de prueba\n","test_videos = []\n","for class_name in data_module.classes:\n","    class_dir = os.path.join(TEST_PATH, class_name)\n","    videos = os.listdir(class_dir)[:3]  # Tomar 3 videos de cada clase\n","    test_videos.extend([(os.path.join(class_dir, video), class_name) for video in videos])\n","\n","# Mostrar resultados de las predicciones\n","print(\"\\n=== Prueba de Inferencia con Videos ===\")\n","print(f\"Probando {len(test_videos)} videos...\")\n","\n","results = []\n","for video_path, true_class in test_videos:\n","    prediction = predict_violence(video_path, best_final_model, processor, NUM_FRAMES)\n","\n","    # Mostrar resultado\n","    print(f\"\\nVideo: {os.path.basename(video_path)}\")\n","    print(f\"Clase real: {true_class}\")\n","    print(f\"Predicción: {prediction['class_name']} (confianza: {prediction['confidence']:.4f})\")\n","    print(f\"Probabilidades: {prediction['probabilities']}\")\n","\n","    results.append({\n","        \"video\": os.path.basename(video_path),\n","        \"true_class\": true_class,\n","        \"predicted_class\": prediction['class_name'],\n","        \"confidence\": prediction['confidence'],\n","        \"correct\": true_class == prediction['class_name']\n","    })\n","\n","# Calcular precisión en este conjunto de prueba pequeño\n","accuracy = sum(r[\"correct\"] for r in results) / len(results)\n","print(f\"\\nPrecisión en la prueba de inferencia: {accuracy:.4f}\")\n","\n","# Guardar resultados\n","with open(os.path.join(MODEL_SAVE_PATH, \"inference_test_results.json\"), \"w\") as f:\n","    json.dump(results, f, indent=4)"],"metadata":{"id":"Fo0njJ_2ubps"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 13: Visualización de la Activación del Modelo (Interpretabilidad)"],"metadata":{"id":"JmbgrxZVufbz"}},{"cell_type":"code","source":["# Implementar visualización de atención (Grad-CAM) para TimeSformer\n","def visualize_attention(video_path, model, processor, num_frames=8, class_idx=None):\n","    # Cargar y procesar video\n","    prediction = predict_violence(video_path, model, processor, num_frames)\n","    pred_class = prediction[\"predicted_class\"] if class_idx is None else class_idx\n","\n","    # Cargar frames originales para visualización\n","    container = av.open(video_path)\n","    indices = np.linspace(0, container.streams.video[0].frames - 1, num_frames, dtype=int)\n","\n","    frames = []\n","    container.seek(0)\n","    for i, frame in enumerate(container.decode(video=0)):\n","        if i in indices:\n","            img = frame.to_ndarray(format=\"rgb\")\n","            frames.append(img)\n","        if len(frames) == num_frames:\n","            break\n","\n","    # Extraer atención de la última capa\n","    # Nota: Esto es una simplificación, la visualización real de atención es más compleja\n","    # y específica para cada arquitectura\n","\n","    # Como alternativa, mostramos frames con probabilidades\n","    plt.figure(figsize=(20, 10))\n","    for i, frame in enumerate(frames):\n","        plt.subplot(2, 4, i+1)\n","        plt.imshow(frame)\n","        plt.title(f\"Frame {i}\")\n","        plt.axis('off')\n","\n","    plt.suptitle(f\"Predicción: {prediction['class_name']} (Confianza: {prediction['confidence']:.4f})\",\n","                 fontsize=16)\n","    plt.tight_layout()\n","\n","    # Guardar visualización\n","    viz_dir = os.path.join(MODEL_SAVE_PATH, \"visualizations\")\n","    os.makedirs(viz_dir, exist_ok=True)\n","    plt.savefig(os.path.join(viz_dir, f\"frames_{os.path.basename(video_path)}.png\"))\n","    plt.close()\n","\n","    return frames, prediction\n","\n","# Visualizar algunos ejemplos\n","for video_path, true_class in test_videos[:2]:  # Solo 2 videos para ejemplo\n","    visualize_attention(video_path, best_final_model, processor, NUM_FRAMES)\n","    print(f\"Visualización guardada para {os.path.basename(video_path)}\")"],"metadata":{"id":"oAneaFk7ugsy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 14: Resumen y Conclusiones"],"metadata":{"id":"sBJYvbXtuj1T"}},{"cell_type":"code","source":["# Generar resumen del experimento\n","summary = {\n","    \"experiment_date\": time.strftime(\"%Y-%m-%d %H:%M:%S\"),\n","    \"dataset\": {\n","        \"train_samples\": train_samples,\n","        \"val_samples\": val_samples,\n","        \"test_samples\": test_samples,\n","        \"classes\": data_module.classes\n","    },\n","    \"transfer_learning\": {\n","        \"base_model\": MODEL_CHECKPOINT,\n","        \"learning_rate\": LEARNING_RATE,\n","        \"batch_size\": BATCH_SIZE,\n","        \"num_frames\": NUM_FRAMES,\n","        \"epochs\": MAX_EPOCHS,\n","        \"metrics\": test_metrics\n","    },\n","    \"fine_tuning\": {\n","        \"base_model\": \"transfer_learning_model\",\n","        \"learning_rate\": LEARNING_RATE,\n","        \"batch_size\": BATCH_SIZE,\n","        \"num_frames\": NUM_FRAMES,\n","        \"epochs\": MAX_EPOCHS,\n","        \"metrics\": test_metrics_ft\n","    },\n","    \"final_model\": {\n","        \"path\": final_export_path,\n","        \"type\": \"fine_tuning\" if test_metrics_ft['f1'] >= test_metrics['f1'] else \"transfer_learning\",\n","        \"metrics\": test_metrics_ft if test_metrics_ft['f1'] >= test_metrics['f1'] else test_metrics\n","    }\n","}\n","\n","# Guardar resumen\n","with open(os.path.join(MODEL_SAVE_PATH, \"experiment_summary.json\"), \"w\") as f:\n","    json.dump(summary, f, indent=4)\n","\n","# Impresión de resumen\n","print(\"\\n===== RESUMEN DEL EXPERIMENTO =====\")\n","print(f\"Fecha: {summary['experiment_date']}\")\n","print(\"\\nDataset:\")\n","print(f\"- Muestras entrenamiento: {summary['dataset']['train_samples']}\")\n","print(f\"- Muestras validación: {summary['dataset']['val_samples']}\")\n","print(f\"- Muestras test: {summary['dataset']['test_samples']}\")\n","print(f\"- Clases: {summary['dataset']['classes']}\")\n","\n","print(\"\\nTransfer Learning:\")\n","print(f\"- Modelo base: {summary['transfer_learning']['base_model']}\")\n","print(f\"- Frames por clip: {summary['transfer_learning']['num_frames']}\")\n","print(f\"- Métricas principales:\")\n","print(f\"  - Accuracy: {summary['transfer_learning']['metrics']['accuracy']:.4f}\")\n","print(f\"  - F1-Score: {summary['transfer_learning']['metrics']['f1']:.4f}\")\n","print(f\"  - ROC AUC: {summary['transfer_learning']['metrics']['roc_auc']:.4f}\")\n","\n","print(\"\\nFine-Tuning:\")\n","print(f\"- Modelo base: {summary['fine_tuning']['base_model']}\")\n","print(f\"- Métricas principales:\")\n","print(f\"  - Accuracy: {summary['fine_tuning']['metrics']['accuracy']:.4f}\")\n","print(f\"  - F1-Score: {summary['fine_tuning']['metrics']['f1']:.4f}\")\n","print(f\"  - ROC AUC: {summary['fine_tuning']['metrics']['roc_auc']:.4f}\")\n","\n","print(\"\\nModelo Final:\")\n","print(f\"- Tipo: {summary['final_model']['type']}\")\n","print(f\"- Ruta: {summary['final_model']['path']}\")\n","print(f\"- Accuracy: {summary['final_model']['metrics']['accuracy']:.4f}\")\n","print(f\"- F1-Score: {summary['final_model']['metrics']['f1']:.4f}\")\n","print(f\"- ROC AUC: {summary['final_model']['metrics']['roc_auc']:.4f}\")\n","\n","print(\"\\n===== CONCLUSIONES =====\")\n","# Determinar modelo con mejor rendimiento\n","if test_metrics_ft['f1'] > test_metrics['f1']:\n","    improvement = (test_metrics_ft['f1'] - test_metrics['f1']) / test_metrics['f1'] * 100\n","    print(f\"El fine-tuning mejoró el rendimiento del modelo en un {improvement:.2f}% en términos de F1-Score.\")\n","    best_approach = \"fine-tuning\"\n","elif test_metrics_ft['f1'] < test_metrics['f1']:\n","    decline = (test_metrics['f1'] - test_metrics_ft['f1']) / test_metrics['f1'] * 100\n","    print(f\"El fine-tuning no mejoró el rendimiento y disminuyó un {decline:.2f}% en términos de F1-Score.\")\n","    print(\"Esto sugiere posible sobreajuste durante el fine-tuning.\")\n","    best_approach = \"transfer learning\"\n","else:\n","    print(\"No hubo diferencia significativa entre transfer learning y fine-tuning.\")\n","    best_approach = \"ambos enfoques\"\n","\n","# Análisis de métricas críticas para detección de violencia\n","if best_approach == \"fine-tuning\":\n","    recall = test_metrics_ft['recall']\n","    fpr = test_metrics_ft['fpr']\n","elif best_approach == \"transfer learning\":\n","    recall = test_metrics['recall']\n","    fpr = test_metrics['fpr']\n","else:\n","    recall = max(test_metrics['recall'], test_metrics_ft['recall'])\n","    fpr = min(test_metrics['fpr'], test_metrics_ft['fpr'])\n","\n","print(f\"\\nPara un sistema de detección de violencia escolar, es crucial:\")\n","print(f\"- Alta sensibilidad/recall: {recall:.4f} (porcentaje de casos de violencia detectados)\")\n","print(f\"- Baja tasa de falsos positivos: {fpr:.4f} (porcentaje de falsos alertas)\")\n","\n","# Recomendaciones finales\n","print(\"\\nRecomendaciones para despliegue:\")\n","print(\"1. Implementar el modelo exportado con sistema de umbral ajustable para balancear\")\n","print(\"   sensibilidad vs. falsos positivos según las necesidades específicas del entorno escolar.\")\n","print(\"2. Considerar procesamiento de video en ventanas deslizantes con solapamiento\")\n","print(\"   para garantizar detección continua en transmisiones de video en tiempo real.\")\n","print(\"3. Implementar mecanismo de retroalimentación que permita ajustar el modelo\")\n","print(\"   con casos específicos del entorno particular donde se despliega.\")\n","print(\"4. Complementar la detección visual con análisis de audio para mejorar precisión.\")"],"metadata":{"id":"oYp1m55Euk76"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 15: Código para Integración del Modelo en Sistema de Tiempo Real"],"metadata":{"id":"N_ErQhS8uvlQ"}},{"cell_type":"code","source":["# Crear una clase de inferencia que pueda ser utilizada en un sistema en tiempo real\n","class ViolenceDetector:\n","    def __init__(self, model_path, device=\"cuda\" if torch.cuda.is_available() else \"cpu\"):\n","        \"\"\"\n","        Inicializa el detector de violencia con un modelo entrenado\n","\n","        Args:\n","            model_path: Ruta al modelo exportado\n","            device: Dispositivo para inferencia ('cuda' o 'cpu')\n","        \"\"\"\n","        self.device = device\n","        self.config = None\n","\n","        # Cargar configuración\n","        config_path = os.path.join(model_path, \"config.json\")\n","        if os.path.exists(config_path):\n","            with open(config_path, \"r\") as f:\n","                self.config = json.load(f)\n","\n","        # Establecer parámetros\n","        self.num_frames = self.config.get(\"num_frames\", 8) if self.config else 8\n","        self.frame_sample_rate = self.config.get(\"frame_sample_rate\", 2) if self.config else 2\n","        self.classes = self.config.get(\"classes\", [\"no_violence\", \"violence\"]) if self.config else [\"no_violence\", \"violence\"]\n","        self.threshold = 0.6  # Umbral de confianza para detectar violencia\n","\n","        # Cargar modelo y procesador\n","        from transformers import TimesformerForVideoClassification, AutoImageProcessor\n","\n","        print(f\"Cargando modelo desde {model_path}...\")\n","        self.model = TimesformerForVideoClassification.from_pretrained(model_path)\n","        self.model.to(device)\n","        self.model.eval()\n","\n","        self.processor = AutoImageProcessor.from_pretrained(model_path)\n","\n","        # Preparar transformaciones\n","        self.transform = transforms.Compose([\n","            transforms.ToPILImage(),\n","            transforms.Resize(256),\n","            transforms.CenterCrop(224),\n","            transforms.ToTensor(),\n","        ])\n","\n","        print(\"Detector de violencia inicializado correctamente.\")\n","\n","    def preprocess_frames(self, frames):\n","        \"\"\"\n","        Preprocesa una lista de frames para la inferencia\n","\n","        Args:\n","            frames: Lista de arrays numpy RGB\n","\n","        Returns:\n","            Tensor procesado listo para el modelo\n","        \"\"\"\n","        # Asegurar que tenemos el número correcto de frames\n","        if len(frames) < self.num_frames:\n","            # Repetir último frame si no hay suficientes\n","            last_frame = frames[-1] if frames else np.zeros((224, 224, 3), dtype=np.uint8)\n","            frames = frames + [last_frame] * (self.num_frames - len(frames))\n","        elif len(frames) > self.num_frames:\n","            # Muestrear frames uniformemente\n","            indices = np.linspace(0, len(frames) - 1, self.num_frames, dtype=int)\n","            frames = [frames[i] for i in indices]\n","\n","        # Aplicar transformaciones\n","        frames_processed = [self.transform(frame) for frame in frames]\n","\n","        # Procesar con el procesador de TimeSformer\n","        inputs = self.processor(frames_processed, return_tensors=\"pt\")\n","        return inputs.pixel_values.to(self.device)\n","\n","    def predict(self, frames):\n","        \"\"\"\n","        Realiza la predicción sobre un conjunto de frames\n","\n","        Args:\n","            frames: Lista de arrays numpy RGB\n","\n","        Returns:\n","            dict: Resultado de la predicción con etiqueta, confianza y flag de violencia\n","        \"\"\"\n","        # Preprocesar frames\n","        pixel_values = self.preprocess_frames(frames)\n","\n","        # Realizar inferencia\n","        with torch.no_grad():\n","            outputs = self.model(pixel_values=pixel_values)\n","            logits = outputs.logits\n","            probs = F.softmax(logits, dim=1)\n","            predicted_class = torch.argmax(logits, dim=1).item()\n","\n","        # Obtener índice de la clase 'violence' (normalmente 1 en un sistema binario)\n","        violence_idx = self.classes.index(\"violence\") if \"violence\" in self.classes else 1\n","        violence_prob = probs[0][violence_idx].item()\n","\n","        # Determinar si se detecta violencia según umbral\n","        is_violence = predicted_class == violence_idx and violence_prob >= self.threshold\n","\n","        return {\n","            \"is_violence\": is_violence,\n","            \"class_name\": self.classes[predicted_class],\n","            \"confidence\": probs[0][predicted_class].item(),\n","            \"violence_confidence\": violence_prob,\n","            \"probabilities\": {self.classes[i]: prob.item() for i, prob in enumerate(probs[0])}\n","        }\n","\n","    def process_video_file(self, video_path):\n","        \"\"\"\n","        Procesa un archivo de video completo\n","\n","        Args:\n","            video_path: Ruta al archivo de video\n","\n","        Returns:\n","            dict: Resultado de la predicción\n","        \"\"\"\n","        # Cargar video\n","        container = av.open(video_path)\n","\n","        # Estimar total de frames\n","        total_frames = container.streams.video[0].frames\n","        if total_frames <= 0:\n","            # Estimar frames basado en duración y fps\n","            fps = container.streams.video[0].average_rate\n","            total_duration = container.streams.video[0].duration\n","            if fps:\n","                total_frames = int(total_duration * float(fps) / 1e6)  # duration en microsegundos\n","\n","        if total_frames <= 0:\n","            # Fallback: asumir 30 fps y 5 segundos\n","            total_frames = 150\n","\n","        # Muestrear frames\n","        indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n","\n","        # Extraer frames\n","        frames = []\n","        container.seek(0)\n","        for i, frame in enumerate(container.decode(video=0)):\n","            if i in indices:\n","                img = frame.to_ndarray(format=\"rgb\")\n","                frames.append(img)\n","            if len(frames) == self.num_frames:\n","                break\n","\n","        # Realizar predicción\n","        return self.predict(frames)\n","\n","    def process_video_stream(self, video_capture, callback=None, window_size=30, stride=15):\n","        \"\"\"\n","        Procesa un stream de video en tiempo real con ventanas deslizantes\n","\n","        Args:\n","            video_capture: Objeto de captura de OpenCV\n","            callback: Función a llamar cuando se detecta violencia\n","            window_size: Tamaño de la ventana en frames\n","            stride: Avance de la ventana en frames\n","        \"\"\"\n","        frame_buffer = []\n","        frame_count = 0\n","\n","        print(\"Iniciando procesamiento de stream de video...\")\n","        print(f\"Presiona 'q' para salir\")\n","\n","        try:\n","            while video_capture.isOpened():\n","                ret, frame = video_capture.read()\n","                if not ret:\n","                    break\n","\n","                # Convertir BGR a RGB\n","                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","                frame_buffer.append(frame_rgb)\n","\n","                # Mantener buffer con tamaño máximo de window_size\n","                if len(frame_buffer) > window_size:\n","                    frame_buffer.pop(0)\n","\n","                # Procesar cuando tenemos suficientes frames y estamos en stride\n","                if len(frame_buffer) == window_size and frame_count % stride == 0:\n","                    result = self.predict(frame_buffer)\n","\n","                    # Mostrar resultado en la imagen\n","                    label = f\"{result['class_name']}: {result['confidence']:.2f}\"\n","                    color = (0, 0, 255) if result['is_violence'] else (0, 255, 0)\n","                    cv2.putText(frame, label, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n","\n","                    # Llamar callback si hay violencia\n","                    if result['is_violence'] and callback:\n","                        callback(result, frame_rgb)\n","\n","                # Mostrar frame\n","                cv2.imshow('Video', frame)\n","                if cv2.waitKey(1) & 0xFF == ord('q'):\n","                    break\n","\n","                frame_count += 1\n","\n","        finally:\n","            video_capture.release()\n","            cv2.destroyAllWindows()\n","\n","# Ejemplo de cómo usar el detector para integración en un sistema real\n","print(\"\\n===== CÓDIGO DE INTEGRACIÓN PARA SISTEMA REAL =====\")\n","print(\"Este código puede ser integrado en una aplicación para detección en tiempo real:\")\n","print(\"\"\"\n","# Ejemplo de uso:\n","detector = ViolenceDetector(\"ruta/al/modelo/exportado\")\n","\n","# Para procesar un video grabado:\n","result = detector.process_video_file(\"ruta/al/video.mp4\")\n","print(f\"Violencia detectada: {result['is_violence']}\")\n","print(f\"Confianza: {result['confidence']:.4f}\")\n","\n","# Para procesar un stream en tiempo real:\n","def alert_callback(result, frame):\n","    print(f\"¡ALERTA! Violencia detectada con confianza {result['confidence']:.4f}\")\n","    # Aquí se pueden agregar acciones como:\n","    # - Enviar notificación\n","    # - Guardar el clip de video\n","    # - Activar alarma\n","\n","# Conectar a cámara (0 para webcam, o URL para cámara IP)\n","cap = cv2.VideoCapture(0)\n","detector.process_video_stream(cap, callback=alert_callback)\n","\"\"\")"],"metadata":{"id":"kzgmCiPkuxUc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 16: Evaluación con Número de Frames Alternativo"],"metadata":{"id":"3hdnz9FDu1wJ"}},{"cell_type":"code","source":["# Probar con un número diferente de frames para comparar rendimiento\n","ALTERNATIVE_NUM_FRAMES = 16  # Probar con más frames\n","\n","print(\"\\n===== EVALUANDO CON NÚMERO ALTERNATIVO DE FRAMES =====\")\n","print(f\"Configurando entrenamiento con {ALTERNATIVE_NUM_FRAMES} frames...\")\n","\n","# Crear datasets con número alternativo de frames\n","alt_data_module = VideoDataModule(\n","    train_path=TRAIN_PATH,\n","    val_path=VAL_PATH,\n","    test_path=TEST_PATH,\n","    processor=processor,\n","    num_frames=ALTERNATIVE_NUM_FRAMES,\n","    batch_size=2,  # Reducir por mayor consumo de memoria\n","    num_workers=NUM_WORKERS\n",")\n","alt_data_module.setup()\n","\n","# Configurar entrenamiento más corto para prueba de concepto\n","alt_experiment_name = f\"violence_detection_frames_{ALTERNATIVE_NUM_FRAMES}\"\n","ALT_MAX_EPOCHS = 5\n","\n","# Crear modelo con número alternativo de frames\n","alt_model = TimeSformerLightningModule(\n","    num_classes=len(alt_data_module.classes),\n","    num_frames=ALTERNATIVE_NUM_FRAMES,\n","    learning_rate=LEARNING_RATE,\n","    weight_decay=WEIGHT_DECAY,\n","    warmup_steps=warmup_steps,\n","    total_steps=total_steps,\n","    freeze_backbone=True,\n","    unfreeze_layers=1\n",")\n","\n","# Configurar callbacks\n","alt_checkpoint_callback = ModelCheckpoint(\n","    dirpath=CHECKPOINT_PATH,\n","    filename=f\"timesformer-violence-frames{ALTERNATIVE_NUM_FRAMES}-\"+\"{epoch:02d}-{val_f1:.4f}\",\n","    monitor=\"val_f1\",\n","    mode=\"max\",\n","    save_top_k=1,\n","    save_last=True,\n","    verbose=True\n",")\n","\n","alt_early_stopping = EarlyStopping(\n","    monitor=\"val_f1\",\n","    patience=3,\n","    mode=\"max\",\n","    verbose=True\n",")\n","\n","# Configurar logger\n","alt_tb_logger = TensorBoardLogger(\"lightning_logs\", name=alt_experiment_name)\n","\n","# Iniciar entrenamiento\n","alt_trainer = pl.Trainer(\n","    max_epochs=ALT_MAX_EPOCHS,\n","    callbacks=[alt_checkpoint_callback, alt_early_stopping, lr_monitor],\n","    logger=alt_tb_logger,\n","    log_every_n_steps=10,\n","    accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n","    precision=\"16-mixed\" if torch.cuda.is_available() else \"32\",\n",")\n","\n","# Entrenar modelo con frames alternativos\n","print(f\"Iniciando entrenamiento con {ALTERNATIVE_NUM_FRAMES} frames...\")\n","alt_trainer.fit(alt_model, datamodule=alt_data_module)\n","\n","# Evaluar en conjunto de prueba\n","alt_test_results = alt_trainer.test(alt_model, datamodule=alt_data_module)\n","\n","# Comparar con resultados anteriores\n","print(\"\\n=== Comparación por Número de Frames ===\")\n","print(f\"Frames: {NUM_FRAMES} vs {ALTERNATIVE_NUM_FRAMES}\")\n","\n","# Código simplificado para evaluación rápida\n","alt_loaded_model = alt_model\n","with torch.no_grad():\n","    all_preds = []\n","    all_labels = []\n","\n","    for batch in alt_data_module.test_dataloader():\n","        pixel_values = batch[\"pixel_values\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","\n","        outputs = alt_loaded_model.model(pixel_values=pixel_values)\n","        logits = outputs.logits\n","        preds = torch.argmax(logits, dim=1)\n","\n","        all_preds.extend(preds.cpu().numpy())\n","        all_labels.extend(labels.cpu().numpy())\n","\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","\n","    alt_accuracy = (all_preds == all_labels).mean()\n","    alt_f1 = f1_score(all_labels, all_preds, average='macro')\n","\n","# Comparar métricas\n","frames_comparison = {\n","    \"Métrica\": [\"Accuracy\", \"F1-Score\"],\n","    f\"{NUM_FRAMES} frames\": [\n","        f\"{test_metrics_ft['accuracy']:.4f}\",\n","        f\"{test_metrics_ft['f1']:.4f}\"\n","    ],\n","    f\"{ALTERNATIVE_NUM_FRAMES} frames\": [\n","        f\"{alt_accuracy:.4f}\",\n","        f\"{alt_f1:.4f}\"\n","    ]\n","}\n","\n","# Mostrar comparación\n","print(\"\\nComparación de rendimiento por número de frames:\")\n","frames_comparison_df = pd.DataFrame(frames_comparison)\n","print(frames_comparison_df.to_string(index=False))\n","\n","# Añadir conclusión\n","print(\"\\nConclusión sobre número de frames:\")\n","if alt_f1 > test_metrics_ft['f1']:\n","    print(f\"Utilizar {ALTERNATIVE_NUM_FRAMES} frames mejora el rendimiento del modelo.\")\n","    print(\"Considerar este parámetro para entrenamientos futuros completos.\")\n","else:\n","    print(f\"Utilizar {NUM_FRAMES} frames proporciona mejor balance entre rendimiento y eficiencia.\")"],"metadata":{"id":"8qxh4eDvu2tn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Sección 17: Guardado de Documentación Final y Guía de Uso"],"metadata":{"id":"WnfWe5PMu71B"}},{"cell_type":"code","source":["# Generar documentación final del modelo\n","documentation = f\"\"\"\n","# Modelo de Detección de Violencia Escolar\n","\n","Este documento describe el modelo de detección de violencia escolar entrenado con TimeSformer.\n","\n","## Información General\n","\n","- **Fecha de entrenamiento**: {time.strftime(\"%Y-%m-%d\")}\n","- **Modelo base**: TimeSformer\n","- **Arquitectura**: {MODEL_CHECKPOINT}\n","- **Número de frames**: {NUM_FRAMES}\n","- **Resolución**: 224x224\n","- **Clases**: {data_module.classes}\n","- **Autor**: Franz Reinaldo Gonzales Suyo\n","\n","## Métricas de Rendimiento\n","\n","- **Accuracy**: {test_metrics_ft['accuracy']:.4f}\n","- **Precision**: {test_metrics_ft['precision']:.4f}\n","- **Recall (Sensibilidad)**: {test_metrics_ft['recall']:.4f}\n","- **Specificity**: {test_metrics_ft['specificity']:.4f}\n","- **F1-Score**: {test_metrics_ft['f1']:.4f}\n","- **ROC AUC**: {test_metrics_ft['roc_auc']:.4f}\n","\n","## Uso del Modelo\n","\n","1. **Carga del modelo**:\n","```python\n","from transformers import TimesformerForVideoClassification, AutoImageProcessor\n","\n","model_path = \"ruta/al/modelo/exportado\"\n","model = TimesformerForVideoClassification.from_pretrained(model_path)\n","processor = AutoImageProcessor.from_pretrained(model_path)"],"metadata":{"id":"ReDASKWhu8qB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Procesamiento de video:"],"metadata":{"id":"jKlaD1XxvA9Z"}},{"cell_type":"code","source":["import av\n","import torch\n","import numpy as np\n","from torchvision import transforms\n","\n","# Cargar video\n","container = av.open(\"video.mp4\")\n","\n","# Extraer frames\n","num_frames = {NUM_FRAMES}\n","indices = np.linspace(0, container.streams.video[0].frames - 1, num_frames, dtype=int)\n","frames = []\n","for i, frame in enumerate(container.decode(video=0)):\n","    if i in indices:\n","        img = frame.to_ndarray(format=\"rgb\")\n","        frames.append(img)\n","    if len(frames) == num_frames:\n","        break\n","\n","# Preprocesar frames\n","transform = transforms.Compose([\n","    transforms.ToPILImage(),\n","    transforms.Resize(256),\n","    transforms.CenterCrop(224),\n","    transforms.ToTensor(),\n","])\n","frames_processed = [transform(frame) for frame in frames]\n","\n","# Procesar con el procesador de TimeSformer\n","inputs = processor(frames_processed, return_tensors=\"pt\")"],"metadata":{"id":"bRWlnpxivBrK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Inferencia:"],"metadata":{"id":"OV94zW4HvEtS"}},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","with torch.no_grad():\n","    outputs = model(pixel_values=inputs.pixel_values)\n","    logits = outputs.logits\n","    probs = F.softmax(logits, dim=1)\n","    predicted_class = torch.argmax(logits, dim=1).item()\n","\n","classes = {data_module.classes}\n","print(f\"Clase predicha: {classes[predicted_class]}\")\n","print(f\"Confianza: {probs[0][predicted_class].item():.4f}\")"],"metadata":{"id":"8zG43fpnvHaj"},"execution_count":null,"outputs":[]}]}