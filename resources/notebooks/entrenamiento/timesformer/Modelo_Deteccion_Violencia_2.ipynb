{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNm9lxlDa9fN3xUrfMCwnTI"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"7924adcf74bf4a42a7f0ab27e6d6ef04":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_697dfc0588c44317a338cbaa788ed59f","IPY_MODEL_fe1a2c253809498193f32f3a18d768ad","IPY_MODEL_43b8b830de504b0883aa9b70daa950c4"],"layout":"IPY_MODEL_f382f011b94141d79bbdb997c7b68324"}},"697dfc0588c44317a338cbaa788ed59f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ba32710f95c4391a9dc7f73dbd3f5aa","placeholder":"​","style":"IPY_MODEL_df84d3f3c2ba48b8ab1e80c37a3ca01b","value":"preprocessor_config.json: 100%"}},"fe1a2c253809498193f32f3a18d768ad":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f8287a768e04513b79e8a8b6b452c9c","max":412,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3f37708ab1164167883c64e09557874c","value":412}},"43b8b830de504b0883aa9b70daa950c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f9c28cef3376473794b7ccfc5bb0a613","placeholder":"​","style":"IPY_MODEL_a7cb279a936946109aea5bb6b9a7a235","value":" 412/412 [00:00&lt;00:00, 31.3kB/s]"}},"f382f011b94141d79bbdb997c7b68324":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4ba32710f95c4391a9dc7f73dbd3f5aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"df84d3f3c2ba48b8ab1e80c37a3ca01b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f8287a768e04513b79e8a8b6b452c9c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3f37708ab1164167883c64e09557874c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f9c28cef3376473794b7ccfc5bb0a613":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a7cb279a936946109aea5bb6b9a7a235":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["# Instalación de dependencias\n","!pip install transformers\n","!pip install datasets\n","!pip install decord\n","!pip install scikit-learn\n","!pip install matplotlib\n","!pip install seaborn\n","!pip install pandas\n","!pip install tqdm\n","!pip install scipy\n","!pip install torchmetrics\n","!pip install timm\n","!pip install av\n","!pip install einops\n","!pip install evaluate"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KlMSrGEwj-Wh","executionInfo":{"status":"ok","timestamp":1747624094171,"user_tz":240,"elapsed":150498,"user":{"displayName":"Franz Reinaldo Gonzales","userId":"07029606055805766342"}},"outputId":"0c029657-cd2d-4524-a387-bd498228e32f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Collecting decord\n","  Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl.metadata (422 bytes)\n","Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from decord) (2.0.2)\n","Downloading decord-0.6.0-py3-none-manylinux2010_x86_64.whl (13.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.6/13.6 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: decord\n","Successfully installed decord-0.6.0\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (2.0.2)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n","Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.0.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n","Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n","Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.0.2)\n","Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n","Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.11/dist-packages (from seaborn) (3.10.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.2.1)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n","Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (1.15.3)\n","Requirement already satisfied: numpy<2.5,>=1.23.5 in /usr/local/lib/python3.11/dist-packages (from scipy) (2.0.2)\n","Collecting torchmetrics\n","  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n","Requirement already satisfied: numpy>1.20.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.0.2)\n","Requirement already satisfied: packaging>17.1 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (24.2)\n","Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from torchmetrics) (2.6.0+cu124)\n","Collecting lightning-utilities>=0.8.0 (from torchmetrics)\n","  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (75.2.0)\n","Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.8.0->torchmetrics) (4.13.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.18.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->torchmetrics)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->torchmetrics)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->torchmetrics)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->torchmetrics)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->torchmetrics)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->torchmetrics)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->torchmetrics)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->torchmetrics) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->torchmetrics) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->torchmetrics) (3.0.2)\n","Downloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m51.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m30.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 torchmetrics-1.7.1\n","Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.31.2)\n","Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (24.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->timm) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n","Collecting av\n","  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n","Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: av\n","Successfully installed av-14.4.0\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (0.8.1)\n","Collecting evaluate\n","  Downloading evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.14.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.0.2)\n","Requirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.2)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.2)\n","Requirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (24.2)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (18.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.11.15)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (3.18.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.4.3)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.20.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n","Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.0/84.0 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: evaluate\n","Successfully installed evaluate-0.4.3\n"]}]},{"cell_type":"code","source":["# Importar bibliotecas necesarias\n","import os\n","import random\n","import math\n","import time\n","import glob\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","from tqdm import tqdm\n","from datetime import datetime\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torch.utils.data import Dataset, DataLoader, random_split\n","from torch.optim.lr_scheduler import OneCycleLR, CosineAnnealingLR\n","from torchvision import transforms\n","from torchmetrics.classification import BinaryAccuracy, BinaryPrecision, BinaryRecall, BinaryF1Score, BinarySpecificity\n","from sklearn.metrics import confusion_matrix, roc_curve, auc, precision_recall_curve, average_precision_score\n","from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n","from transformers import TimesformerForVideoClassification, TimesformerConfig, AutoImageProcessor\n","from transformers import get_cosine_schedule_with_warmup\n","import decord\n","from decord import VideoReader, cpu\n","import av\n","import gc\n","import warnings\n","import random\n","import io\n","import zipfile\n","import logging\n","import json\n","from pathlib import Path\n"],"metadata":{"id":"Afo36OBfkCKI","executionInfo":{"status":"ok","timestamp":1747624201601,"user_tz":240,"elapsed":22761,"user":{"displayName":"Franz Reinaldo Gonzales","userId":"07029606055805766342"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# Montar Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d3iG0bNlkIfx","executionInfo":{"status":"ok","timestamp":1747624244840,"user_tz":240,"elapsed":22002,"user":{"displayName":"Franz Reinaldo Gonzales","userId":"07029606055805766342"}},"outputId":"3e20560b-975b-4ca9-dc00-14c2d71e97f1"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Configurar advertencias\n","warnings.filterwarnings('ignore')\n","\n","# Configurar logging\n","logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n","logger = logging.getLogger(__name__)\n","\n","\n","# Verificar disponibilidad de GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Usando dispositivo: {device}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_MnaOy73kHBy","executionInfo":{"status":"ok","timestamp":1747624289230,"user_tz":240,"elapsed":59,"user":{"displayName":"Franz Reinaldo Gonzales","userId":"07029606055805766342"}},"outputId":"929f4f33-bf23-457d-c32c-dcf1769efd5f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Usando dispositivo: cuda\n"]}]},{"cell_type":"code","source":["# ============================== CONFIGURACIÓN DE HIPERPARÁMETROS ==============================\n","\n","# Hiperparámetros generales\n","CONFIG = {\n","    # Rutas y nombres\n","    \"dataset_path\": \"/content/drive/MyDrive/dataset_violencia\",  # Ajustar según la ubicación real\n","    \"output_dir\": \"/content/drive/MyDrive/Proyecto-Deteccion-Violencia/modelo_timesformer\",\n","    \"model_name\": \"timesformer_violence_detector\",\n","\n","    # Parámetros del modelo\n","    \"pretrained_model\": \"facebook/timesformer-base-finetuned-k400\",\n","    \"num_frames\": 8,              # Número de frames a procesar\n","    \"image_size\": 224,             # Tamaño de los frames (224x224)\n","    \"num_classes\": 2,              # Violencia / No violencia\n","\n","    # Parámetros de entrenamiento - Transfer Learning\n","    \"tl_batch_size\": 8,            # Tamaño del batch\n","    \"tl_num_epochs\": 15,           # Número de épocas\n","    \"tl_learning_rate\": 5e-5,      # Learning rate inicial\n","    \"tl_weight_decay\": 1e-4,       # Regularización L2\n","    \"tl_dropout\": 0.2,             # Tasa de dropout\n","    \"tl_warmup_ratio\": 0.1,        # Proporción de steps para warmup\n","\n","    # Parámetros de entrenamiento - Fine-Tuning\n","    \"ft_batch_size\": 4,            # Tamaño del batch (más pequeño para fine-tuning)\n","    \"ft_num_epochs\": 5,            # Número de épocas adicionales\n","    \"ft_learning_rate\": 1e-5,      # Learning rate más bajo para fine-tuning\n","    \"ft_weight_decay\": 5e-5,       # Regularización L2 suave\n","\n","    # Umbral de clasificación\n","    \"threshold\": 0.6,              # Umbral de decisión para la clasificación\n","\n","    # Configuración de checkpoints\n","    \"save_steps\": 100,             # Guardar cada X pasos\n","    \"save_total_limit\": 3,         # Máximo número de checkpoints a mantener\n","    \"save_best_only\": True,        # Guardar solo el mejor modelo\n","\n","    # Métricas y evaluación\n","    \"eval_steps\": 50,              # Evaluar cada X pasos\n","    \"logging_steps\": 10,           # Mostrar métricas cada X pasos\n","\n","    # Otros parámetros\n","    \"seed\": 42,                    # Semilla para reproducibilidad\n","    \"mixed_precision\": True,       # Usar precisión mixta para acelerar entrenamiento\n","}\n","\n","# Crear directorio de salida si no existe\n","os.makedirs(CONFIG[\"output_dir\"], exist_ok=True)\n","\n","# Guardar configuración\n","with open(os.path.join(CONFIG[\"output_dir\"], \"config.json\"), 'w') as f:\n","    json.dump(CONFIG, f, indent=4)\n","\n","# Configurar reproducibilidad\n","def set_seed(seed=42):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(CONFIG[\"seed\"])"],"metadata":{"id":"0Z7ad0SlnuLJ","executionInfo":{"status":"ok","timestamp":1747624296520,"user_tz":240,"elapsed":1170,"user":{"displayName":"Franz Reinaldo Gonzales","userId":"07029606055805766342"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# ============================== CLASES PARA EL DATASET Y PROCESAMIENTO ==============================\n","\n","# Clase para procesar y cargar los videos\n","class ViolenceVideoDataset(Dataset):\n","    def __init__(self, root_dir, split='train', transform=None, num_frames=16, image_size=224, max_videos=None):\n","        \"\"\"\n","        Dataset para clasificación de violencia en videos\n","\n","        Args:\n","            root_dir: Directorio raíz del dataset\n","            split: 'train', 'val' o 'test'\n","            transform: Transformaciones a aplicar\n","            num_frames: Número de frames a extraer de cada video\n","            image_size: Tamaño de los frames\n","            max_videos: Limitar número de videos (para pruebas rápidas)\n","        \"\"\"\n","        self.root_dir = root_dir\n","        self.split = split\n","        self.transform = transform\n","        self.num_frames = num_frames\n","        self.image_size = image_size\n","\n","        self.processor = AutoImageProcessor.from_pretrained(CONFIG[\"pretrained_model\"])\n","\n","        # Obtener las rutas de videos y etiquetas\n","        violence_dir = os.path.join(root_dir, split, 'violence')\n","        no_violence_dir = os.path.join(root_dir, split, 'no_violence')\n","\n","        # Verificar que los directorios existan\n","        if not os.path.exists(violence_dir) or not os.path.exists(no_violence_dir):\n","            raise ValueError(f\"No se encontraron los directorios del dataset en {root_dir}/{split}\")\n","\n","        violence_videos = glob.glob(os.path.join(violence_dir, '*.mp4'))\n","        no_violence_videos = glob.glob(os.path.join(no_violence_dir, '*.mp4'))\n","\n","        if len(violence_videos) == 0 or len(no_violence_videos) == 0:\n","            raise ValueError(f\"No se encontraron videos en {violence_dir} o {no_violence_dir}\")\n","\n","        # Limitar videos si es necesario\n","        if max_videos is not None:\n","            max_per_class = max_videos // 2\n","            violence_videos = violence_videos[:max_per_class]\n","            no_violence_videos = no_violence_videos[:max_per_class]\n","\n","        self.video_paths = violence_videos + no_violence_videos\n","        self.labels = [1] * len(violence_videos) + [0] * len(no_violence_videos)\n","\n","        # Mezclar los datos manteniendo correspondencia entre paths y labels\n","        combined = list(zip(self.video_paths, self.labels))\n","        random.shuffle(combined)\n","        self.video_paths, self.labels = zip(*combined)\n","\n","        # Convertir a lista\n","        self.video_paths = list(self.video_paths)\n","        self.labels = list(self.labels)\n","\n","        print(f\"Cargados {len(self.video_paths)} videos para split '{split}'\")\n","        print(f\"Violencia: {len(violence_videos)}, No Violencia: {len(no_violence_videos)}\")\n","\n","    def __len__(self):\n","        return len(self.video_paths)\n","\n","    def sample_frames_from_video(self, video_path):\n","        \"\"\"Extrae frames uniformemente espaciados del video\"\"\"\n","        try:\n","            # Usar decord para cargar el video eficientemente\n","            video_reader = VideoReader(video_path, ctx=cpu(0))\n","            total_frames = len(video_reader)\n","\n","            if total_frames == 0:\n","                raise ValueError(f\"Video vacío o corrupto: {video_path}\")\n","\n","            # Seleccionar frames uniformemente\n","            indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)\n","            frames = video_reader.get_batch(indices).asnumpy()  # (num_frames, H, W, C)\n","\n","            # Aplicar resize y normalización\n","            processed_frames = []\n","            for frame in frames:\n","                # Redimensionar\n","                frame = transforms.functional.resize(\n","                    transforms.functional.to_tensor(frame),\n","                    (self.image_size, self.image_size)\n","                )\n","                processed_frames.append(frame)\n","\n","            # Apilar frames\n","            frames_tensor = torch.stack(processed_frames)  # (T, C, H, W)\n","\n","            # Mover dimensiones para coincidir con lo que espera el modelo (B, C, T, H, W)\n","            frames_tensor = frames_tensor.permute(1, 0, 2, 3).unsqueeze(0)\n","\n","            return frames_tensor\n","\n","        except Exception as e:\n","            logger.error(f\"Error al procesar video {video_path}: {str(e)}\")\n","            # Retornar un tensor de ceros en caso de error\n","            return torch.zeros((1, 3, self.num_frames, self.image_size, self.image_size))\n","\n","    def __getitem__(self, idx):\n","        \"\"\"Obtiene un item por su índice\"\"\"\n","        video_path = self.video_paths[idx]\n","        label = self.labels[idx]\n","\n","        # Extraer frames\n","        frames = self.sample_frames_from_video(video_path)\n","\n","        # Preprocesar frames usando el procesador de TimeSformer\n","        try:\n","            frames_list = list(frames.squeeze(0).permute(1, 0, 2, 3))  # Convertir a lista de tensores (T, C, H, W)\n","            inputs = self.processor(frames_list, return_tensors=\"pt\", do_rescale=False)\n","            pixel_values = inputs['pixel_values'].squeeze(0)  # Eliminar dim de batch\n","        except Exception as e:\n","            logger.error(f\"Error al procesar frames del video {video_path}: {str(e)}\")\n","            # Crear input vacío de tamaño correcto en caso de error\n","            pixel_values = torch.zeros((3, self.num_frames, self.image_size, self.image_size))\n","\n","        return {\n","            'pixel_values': pixel_values,\n","            'labels': torch.tensor(label, dtype=torch.long),\n","            'video_path': video_path\n","        }\n","\n","# ============================== FUNCIONES DE ENTRENAMIENTO Y EVALUACIÓN ==============================\n","\n","def train_epoch(model, dataloader, optimizer, scheduler, criterion, device, epoch, config):\n","    \"\"\"Entrena el modelo durante una época completa\"\"\"\n","    model.train()\n","    epoch_loss = 0\n","    epoch_acc = 0\n","    epoch_precision = 0\n","    epoch_recall = 0\n","    epoch_f1 = 0\n","\n","    # Métricas\n","    accuracy_metric = BinaryAccuracy().to(device)\n","    precision_metric = BinaryPrecision().to(device)\n","    recall_metric = BinaryRecall().to(device)\n","    f1_metric = BinaryF1Score().to(device)\n","\n","    progress_bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f\"Época {epoch+1}\")\n","\n","    for step, batch in progress_bar:\n","        try:\n","            # Mover datos al dispositivo\n","            pixel_values = batch['pixel_values'].to(device)\n","            labels = batch['labels'].to(device)\n","\n","            # Forward pass\n","            outputs = model(pixel_values=pixel_values, labels=labels)\n","            loss = outputs.loss\n","\n","            # Backward pass\n","            optimizer.zero_grad()\n","            loss.backward()\n","\n","            # Clip gradient norm para estabilidad\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            optimizer.step()\n","            if scheduler is not None:\n","                scheduler.step()\n","\n","            # Calcular métricas\n","            logits = outputs.logits\n","            preds = torch.sigmoid(logits[:, 1])  # Solo necesitamos la probabilidad de 'violencia'\n","\n","            accuracy = accuracy_metric(preds, labels)\n","            precision = precision_metric(preds, labels)\n","            recall = recall_metric(preds, labels)\n","            f1 = f1_metric(preds, labels)\n","\n","            # Acumular métricas\n","            epoch_loss += loss.item()\n","            epoch_acc += accuracy.item()\n","            epoch_precision += precision.item()\n","            epoch_recall += recall.item()\n","            epoch_f1 += f1.item()\n","\n","            # Actualizar progreso\n","            progress_bar.set_postfix({\n","                'loss': loss.item(),\n","                'acc': accuracy.item(),\n","                'prec': precision.item(),\n","                'rec': recall.item(),\n","                'f1': f1.item()\n","            })\n","\n","            # Liberar memoria explícitamente\n","            del pixel_values, labels, outputs, loss, logits, preds\n","            torch.cuda.empty_cache()\n","\n","            # Guardar checkpoint cada ciertos pasos\n","            if (step + 1) % config[\"save_steps\"] == 0:\n","                checkpoint_path = os.path.join(\n","                    config[\"output_dir\"],\n","                    f\"checkpoint_epoch{epoch+1}_step{step+1}.pt\"\n","                )\n","                torch.save({\n","                    'epoch': epoch,\n","                    'step': step,\n","                    'model_state_dict': model.state_dict(),\n","                    'optimizer_state_dict': optimizer.state_dict(),\n","                    'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n","                    'loss': loss.item(),\n","                }, checkpoint_path)\n","                logger.info(f\"Guardado checkpoint en {checkpoint_path}\")\n","\n","        except Exception as e:\n","            logger.error(f\"Error en paso {step}, época {epoch+1}: {str(e)}\")\n","            # Intentar liberar memoria y continuar\n","            torch.cuda.empty_cache()\n","            continue\n","\n","    # Calcular métricas promedio\n","    num_batches = len(dataloader)\n","    epoch_loss /= num_batches\n","    epoch_acc /= num_batches\n","    epoch_precision /= num_batches\n","    epoch_recall /= num_batches\n","    epoch_f1 /= num_batches\n","\n","    return {\n","        'loss': epoch_loss,\n","        'accuracy': epoch_acc,\n","        'precision': epoch_precision,\n","        'recall': epoch_recall,\n","        'f1': epoch_f1\n","    }\n","\n","def evaluate(model, dataloader, criterion, device, config):\n","    \"\"\"Evalúa el modelo en un conjunto de datos\"\"\"\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    val_loss = 0\n","\n","    with torch.no_grad():\n","        for batch in tqdm(dataloader, desc=\"Evaluando\"):\n","            try:\n","                # Mover datos al dispositivo\n","                pixel_values = batch['pixel_values'].to(device)\n","                labels = batch['labels'].to(device)\n","\n","                # Forward pass\n","                outputs = model(pixel_values=pixel_values, labels=labels)\n","                loss = outputs.loss\n","                val_loss += loss.item()\n","\n","                # Obtener predicciones\n","                logits = outputs.logits\n","                preds = torch.sigmoid(logits[:, 1])  # Solo la probabilidad de 'violencia'\n","\n","                # Guardar predicciones y etiquetas\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","                # Liberar memoria\n","                del pixel_values, labels, outputs, loss, logits, preds\n","                torch.cuda.empty_cache()\n","\n","            except Exception as e:\n","                logger.error(f\"Error al evaluar batch: {str(e)}\")\n","                continue\n","\n","    # Convertir a arrays numpy\n","    all_preds = np.array(all_preds)\n","    all_labels = np.array(all_labels)\n","\n","    if len(all_preds) == 0 or len(all_labels) == 0:\n","        logger.error(\"No se pudieron obtener predicciones o etiquetas durante la evaluación\")\n","        return {\n","            'loss': float('inf'),\n","            'accuracy': 0,\n","            'precision': 0,\n","            'recall': 0,\n","            'specificity': 0,\n","            'f1': 0,\n","            'roc_auc': 0,\n","            'confusion_matrix': np.zeros((2, 2)),\n","            'fpr': np.array([0, 1]),\n","            'tpr': np.array([0, 0]),\n","            'predictions': np.array([]),\n","            'labels': np.array([])\n","        }\n","\n","    # Calcular métricas\n","    binary_preds = (all_preds >= config[\"threshold\"]).astype(int)\n","\n","    accuracy = accuracy_score(all_labels, binary_preds)\n","    precision = precision_score(all_labels, binary_preds, zero_division=0)\n","    recall = recall_score(all_labels, binary_preds, zero_division=0)\n","    f1 = f1_score(all_labels, binary_preds, zero_division=0)\n","\n","    # Calcular especificidad (TN / (TN + FP))\n","    tn, fp, fn, tp = confusion_matrix(all_labels, binary_preds, labels=[0, 1]).ravel()\n","    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n","\n","    # Calcular métricas de curva ROC\n","    try:\n","        fpr, tpr, _ = roc_curve(all_labels, all_preds)\n","        roc_auc = auc(fpr, tpr)\n","    except Exception as e:\n","        logger.error(f\"Error al calcular curva ROC: {str(e)}\")\n","        fpr, tpr = np.array([0, 1]), np.array([0, 0])\n","        roc_auc = 0\n","\n","    # Matriz de confusión\n","    cm = confusion_matrix(all_labels, binary_preds, labels=[0, 1])\n","\n","    # Pérdida promedio\n","    val_loss /= len(dataloader)\n","\n","    # Crear informe de evaluación\n","    eval_results = {\n","        'loss': val_loss,\n","        'accuracy': accuracy,\n","        'precision': precision,\n","        'recall': recall,  # Sensibilidad\n","        'specificity': specificity,\n","        'f1': f1,\n","        'roc_auc': roc_auc,\n","        'confusion_matrix': cm,\n","        'fpr': fpr,\n","        'tpr': tpr,\n","        'predictions': all_preds,\n","        'labels': all_labels\n","    }\n","\n","    return eval_results\n","\n","def plot_metrics(train_metrics, val_metrics, config):\n","    \"\"\"Genera gráficos de métricas de entrenamiento\"\"\"\n","    metrics_to_plot = ['loss', 'accuracy', 'precision', 'recall', 'f1']\n","    epochs = range(1, len(train_metrics['loss']) + 1)\n","\n","    plt.figure(figsize=(20, 15))\n","\n","    for i, metric in enumerate(metrics_to_plot):\n","        plt.subplot(3, 2, i+1)\n","        plt.plot(epochs, train_metrics[metric], 'b-', label=f'Training {metric}')\n","        plt.plot(epochs, val_metrics[metric], 'r-', label=f'Validation {metric}')\n","        plt.title(f'{metric.capitalize()} vs. Epochs')\n","        plt.xlabel('Epochs')\n","        plt.ylabel(metric.capitalize())\n","        plt.legend()\n","        plt.grid(True)\n","\n","    # Guardar figura\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config[\"output_dir\"], \"training_metrics.png\"))\n","    plt.close()\n","\n","def plot_confusion_matrix(cm, config, phase='transfer_learning'):\n","    \"\"\"Visualiza la matriz de confusión\"\"\"\n","    plt.figure(figsize=(10, 8))\n","    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n","                xticklabels=['No Violencia', 'Violencia'],\n","                yticklabels=['No Violencia', 'Violencia'])\n","    plt.xlabel('Predicción')\n","    plt.ylabel('Real')\n","    plt.title('Matriz de Confusión')\n","\n","    # Guardar figura\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config[\"output_dir\"], f\"confusion_matrix_{phase}.png\"))\n","    plt.close()\n","\n","def plot_roc_curve(fpr, tpr, roc_auc, config, phase='transfer_learning'):\n","    \"\"\"Visualiza la curva ROC\"\"\"\n","    plt.figure(figsize=(10, 8))\n","    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n","    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC)')\n","    plt.legend(loc=\"lower right\")\n","\n","    # Guardar figura\n","    plt.tight_layout()\n","    plt.savefig(os.path.join(config[\"output_dir\"], f\"roc_curve_{phase}.png\"))\n","    plt.close()\n","\n","def save_evaluation_report(eval_results, config, phase='transfer_learning'):\n","    \"\"\"Guarda un informe detallado de la evaluación\"\"\"\n","    report = {\n","        'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n","        'phase': phase,\n","        'metrics': {\n","            'loss': float(eval_results['loss']),\n","            'accuracy': float(eval_results['accuracy']),\n","            'precision': float(eval_results['precision']),\n","            'recall': float(eval_results['recall']),\n","            'specificity': float(eval_results['specificity']),\n","            'f1_score': float(eval_results['f1']),\n","            'roc_auc': float(eval_results['roc_auc']),\n","        },\n","        'confusion_matrix': eval_results['confusion_matrix'].tolist(),\n","    }\n","\n","    # Guardar informe en formato JSON\n","    with open(os.path.join(config[\"output_dir\"], f\"evaluation_report_{phase}.json\"), 'w') as f:\n","        json.dump(report, f, indent=4)\n","\n","    # También guardar en formato de texto para mejor legibilidad\n","    with open(os.path.join(config[\"output_dir\"], f\"evaluation_report_{phase}.txt\"), 'w') as f:\n","        f.write(f\"Evaluación del Modelo - Fase: {phase}\\n\")\n","        f.write(f\"Fecha: {report['timestamp']}\\n\")\n","        f.write(\"\\n=== Métricas ===\\n\")\n","        f.write(f\"Loss: {report['metrics']['loss']:.4f}\\n\")\n","        f.write(f\"Accuracy: {report['metrics']['accuracy']:.4f}\\n\")\n","        f.write(f\"Precision: {report['metrics']['precision']:.4f}\\n\")\n","        f.write(f\"Recall (Sensibilidad): {report['metrics']['recall']:.4f}\\n\")\n","        f.write(f\"Specificity: {report['metrics']['specificity']:.4f}\\n\")\n","        f.write(f\"F1-Score: {report['metrics']['f1_score']:.4f}\\n\")\n","        f.write(f\"ROC AUC: {report['metrics']['roc_auc']:.4f}\\n\")\n","        f.write(\"\\n=== Matriz de Confusión ===\\n\")\n","        f.write(\"                Pred: No Violencia  Pred: Violencia\\n\")\n","        f.write(f\"Real: No Violencia    {eval_results['confusion_matrix'][0][0]}               {eval_results['confusion_matrix'][0][1]}\\n\")\n","        f.write(f\"Real: Violencia       {eval_results['confusion_matrix'][1][0]}               {eval_results['confusion_matrix'][1][1]}\\n\")"],"metadata":{"id":"A_RQ45EhnyXZ","executionInfo":{"status":"ok","timestamp":1747624299953,"user_tz":240,"elapsed":82,"user":{"displayName":"Franz Reinaldo Gonzales","userId":"07029606055805766342"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# ============================== ENTRENAMIENTO CON TRANSFER LEARNING ==============================\n","\n","logger.info(\"Iniciando fase de Transfer Learning\")\n","print(\"Iniciando fase de Transfer Learning\")\n","\n","# 1. Cargar modelo pre-entrenado\n","model = TimesformerForVideoClassification.from_pretrained(\n","    CONFIG[\"pretrained_model\"],\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"],\n","    num_labels=CONFIG[\"num_classes\"],  # Añadir esto para configurar 2 clases desde el inicio\n","    ignore_mismatched_sizes=True\n",")\n","\n","# 2. Asegurarnos de que la clasificación final tiene el número correcto de salidas\n","if hasattr(model, 'classifier'):\n","    if hasattr(model.classifier, 'out_features') and model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        # Guardar dimensión de entrada\n","        in_features = model.classifier.in_features\n","\n","        # Reemplazar completamente el clasificador\n","        model.classifier = nn.Linear(in_features, CONFIG[\"num_classes\"])\n","\n","        logger.info(f\"Reemplazada capa de clasificación: {in_features} -> {CONFIG['num_classes']}\")\n","    elif isinstance(model.classifier, nn.Sequential):\n","        # Si ya es una secuencia, asegurarnos que la última capa tenga la salida correcta\n","        last_layer = model.classifier[-1]\n","        if hasattr(last_layer, 'out_features') and last_layer.out_features != CONFIG[\"num_classes\"]:\n","            in_features = last_layer.in_features\n","            model.classifier[-1] = nn.Linear(in_features, CONFIG[\"num_classes\"])\n","            logger.info(f\"Reemplazada última capa de clasificación: {in_features} -> {CONFIG['num_classes']}\")\n","\n","\n","# 3. Congelar los parámetros del modelo base (excepto los de la capa de clasificación)\n","for name, param in model.named_parameters():\n","    if 'classifier' not in name:  # Congelar todos los parámetros excepto los del clasificador\n","        param.requires_grad = False\n","\n","# Verificar parámetros entrenables vs congelados\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in model.parameters())\n","logger.info(f\"Parámetros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n","print(f\"Parámetros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n","\n","# Mover modelo a GPU\n","model.to(device)\n","\n","# 4. Preparar datasets y dataloaders\n","train_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='train',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","val_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='val',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=CONFIG[\"tl_batch_size\"],\n","    shuffle=True,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=CONFIG[\"tl_batch_size\"],\n","    shuffle=False,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","# 5. Configurar optimizador y scheduler\n","optimizer = optim.AdamW(\n","    filter(lambda p: p.requires_grad, model.parameters()),\n","    lr=CONFIG[\"tl_learning_rate\"],\n","    weight_decay=CONFIG[\"tl_weight_decay\"]\n",")\n","\n","# Calcular pasos totales para schedulers\n","num_training_steps = len(train_dataloader) * CONFIG[\"tl_num_epochs\"]\n","num_warmup_steps = int(num_training_steps * CONFIG[\"tl_warmup_ratio\"])\n","\n","scheduler = get_cosine_schedule_with_warmup(\n","    optimizer,\n","    num_warmup_steps=num_warmup_steps,\n","    num_training_steps=num_training_steps\n",")\n","\n","# 6. Criterio de pérdida (ya incluido en el modelo)\n","criterion = nn.CrossEntropyLoss()\n","\n","# 7. Inicializar tracking de métricas\n","best_val_f1 = 0.0\n","train_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n","val_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n","\n","# 8. Entrenamiento por épocas\n","for epoch in range(CONFIG[\"tl_num_epochs\"]):\n","    logger.info(f\"Iniciando época {epoch+1}/{CONFIG['tl_num_epochs']}\")\n","    print(f\"Iniciando época {epoch+1}/{CONFIG['tl_num_epochs']}\")\n","\n","    # Entrenamiento\n","    train_results = train_epoch(\n","        model=model,\n","        dataloader=train_dataloader,\n","        optimizer=optimizer,\n","        scheduler=scheduler,\n","        criterion=criterion,\n","        device=device,\n","        epoch=epoch,\n","        config=CONFIG\n","    )\n","\n","    # Evaluación\n","    eval_results = evaluate(\n","        model=model,\n","        dataloader=val_dataloader,\n","        criterion=criterion,\n","        device=device,\n","        config=CONFIG\n","    )\n","\n","    # Registrar métricas\n","    for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1']:\n","        train_metrics[metric].append(train_results[metric])\n","        val_metrics[metric].append(eval_results[metric])\n","\n","    # Mostrar resultados\n","    logger.info(f\"Epoch {epoch+1}/{CONFIG['tl_num_epochs']} - \"\n","               f\"Train Loss: {train_results['loss']:.4f}, \"\n","               f\"Val Loss: {eval_results['loss']:.4f}, \"\n","               f\"Train Acc: {train_results['accuracy']:.4f}, \"\n","               f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n","               f\"Val F1: {eval_results['f1']:.4f}\")\n","    print(f\"Epoch {epoch+1}/{CONFIG['tl_num_epochs']} - \"\n","          f\"Train Loss: {train_results['loss']:.4f}, \"\n","          f\"Val Loss: {eval_results['loss']:.4f}, \"\n","          f\"Train Acc: {train_results['accuracy']:.4f}, \"\n","          f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n","          f\"Val F1: {eval_results['f1']:.4f}\")\n","\n","    # Guardar mejor modelo\n","    if eval_results['f1'] > best_val_f1:\n","        best_val_f1 = eval_results['f1']\n","\n","        # Guardar modelo\n","        model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_tl.pt\")\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'val_f1': best_val_f1,\n","            'config': CONFIG,\n","        }, model_path)\n","\n","        logger.info(f\"Guardado mejor modelo con F1: {best_val_f1:.4f} en {model_path}\")\n","        print(f\"Guardado mejor modelo con F1: {best_val_f1:.4f} en {model_path}\")\n","\n","    # Guardar checkpoint al final de cada época\n","    checkpoint_path = os.path.join(CONFIG[\"output_dir\"], f\"checkpoint_epoch{epoch+1}.pt\")\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'train_metrics': train_metrics,\n","        'val_metrics': val_metrics,\n","    }, checkpoint_path)\n","\n","    logger.info(f\"Guardado checkpoint de época {epoch+1} en {checkpoint_path}\")\n","    print(f\"Guardado checkpoint de época {epoch+1} en {checkpoint_path}\")\n","\n","# 9. Visualizar y guardar métricas\n","plot_metrics(train_metrics, val_metrics, CONFIG)\n","\n","# 10. Evaluación final del mejor modelo\n","# Cargar el mejor modelo\n","best_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_tl.pt\")\n","checkpoint = torch.load(best_model_path)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","logger.info(f\"Evaluando mejor modelo de Transfer Learning (F1: {checkpoint['val_f1']:.4f})\")\n","print(f\"Evaluando mejor modelo de Transfer Learning (F1: {checkpoint['val_f1']:.4f})\")\n","\n","final_eval_results = evaluate(\n","    model=model,\n","    dataloader=val_dataloader,\n","    criterion=criterion,\n","    device=device,\n","    config=CONFIG\n",")\n","\n","# Visualizar matriz de confusión\n","plot_confusion_matrix(final_eval_results['confusion_matrix'], CONFIG, phase='transfer_learning')\n","\n","# Visualizar curva ROC\n","plot_roc_curve(\n","    final_eval_results['fpr'],\n","    final_eval_results['tpr'],\n","    final_eval_results['roc_auc'],\n","    CONFIG,\n","    phase='transfer_learning'\n",")\n","\n","# Guardar informe detallado\n","save_evaluation_report(final_eval_results, CONFIG, phase='transfer_learning')\n","\n","logger.info(\"Completada fase de Transfer Learning\")\n","print(\"Completada fase de Transfer Learning\")\n","\n","# Guardar resultados para usarlos en etapas posteriores\n","tl_results = final_eval_results"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["7924adcf74bf4a42a7f0ab27e6d6ef04","697dfc0588c44317a338cbaa788ed59f","fe1a2c253809498193f32f3a18d768ad","43b8b830de504b0883aa9b70daa950c4","f382f011b94141d79bbdb997c7b68324","4ba32710f95c4391a9dc7f73dbd3f5aa","df84d3f3c2ba48b8ab1e80c37a3ca01b","3f8287a768e04513b79e8a8b6b452c9c","3f37708ab1164167883c64e09557874c","f9c28cef3376473794b7ccfc5bb0a613","a7cb279a936946109aea5bb6b9a7a235"]},"id":"cs5N5PgCn7vg","outputId":"ec99e48e-c940-4909-d363-77e4f3b2ac47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Iniciando fase de Transfer Learning\n"]},{"output_type":"stream","name":"stderr","text":["Some weights of TimesformerForVideoClassification were not initialized from the model checkpoint at facebook/timesformer-base-finetuned-k400 and are newly initialized because the shapes did not match:\n","- classifier.weight: found shape torch.Size([400, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n","- classifier.bias: found shape torch.Size([400]) in the checkpoint and torch.Size([2]) in the model instantiated\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Parámetros entrenables: 1,538 / 121,260,290 (0.00%)\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7924adcf74bf4a42a7f0ab27e6d6ef04"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"]},{"output_type":"stream","name":"stdout","text":["Cargados 8000 videos para split 'train'\n","Violencia: 4000, No Violencia: 4000\n","Cargados 1500 videos para split 'val'\n","Violencia: 750, No Violencia: 750\n","Iniciando época 1/15\n"]},{"output_type":"stream","name":"stderr","text":["Época 1:  10%|▉         | 99/1000 [03:57<21:12,  1.41s/it, loss=0.981, acc=0.25, prec=0.4, rec=0.4, f1=0.4]   ERROR:__main__:Error en paso 99, época 1: cannot access local variable 'loss' where it is not associated with a value\n","Época 1:  20%|█▉        | 199/1000 [06:19<33:15,  2.49s/it, loss=0.725, acc=0.375, prec=0.2, rec=0.5, f1=0.286]ERROR:__main__:Error en paso 199, época 1: cannot access local variable 'loss' where it is not associated with a value\n","Época 1:  30%|██▉       | 299/1000 [08:42<18:57,  1.62s/it, loss=0.988, acc=0.25, prec=0, rec=0, f1=0]      ERROR:__main__:Error en paso 299, época 1: cannot access local variable 'loss' where it is not associated with a value\n","Época 1:  40%|███▉      | 399/1000 [10:57<14:41,  1.47s/it, loss=1.03, acc=0.375, prec=0.5, rec=0.4, f1=0.444]ERROR:__main__:Error en paso 399, época 1: cannot access local variable 'loss' where it is not associated with a value\n","Época 1:  50%|████▉     | 499/1000 [13:19<11:02,  1.32s/it, loss=0.644, acc=0.25, prec=0.333, rec=0.2, f1=0.25] ERROR:__main__:Error en paso 499, época 1: cannot access local variable 'loss' where it is not associated with a value\n","Época 1:  60%|█████▉    | 599/1000 [15:38<10:10,  1.52s/it, loss=0.721, acc=0.375, prec=0.5, rec=0.2, f1=0.286]ERROR:__main__:Error en paso 599, época 1: cannot access local variable 'loss' where it is not associated with a value\n","Época 1:  70%|██████▉   | 699/1000 [18:02<07:29,  1.49s/it, loss=0.598, acc=0.5, prec=1, rec=0.2, f1=0.333]ERROR:__main__:Error en paso 699, época 1: cannot access local variable 'loss' where it is not associated with a value\n","Época 1:  80%|███████▉  | 799/1000 [20:23<04:50,  1.44s/it, loss=0.525, acc=0.5, prec=1, rec=0.333, f1=0.5]    ERROR:__main__:Error en paso 799, época 1: cannot access local variable 'loss' where it is not associated with a value\n","Época 1:  90%|████████▉ | 899/1000 [23:28<02:21,  1.40s/it, loss=0.431, acc=0.375, prec=0.5, rec=0.2, f1=0.286]ERROR:__main__:Error en paso 899, época 1: cannot access local variable 'loss' where it is not associated with a value\n","Época 1: 100%|█████████▉| 999/1000 [25:47<00:01,  1.24s/it, loss=0.333, acc=0.875, prec=1, rec=0.75, f1=0.857] ERROR:__main__:Error en paso 999, época 1: cannot access local variable 'loss' where it is not associated with a value\n","Época 1: 100%|██████████| 1000/1000 [25:47<00:00,  1.55s/it, loss=0.333, acc=0.875, prec=1, rec=0.75, f1=0.857]\n","Evaluando: 100%|██████████| 188/188 [06:38<00:00,  2.12s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 1/15 - Train Loss: 0.6755, Val Loss: 0.3881, Train Acc: 0.4586, Val Acc: 0.6793, Val F1: 0.5575\n","Guardado mejor modelo con F1: 0.5575 en /content/drive/MyDrive/Proyecto-Deteccion-Violencia/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n","Guardado checkpoint de época 1 en /content/drive/MyDrive/Proyecto-Deteccion-Violencia/modelo_timesformer/checkpoint_epoch1.pt\n","Iniciando época 2/15\n"]},{"output_type":"stream","name":"stderr","text":["Época 2:  10%|▉         | 99/1000 [01:55<17:12,  1.15s/it, loss=0.353, acc=0.875, prec=1, rec=0.8, f1=0.889]    ERROR:__main__:Error en paso 99, época 2: cannot access local variable 'loss' where it is not associated with a value\n","Época 2:  20%|█▉        | 199/1000 [03:48<15:08,  1.13s/it, loss=0.458, acc=0.75, prec=1, rec=0.667, f1=0.8]ERROR:__main__:Error en paso 199, época 2: cannot access local variable 'loss' where it is not associated with a value\n","Época 2:  30%|██▉       | 299/1000 [05:41<13:09,  1.13s/it, loss=0.234, acc=1, prec=1, rec=1, f1=1]         ERROR:__main__:Error en paso 299, época 2: cannot access local variable 'loss' where it is not associated with a value\n","Época 2:  40%|███▉      | 399/1000 [07:34<11:15,  1.12s/it, loss=0.206, acc=0.875, prec=1, rec=0.667, f1=0.8]    ERROR:__main__:Error en paso 399, época 2: cannot access local variable 'loss' where it is not associated with a value\n","Época 2:  50%|████▉     | 499/1000 [09:27<09:23,  1.13s/it, loss=0.198, acc=0.75, prec=1, rec=0.6, f1=0.75]ERROR:__main__:Error en paso 499, época 2: cannot access local variable 'loss' where it is not associated with a value\n","Época 2:  60%|█████▉    | 599/1000 [11:21<07:32,  1.13s/it, loss=0.39, acc=0.75, prec=0.75, rec=0.75, f1=0.75]ERROR:__main__:Error en paso 599, época 2: cannot access local variable 'loss' where it is not associated with a value\n","Época 2:  70%|██████▉   | 699/1000 [13:14<05:44,  1.14s/it, loss=0.111, acc=1, prec=1, rec=1, f1=1]            ERROR:__main__:Error en paso 699, época 2: cannot access local variable 'loss' where it is not associated with a value\n","Época 2:  80%|███████▉  | 799/1000 [15:07<03:48,  1.13s/it, loss=0.261, acc=0.875, prec=0.8, rec=1, f1=0.889]ERROR:__main__:Error en paso 799, época 2: cannot access local variable 'loss' where it is not associated with a value\n","Época 2:  90%|████████▉ | 899/1000 [17:00<01:54,  1.14s/it, loss=0.255, acc=0.875, prec=1, rec=0.8, f1=0.889]ERROR:__main__:Error en paso 899, época 2: cannot access local variable 'loss' where it is not associated with a value\n","Época 2: 100%|█████████▉| 999/1000 [18:53<00:01,  1.13s/it, loss=0.136, acc=0.875, prec=0.857, rec=1, f1=0.923]ERROR:__main__:Error en paso 999, época 2: cannot access local variable 'loss' where it is not associated with a value\n","Época 2: 100%|██████████| 1000/1000 [18:53<00:00,  1.13s/it, loss=0.136, acc=0.875, prec=0.857, rec=1, f1=0.923]\n","Evaluando: 100%|██████████| 188/188 [03:32<00:00,  1.13s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 2/15 - Train Loss: 0.2773, Val Loss: 0.2023, Train Acc: 0.8586, Val Acc: 0.8840, Val F1: 0.8721\n","Guardado mejor modelo con F1: 0.8721 en /content/drive/MyDrive/Proyecto-Deteccion-Violencia/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n","Guardado checkpoint de época 2 en /content/drive/MyDrive/Proyecto-Deteccion-Violencia/modelo_timesformer/checkpoint_epoch2.pt\n","Iniciando época 3/15\n"]},{"output_type":"stream","name":"stderr","text":["Época 3:  10%|▉         | 99/1000 [01:58<17:09,  1.14s/it, loss=0.077, acc=1, prec=1, rec=1, f1=1] ERROR:__main__:Error en paso 99, época 3: cannot access local variable 'loss' where it is not associated with a value\n","Época 3:  20%|█▉        | 199/1000 [03:52<15:31,  1.16s/it, loss=0.272, acc=0.875, prec=0.857, rec=1, f1=0.923]ERROR:__main__:Error en paso 199, época 3: cannot access local variable 'loss' where it is not associated with a value\n","Época 3:  30%|██▉       | 299/1000 [05:44<13:12,  1.13s/it, loss=0.181, acc=1, prec=1, rec=1, f1=1]            ERROR:__main__:Error en paso 299, época 3: cannot access local variable 'loss' where it is not associated with a value\n","Época 3:  40%|███▉      | 399/1000 [07:38<11:20,  1.13s/it, loss=0.135, acc=0.875, prec=1, rec=0.833, f1=0.909]ERROR:__main__:Error en paso 399, época 3: cannot access local variable 'loss' where it is not associated with a value\n","Época 3:  50%|████▉     | 499/1000 [09:30<09:25,  1.13s/it, loss=0.321, acc=0.875, prec=0.75, rec=1, f1=0.857]ERROR:__main__:Error en paso 499, época 3: cannot access local variable 'loss' where it is not associated with a value\n","Época 3:  60%|█████▉    | 599/1000 [11:24<07:30,  1.12s/it, loss=0.15, acc=0.875, prec=1, rec=0.8, f1=0.889]  ERROR:__main__:Error en paso 599, época 3: cannot access local variable 'loss' where it is not associated with a value\n","Época 3:  70%|██████▉   | 699/1000 [13:17<05:38,  1.12s/it, loss=0.066, acc=0.75, prec=1, rec=0.6, f1=0.75]ERROR:__main__:Error en paso 699, época 3: cannot access local variable 'loss' where it is not associated with a value\n","Época 3:  80%|███████▉  | 799/1000 [15:10<03:45,  1.12s/it, loss=0.247, acc=0.875, prec=0.667, rec=1, f1=0.8]ERROR:__main__:Error en paso 799, época 3: cannot access local variable 'loss' where it is not associated with a value\n","Época 3:  90%|████████▉ | 899/1000 [17:03<01:58,  1.17s/it, loss=0.0852, acc=1, prec=1, rec=1, f1=1]         ERROR:__main__:Error en paso 899, época 3: cannot access local variable 'loss' where it is not associated with a value\n","Época 3: 100%|█████████▉| 999/1000 [18:56<00:01,  1.13s/it, loss=0.244, acc=1, prec=1, rec=1, f1=1]           ERROR:__main__:Error en paso 999, época 3: cannot access local variable 'loss' where it is not associated with a value\n","Época 3: 100%|██████████| 1000/1000 [18:56<00:00,  1.14s/it, loss=0.244, acc=1, prec=1, rec=1, f1=1]\n","Evaluando: 100%|██████████| 188/188 [03:32<00:00,  1.13s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Epoch 3/15 - Train Loss: 0.1906, Val Loss: 0.1640, Train Acc: 0.9140, Val Acc: 0.9120, Val F1: 0.9060\n","Guardado mejor modelo con F1: 0.9060 en /content/drive/MyDrive/Proyecto-Deteccion-Violencia/modelo_timesformer/timesformer_violence_detector_best_tl.pt\n","Guardado checkpoint de época 3 en /content/drive/MyDrive/Proyecto-Deteccion-Violencia/modelo_timesformer/checkpoint_epoch3.pt\n","Iniciando época 4/15\n"]},{"output_type":"stream","name":"stderr","text":["Época 4:  10%|▉         | 99/1000 [01:58<17:21,  1.16s/it, loss=0.301, acc=0.875, prec=0.75, rec=1, f1=0.857]ERROR:__main__:Error en paso 99, época 4: cannot access local variable 'loss' where it is not associated with a value\n","Época 4:  20%|█▉        | 199/1000 [03:51<15:10,  1.14s/it, loss=0.504, acc=0.875, prec=0.833, rec=1, f1=0.909]  ERROR:__main__:Error en paso 199, época 4: cannot access local variable 'loss' where it is not associated with a value\n","Época 4:  30%|██▉       | 299/1000 [05:45<13:09,  1.13s/it, loss=0.324, acc=0.75, prec=0.667, rec=1, f1=0.8] ERROR:__main__:Error en paso 299, época 4: cannot access local variable 'loss' where it is not associated with a value\n","Época 4:  38%|███▊      | 376/1000 [07:10<11:41,  1.12s/it, loss=0.00365, acc=1, prec=1, rec=1, f1=1]"]}]},{"cell_type":"code","source":["# ============================== FINE TUNING ==============================\n","\n","logger.info(\"Iniciando fase de Fine-Tuning\")\n","\n","# Cargar el mejor modelo de Transfer Learning\n","best_tl_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_tl.pt\")\n","checkpoint = torch.load(best_tl_model_path)\n","\n","# Comprobar si ya tenemos el modelo cargado (de la celda anterior) o necesitamos cargarlo\n","try:\n","    # Intentar acceder al modelo, si no está definido, lanzará una excepción\n","    model\n","    logger.info(\"Usando modelo ya cargado de celda anterior\")\n","    # Cargar estado del modelo desde checkpoint\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","except NameError:\n","    # Si el modelo no está definido, crear uno nuevo y cargarlo\n","    logger.info(\"Cargando modelo desde checkpoint\")\n","    model = TimesformerForVideoClassification.from_pretrained(\n","        CONFIG[\"pretrained_model\"],\n","        num_frames=CONFIG[\"num_frames\"],\n","        image_size=CONFIG[\"image_size\"],\n","    )\n","\n","    # Adaptarlo a nuestra tarea si es necesario\n","    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(CONFIG[\"tl_dropout\"]),\n","            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n","        )\n","\n","    # Cargar estado del modelo desde checkpoint\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# 1. Descongelar todos los parámetros del modelo\n","for param in model.parameters():\n","    param.requires_grad = True\n","\n","# Verificar parámetros entrenables\n","trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","total_params = sum(p.numel() for p in model.parameters())\n","logger.info(f\"Parámetros entrenables: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n","\n","# Asegurar que el modelo está en el dispositivo correcto\n","model.to(device)\n","\n","# 2. Preparar datasets y dataloaders (mismos que antes)\n","train_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='train',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","val_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='val',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","train_dataloader = DataLoader(\n","    train_dataset,\n","    batch_size=CONFIG[\"ft_batch_size\"],  # Tamaño de batch más pequeño para fine-tuning\n","    shuffle=True,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","val_dataloader = DataLoader(\n","    val_dataset,\n","    batch_size=CONFIG[\"ft_batch_size\"],\n","    shuffle=False,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","# 3. Configurar optimizador con learning rate diferenciado\n","# Usar learning rates más pequeños para capas base y más grandes para capas superiores\n","param_groups = [\n","    {\n","        'params': [p for n, p in model.named_parameters() if 'classifier' not in n],\n","        'lr': CONFIG[\"ft_learning_rate\"] * 0.1  # LR más bajo para el backbone\n","    },\n","    {\n","        'params': [p for n, p in model.named_parameters() if 'classifier' in n],\n","        'lr': CONFIG[\"ft_learning_rate\"]  # LR normal para el clasificador\n","    }\n","]\n","\n","optimizer = optim.AdamW(\n","    param_groups,\n","    weight_decay=CONFIG[\"ft_weight_decay\"]\n",")\n","\n","# Calcular pasos totales para scheduler\n","num_training_steps = len(train_dataloader) * CONFIG[\"ft_num_epochs\"]\n","\n","# Scheduler con cosine annealing\n","scheduler = CosineAnnealingLR(\n","    optimizer,\n","    T_max=num_training_steps,\n","    eta_min=1e-6\n",")\n","\n","# 4. Criterio de pérdida (ya incluido en el modelo)\n","criterion = nn.CrossEntropyLoss()\n","\n","# 5. Inicializar tracking de métricas\n","best_val_f1 = 0.0\n","train_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n","val_metrics = {'loss': [], 'accuracy': [], 'precision': [], 'recall': [], 'f1': []}\n","\n","# 6. Entrenamiento por épocas\n","for epoch in range(CONFIG[\"ft_num_epochs\"]):\n","    logger.info(f\"Iniciando época {epoch+1}/{CONFIG['ft_num_epochs']} (fine-tuning)\")\n","\n","    # Entrenamiento\n","    train_results = train_epoch(\n","        model=model,\n","        dataloader=train_dataloader,\n","        optimizer=optimizer,\n","        scheduler=scheduler,\n","        criterion=criterion,\n","        device=device,\n","        epoch=epoch,\n","        config=CONFIG\n","    )\n","\n","    # Evaluación\n","    eval_results = evaluate(\n","        model=model,\n","        dataloader=val_dataloader,\n","        criterion=criterion,\n","        device=device,\n","        config=CONFIG\n","    )\n","\n","    # Registrar métricas\n","    for metric in ['loss', 'accuracy', 'precision', 'recall', 'f1']:\n","        train_metrics[metric].append(train_results[metric])\n","        val_metrics[metric].append(eval_results[metric])\n","\n","    # Mostrar resultados\n","    logger.info(f\"Epoch {epoch+1}/{CONFIG['ft_num_epochs']} (FT) - \"\n","               f\"Train Loss: {train_results['loss']:.4f}, \"\n","               f\"Val Loss: {eval_results['loss']:.4f}, \"\n","               f\"Train Acc: {train_results['accuracy']:.4f}, \"\n","               f\"Val Acc: {eval_results['accuracy']:.4f}, \"\n","               f\"Val F1: {eval_results['f1']:.4f}\")\n","\n","    # Guardar mejor modelo\n","    if eval_results['f1'] > best_val_f1:\n","        best_val_f1 = eval_results['f1']\n","\n","        # Guardar modelo\n","        model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","        torch.save({\n","            'epoch': epoch,\n","            'model_state_dict': model.state_dict(),\n","            'optimizer_state_dict': optimizer.state_dict(),\n","            'scheduler_state_dict': scheduler.state_dict(),\n","            'val_f1': best_val_f1,\n","            'config': CONFIG,\n","        }, model_path)\n","\n","        logger.info(f\"Guardado mejor modelo (FT) con F1: {best_val_f1:.4f} en {model_path}\")\n","\n","    # Guardar checkpoint al final de cada época\n","    checkpoint_path = os.path.join(CONFIG[\"output_dir\"], f\"checkpoint_ft_epoch{epoch+1}.pt\")\n","    torch.save({\n","        'epoch': epoch,\n","        'model_state_dict': model.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'scheduler_state_dict': scheduler.state_dict(),\n","        'train_metrics': train_metrics,\n","        'val_metrics': val_metrics,\n","    }, checkpoint_path)\n","\n","    logger.info(f\"Guardado checkpoint de fine-tuning época {epoch+1} en {checkpoint_path}\")\n","\n","# 7. Visualizar y guardar métricas\n","# Crear nuevas visualizaciones para fine-tuning\n","plt.figure(figsize=(20, 15))\n","\n","metrics_to_plot = ['loss', 'accuracy', 'precision', 'recall', 'f1']\n","epochs = range(1, len(train_metrics['loss']) + 1)\n","\n","for i, metric in enumerate(metrics_to_plot):\n","    plt.subplot(3, 2, i+1)\n","    plt.plot(epochs, train_metrics[metric], 'b-', label=f'Training {metric}')\n","    plt.plot(epochs, val_metrics[metric], 'r-', label=f'Validation {metric}')\n","    plt.title(f'{metric.capitalize()} vs. Epochs (Fine-Tuning)')\n","    plt.xlabel('Epochs')\n","    plt.ylabel(metric.capitalize())\n","    plt.legend()\n","    plt.grid(True)\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"fine_tuning_metrics.png\"))\n","plt.close()\n","\n","# 8. Evaluación final del mejor modelo\n","# Cargar el mejor modelo\n","best_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","checkpoint = torch.load(best_model_path)\n","model.load_state_dict(checkpoint['model_state_dict'])\n","\n","logger.info(f\"Evaluando mejor modelo de Fine-Tuning (F1: {checkpoint['val_f1']:.4f})\")\n","\n","final_eval_results = evaluate(\n","    model=model,\n","    dataloader=val_dataloader,\n","    criterion=criterion,\n","    device=device,\n","    config=CONFIG\n",")\n","\n","# Visualizar matriz de confusión\n","plot_confusion_matrix(final_eval_results['confusion_matrix'], CONFIG, phase='fine_tuning')\n","\n","# Visualizar curva ROC\n","plot_roc_curve(\n","    final_eval_results['fpr'],\n","    final_eval_results['tpr'],\n","    final_eval_results['roc_auc'],\n","    CONFIG,\n","    phase='fine_tuning'\n",")\n","\n","# Guardar informe detallado\n","save_evaluation_report(final_eval_results, CONFIG, phase='fine_tuning')\n","\n","logger.info(\"Completada fase de Fine-Tuning\")\n","\n","# Guardar resultados para usarlos en etapas posteriores\n","ft_results = final_eval_results"],"metadata":{"id":"D9RikOs4oJZR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== EVALUACIÓN EN CONJUNTO DE PRUEBA ==============================\n","\n","logger.info(\"Evaluando modelo en conjunto de prueba\")\n","\n","# Cargar el mejor modelo de Fine-Tuning\n","best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","\n","# Comprobar si ya tenemos el modelo cargado\n","try:\n","    # Intentar acceder al modelo\n","    model\n","    logger.info(\"Usando modelo ya cargado de celda anterior\")\n","    # Cargar estado del mejor modelo de fine-tuning\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","except NameError:\n","    # Si el modelo no está definido, crear uno nuevo y cargarlo\n","    logger.info(\"Cargando modelo desde checkpoint\")\n","    model = TimesformerForVideoClassification.from_pretrained(\n","        CONFIG[\"pretrained_model\"],\n","        num_frames=CONFIG[\"num_frames\"],\n","        image_size=CONFIG[\"image_size\"],\n","    )\n","\n","    # Adaptarlo a nuestra tarea\n","    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(CONFIG[\"tl_dropout\"]),\n","            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n","        )\n","\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# Asegurar que el modelo está en el dispositivo correcto\n","model.to(device)\n","model.eval()\n","\n","# Cargar dataset de prueba\n","test_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='test',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","test_dataloader = DataLoader(\n","    test_dataset,\n","    batch_size=CONFIG[\"ft_batch_size\"],\n","    shuffle=False,\n","    num_workers=2,\n","    pin_memory=True\n",")\n","\n","# Evaluar\n","criterion = nn.CrossEntropyLoss()\n","test_results = evaluate(\n","    model=model,\n","    dataloader=test_dataloader,\n","    criterion=criterion,\n","    device=device,\n","    config=CONFIG\n",")\n","\n","# Visualizar matriz de confusión\n","plot_confusion_matrix(test_results['confusion_matrix'], CONFIG, phase='test')\n","\n","# Visualizar curva ROC\n","plot_roc_curve(\n","    test_results['fpr'],\n","    test_results['tpr'],\n","    test_results['roc_auc'],\n","    CONFIG,\n","    phase='test'\n",")\n","\n","# Generar y guardar reporte detallado\n","save_evaluation_report(test_results, CONFIG, phase='test')\n","\n","# Métricas adicionales: Precision-Recall curve\n","precision, recall, _ = precision_recall_curve(\n","    test_results['labels'],\n","    test_results['predictions']\n",")\n","pr_auc = average_precision_score(test_results['labels'], test_results['predictions'])\n","\n","# Graficar curva Precision-Recall\n","plt.figure(figsize=(10, 8))\n","plt.plot(recall, precision, color='blue', lw=2, label=f'PR curve (AP = {pr_auc:.2f})')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.legend(loc=\"lower left\")\n","plt.grid(True)\n","plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"precision_recall_curve_test.png\"))\n","plt.close()\n","\n","# Análisis de mejores umbrales\n","# Calcular métricas para diferentes umbrales\n","thresholds = np.linspace(0.1, 0.9, 9)\n","threshold_metrics = []\n","\n","for threshold in thresholds:\n","    binary_preds = (test_results['predictions'] >= threshold).astype(int)\n","\n","    acc = accuracy_score(test_results['labels'], binary_preds)\n","    prec = precision_score(test_results['labels'], binary_preds, zero_division=0)\n","    rec = recall_score(test_results['labels'], binary_preds, zero_division=0)\n","    f1 = f1_score(test_results['labels'], binary_preds, zero_division=0)\n","\n","    threshold_metrics.append({\n","        'threshold': threshold,\n","        'accuracy': acc,\n","        'precision': prec,\n","        'recall': rec,\n","        'f1': f1\n","    })\n","\n","# Convertir a DataFrame para mejor visualización\n","threshold_df = pd.DataFrame(threshold_metrics)\n","\n","# Graficar métricas vs umbral\n","plt.figure(figsize=(12, 8))\n","for metric in ['accuracy', 'precision', 'recall', 'f1']:\n","    plt.plot(threshold_df['threshold'], threshold_df[metric], marker='o', label=metric)\n","\n","plt.xlabel('Umbral de decisión')\n","plt.ylabel('Valor de métrica')\n","plt.title('Métricas vs Umbral de decisión')\n","plt.legend()\n","plt.grid(True)\n","plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"threshold_analysis.png\"))\n","plt.close()\n","\n","# Encontrar mejor umbral según F1\n","best_threshold_idx = threshold_df['f1'].idxmax()\n","best_threshold = threshold_df.loc[best_threshold_idx, 'threshold']\n","\n","logger.info(f\"Mejor umbral encontrado: {best_threshold:.2f} con F1: {threshold_df.loc[best_threshold_idx, 'f1']:.4f}\")\n","\n","# Guardar análisis de umbrales\n","threshold_df.to_csv(os.path.join(CONFIG[\"output_dir\"], \"threshold_analysis.csv\"), index=False)\n","\n","# Actualizar el umbral en la configuración\n","CONFIG[\"threshold\"] = float(best_threshold)\n","with open(os.path.join(CONFIG[\"output_dir\"], \"config.json\"), 'w') as f:\n","    json.dump(CONFIG, f, indent=4)\n","\n","# Calcular y visualizar curva ROC detallada con punto óptimo\n","fpr, tpr, thresholds_roc = roc_curve(test_results['labels'], test_results['predictions'])\n","roc_auc = auc(fpr, tpr)\n","\n","# Calcular distancia al punto óptimo (0,1)\n","distances = np.sqrt((1-tpr)**2 + fpr**2)\n","optimal_idx = np.argmin(distances)\n","optimal_threshold = thresholds_roc[optimal_idx]\n","\n","plt.figure(figsize=(10, 8))\n","plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC (AUC = {roc_auc:.2f})')\n","plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n","plt.scatter(fpr[optimal_idx], tpr[optimal_idx], marker='o', color='red',\n","            label=f'Punto óptimo (umbral={optimal_threshold:.2f})')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('Tasa de Falsos Positivos')\n","plt.ylabel('Tasa de Verdaderos Positivos')\n","plt.title('Curva ROC con punto óptimo')\n","plt.legend(loc=\"lower right\")\n","plt.grid(True)\n","plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"roc_curve_optimal_test.png\"))\n","plt.close()\n","\n","logger.info(f\"Umbral óptimo según distancia a punto ideal en ROC: {optimal_threshold:.4f}\")\n","\n","# Guardar este umbral también\n","with open(os.path.join(CONFIG[\"output_dir\"], \"optimal_thresholds.json\"), 'w') as f:\n","    json.dump({\n","        'f1_optimal': float(best_threshold),\n","        'roc_optimal': float(optimal_threshold)\n","    }, f, indent=4)\n","\n","# Mostrar resumen de resultados\n","logger.info(f\"Resumen de evaluación en conjunto de prueba:\")\n","logger.info(f\"Accuracy: {test_results['accuracy']:.4f}\")\n","logger.info(f\"Precision: {test_results['precision']:.4f}\")\n","logger.info(f\"Recall (Sensibilidad): {test_results['recall']:.4f}\")\n","logger.info(f\"Specificity: {test_results['specificity']:.4f}\")\n","logger.info(f\"F1-Score: {test_results['f1']:.4f}\")\n","logger.info(f\"ROC AUC: {test_results['roc_auc']:.4f}\")\n","logger.info(f\"PR AUC: {pr_auc:.4f}\")"],"metadata":{"id":"_FiIInTroO9o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== EXPORTACIÓN DEL MODELO ==============================\n","\n","logger.info(\"Exportando modelo para inferencia\")\n","\n","# Comprobar si ya tenemos el modelo cargado\n","try:\n","    # Intentar acceder al modelo\n","    model\n","    # Asegurarse de que tiene cargado el mejor modelo de fine-tuning\n","    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","except NameError:\n","    # Si el modelo no está definido, crear uno nuevo y cargarlo\n","    logger.info(\"Cargando modelo desde checkpoint\")\n","    model = TimesformerForVideoClassification.from_pretrained(\n","        CONFIG[\"pretrained_model\"],\n","        num_frames=CONFIG[\"num_frames\"],\n","        image_size=CONFIG[\"image_size\"],\n","    )\n","\n","    # Adaptarlo a nuestra tarea\n","    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(CONFIG[\"tl_dropout\"]),\n","            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n","        )\n","\n","    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# Asegurar que el modelo está en el dispositivo correcto\n","model.to(device)\n","model.eval()\n","\n","# 1. Guardar modelo en formato PyTorch\n","model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_final.pt\")\n","torch.save({\n","    'model_state_dict': model.state_dict(),\n","    'config': CONFIG,\n","}, model_path)\n","\n","logger.info(f\"Modelo guardado en formato PyTorch: {model_path}\")\n","\n","# 2. Crear modelo para inferencia sin calcular pérdidas (más eficiente)\n","class TimesformerInference(nn.Module):\n","    def __init__(self, model):\n","        super().__init__()\n","        self.timesformer = model\n","\n","    def forward(self, pixel_values):\n","        outputs = self.timesformer(pixel_values=pixel_values)\n","        logits = outputs.logits\n","        # Aplicar sigmoid para obtener probabilidades para la clase 'violencia'\n","        probs = torch.sigmoid(logits[:, 1])\n","        return probs\n","\n","inference_model = TimesformerInference(model)\n","inference_model.eval()\n","\n","# Guardar modelo de inferencia\n","inference_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_inference.pt\")\n","torch.save(inference_model, inference_model_path)\n","\n","logger.info(f\"Modelo de inferencia guardado: {inference_model_path}\")\n","\n","# 3. Guardar también usando save_pretrained de Hugging Face\n","save_dir = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_hf\")\n","os.makedirs(save_dir, exist_ok=True)\n","\n","# Guardar modelo y procesador\n","try:\n","    model.save_pretrained(save_dir)\n","    processor = AutoImageProcessor.from_pretrained(CONFIG[\"pretrained_model\"])\n","    processor.save_pretrained(save_dir)\n","    logger.info(f\"Modelo y procesador guardados en formato Hugging Face: {save_dir}\")\n","except Exception as e:\n","    logger.warning(f\"Error al guardar modelo con save_pretrained: {str(e)}\")\n","    logger.info(\"Utilizando método alternativo para guardar en formato HF\")\n","\n","    # Método alternativo\n","    if not os.path.exists(os.path.join(save_dir, \"config.json\")):\n","        config_obj = TimesformerConfig.from_pretrained(CONFIG[\"pretrained_model\"])\n","        config_obj.num_frames = CONFIG[\"num_frames\"]\n","        config_obj.image_size = CONFIG[\"image_size\"]\n","        config_obj.num_labels = CONFIG[\"num_classes\"]\n","        config_obj.save_pretrained(save_dir)\n","\n","    if not os.path.exists(os.path.join(save_dir, \"pytorch_model.bin\")):\n","        torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n","\n","    # Guardar procesador\n","    processor = AutoImageProcessor.from_pretrained(CONFIG[\"pretrained_model\"])\n","    if not os.path.exists(os.path.join(save_dir, \"preprocessor_config.json\")):\n","        processor.save_pretrained(save_dir)\n","\n","    logger.info(f\"Modelo y procesador guardados en formato alternativo: {save_dir}\")\n","\n","# 4. Crear script de ejemplo para inferencia\n","inference_script = \"\"\"\n","import torch\n","import torch.nn as nn\n","from transformers import TimesformerForVideoClassification, AutoImageProcessor\n","from decord import VideoReader, cpu\n","import numpy as np\n","from torchvision import transforms\n","import os\n","import json\n","\n","def load_model(model_path, config_path=None):\n","    \"\"\"Carga el modelo de detección de violencia\"\"\"\n","    # Cargar configuración si se proporciona\n","    if config_path:\n","        with open(config_path, 'r') as f:\n","            config = json.load(f)\n","    else:\n","        # Intentar encontrar config.json en el mismo directorio\n","        config_dir = os.path.dirname(model_path)\n","        config_path = os.path.join(config_dir, \"config.json\")\n","        if os.path.exists(config_path):\n","            with open(config_path, 'r') as f:\n","                config = json.load(f)\n","        else:\n","            # Valores por defecto\n","            config = {\n","                \"num_frames\": 16,\n","                \"image_size\": 224,\n","                \"threshold\": 0.5\n","            }\n","\n","    # Opción 1: Cargar modelo guardado con HF save_pretrained\n","    if os.path.isdir(model_path):\n","        model = TimesformerForVideoClassification.from_pretrained(model_path)\n","        processor = AutoImageProcessor.from_pretrained(model_path)\n","        return model, processor, config\n","\n","    # Opción 2: Cargar modelo guardado con torch.save\n","    checkpoint = torch.load(model_path, map_location=\"cpu\")\n","\n","    if isinstance(checkpoint, torch.nn.Module):\n","        # Es un modelo entero guardado con torch.save(model)\n","        return checkpoint, None, config\n","\n","    # Es un diccionario con state_dict\n","    if \"model_state_dict\" in checkpoint:\n","        model = TimesformerForVideoClassification.from_pretrained(\n","            \"facebook/timesformer-base-finetuned-k400\",\n","            num_frames=config[\"num_frames\"],\n","            image_size=config[\"image_size\"],\n","        )\n","        model.load_state_dict(checkpoint[\"model_state_dict\"])\n","        processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n","        return model, processor, config\n","\n","    raise ValueError(\"Formato de modelo no reconocido\")\n","\n","def process_video(video_path, model, processor=None, config=None):\n","    \"\"\"Procesa un video y detecta violencia\"\"\"\n","    if config is None:\n","        config = {\n","            \"num_frames\": 16,\n","            \"image_size\": 224,\n","            \"threshold\": 0.5\n","        }\n","\n","    # Cargar video con decord\n","    video_reader = VideoReader(video_path, ctx=cpu(0))\n","    total_frames = len(video_reader)\n","\n","    # Seleccionar frames uniformemente\n","    indices = np.linspace(0, total_frames - 1, config[\"num_frames\"], dtype=int)\n","    frames = video_reader.get_batch(indices).asnumpy()  # (num_frames, H, W, C)\n","\n","    # Preprocesar frames\n","    processed_frames = []\n","    for frame in frames:\n","        # Redimensionar\n","        frame = transforms.functional.resize(\n","            transforms.functional.to_tensor(frame),\n","            (config[\"image_size\"], config[\"image_size\"])\n","        )\n","        processed_frames.append(frame)\n","\n","    # Apilar frames\n","    frames_tensor = torch.stack(processed_frames)  # (T, C, H, W)\n","\n","    # Preprocesar con el procesador si está disponible\n","    if processor:\n","        inputs = processor(\n","            list(frames_tensor),  # Lista de tensores (T, C, H, W)\n","            return_tensors=\"pt\"\n","        )\n","        pixel_values = inputs['pixel_values']\n","    else:\n","        # Formato alternativo si no hay procesador\n","        frames_tensor = frames_tensor.permute(1, 0, 2, 3).unsqueeze(0)  # (1, C, T, H, W)\n","        pixel_values = frames_tensor\n","\n","    # Inferencia\n","    model.eval()\n","    with torch.no_grad():\n","        # Comprobar si es modelo de inferencia personalizado\n","        if isinstance(model, nn.Module) and hasattr(model, 'timesformer'):\n","            # Modelo de inferencia que devuelve directamente probabilidades\n","            violence_prob = model(pixel_values)\n","        else:\n","            # Modelo TimesformerForVideoClassification estándar\n","            outputs = model(pixel_values=pixel_values)\n","            # Continuación del script de inferencia\n","            logits = outputs.logits\n","            violence_prob = torch.sigmoid(logits[:, 1])\n","\n","    # Determinar predicción según umbral\n","    is_violent = violence_prob.item() >= config[\"threshold\"]\n","\n","    return {\n","        \"is_violent\": is_violent,\n","        \"violence_probability\": violence_prob.item(),\n","        \"threshold\": config[\"threshold\"]\n","    }\n","\n","# Ejemplo de uso\n","if __name__ == \"__main__\":\n","    # Ruta al modelo guardado\n","    model_path = \"ruta/al/modelo_timesformer_inference.pt\"\n","    config_path = \"ruta/al/config.json\"\n","\n","    # Cargar modelo\n","    model, processor, config = load_model(model_path, config_path)\n","\n","    # Ruta al video a procesar\n","    video_path = \"ruta/al/video_ejemplo.mp4\"\n","\n","    # Procesar video\n","    result = process_video(video_path, model, processor, config)\n","\n","    # Mostrar resultado\n","    print(f\"¿Detectada violencia?: {'Sí' if result['is_violent'] else 'No'}\")\n","    print(f\"Probabilidad de violencia: {result['violence_probability']:.4f}\")\n","\"\"\"\n","\n","# Guardar script de ejemplo\n","with open(os.path.join(CONFIG[\"output_dir\"], \"inference_example.py\"), 'w') as f:\n","    f.write(inference_script)\n","\n","logger.info(f\"Script de ejemplo para inferencia guardado en: {os.path.join(CONFIG['output_dir'], 'inference_example.py')}\")\n","\n","# 5. Exportar modelo en formato ONNX para inferencia más rápida\n","try:\n","    # Generar un input de ejemplo\n","    dummy_input = torch.randn(1, 3, CONFIG[\"num_frames\"], CONFIG[\"image_size\"], CONFIG[\"image_size\"]).to(device)\n","\n","    # Configurar rutas\n","    onnx_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}.onnx\")\n","\n","    # Exportar a ONNX\n","    torch.onnx.export(\n","        inference_model,\n","        dummy_input,\n","        onnx_path,\n","        export_params=True,\n","        opset_version=12,\n","        input_names=['input'],\n","        output_names=['output'],\n","        dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n","    )\n","\n","    logger.info(f\"Modelo exportado en formato ONNX: {onnx_path}\")\n","except Exception as e:\n","    logger.warning(f\"No se pudo exportar a ONNX: {str(e)}\")\n","    logger.info(\"Puede ser debido a compatibilidad con la arquitectura. Esto no afectará al uso del modelo en PyTorch.\")\n","\n","# Mostrar rutas de los modelos exportados\n","export_paths = {\n","    'pytorch_model': model_path,\n","    'inference_model': inference_model_path,\n","    'huggingface_model': save_dir,\n","    'onnx_model': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}.onnx\"),\n","    'config': os.path.join(CONFIG[\"output_dir\"], \"config.json\"),\n","    'inference_script': os.path.join(CONFIG[\"output_dir\"], \"inference_example.py\")\n","}\n","\n","logger.info(\"Rutas de los modelos exportados:\")\n","for key, path in export_paths.items():\n","    logger.info(f\"  - {key}: {path}\")"],"metadata":{"id":"7P-3ujPEoSLR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== PRUEBA DE INFERENCIA ==============================\n","\n","logger.info(\"Realizando pruebas de inferencia en muestras\")\n","\n","# Comprobar si ya tenemos el modelo cargado\n","try:\n","    # Intentar acceder al modelo\n","    model\n","    # Asegurarse de que tiene cargado el mejor modelo de fine-tuning\n","    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","except NameError:\n","    # Si el modelo no está definido, crear uno nuevo y cargarlo\n","    logger.info(\"Cargando modelo desde checkpoint\")\n","    model = TimesformerForVideoClassification.from_pretrained(\n","        CONFIG[\"pretrained_model\"],\n","        num_frames=CONFIG[\"num_frames\"],\n","        image_size=CONFIG[\"image_size\"],\n","    )\n","\n","    # Adaptarlo a nuestra tarea\n","    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(CONFIG[\"tl_dropout\"]),\n","            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n","        )\n","\n","    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","\n","# Asegurar que el modelo está en el dispositivo correcto\n","model.to(device)\n","model.eval()\n","\n","# Cargar dataset de prueba\n","test_dataset = ViolenceVideoDataset(\n","    root_dir=CONFIG[\"dataset_path\"],\n","    split='test',\n","    num_frames=CONFIG[\"num_frames\"],\n","    image_size=CONFIG[\"image_size\"]\n",")\n","\n","# Seleccionar algunas muestras aleatorias\n","num_samples = min(5, len(test_dataset))\n","sample_indices = random.sample(range(len(test_dataset)), num_samples)\n","\n","# Resultados\n","results = []\n","\n","# Crear figura para visualización\n","fig, axes = plt.subplots(num_samples, 2, figsize=(12, 4*num_samples))\n","if num_samples == 1:\n","    axes = axes.reshape(1, 2)\n","\n","for i, idx in enumerate(sample_indices):\n","    try:\n","        # Obtener muestra\n","        sample = test_dataset[idx]\n","        pixel_values = sample['pixel_values'].unsqueeze(0).to(device)  # Añadir dimensión de batch\n","        label = sample['labels'].item()\n","        video_path = sample['video_path']\n","\n","        # Inferencia\n","        with torch.no_grad():\n","            outputs = model(pixel_values=pixel_values)\n","            logits = outputs.logits\n","            probs = torch.softmax(logits, dim=1)\n","            violence_prob = probs[0, 1].item()\n","            prediction = violence_prob >= CONFIG[\"threshold\"]\n","\n","        # Extraer un frame para visualización\n","        video_reader = VideoReader(video_path, ctx=cpu(0))\n","        mid_frame_idx = len(video_reader) // 2\n","        frame = video_reader[mid_frame_idx].asnumpy()\n","\n","        # Guardar resultado\n","        results.append({\n","            'video_path': video_path,\n","            'true_label': label,\n","            'violence_prob': violence_prob,\n","            'prediction': prediction,\n","            'correct': (prediction == label)\n","        })\n","\n","        # Visualizar\n","        axes[i, 0].imshow(frame)\n","        axes[i, 0].set_title(f\"Video: {os.path.basename(video_path)}\")\n","        axes[i, 0].axis('off')\n","\n","        # Graficar probabilidad\n","        bar_colors = ['green', 'red']\n","        class_names = ['No Violencia', 'Violencia']\n","        class_probs = [1 - violence_prob, violence_prob]\n","\n","        axes[i, 1].barh(class_names, class_probs, color=bar_colors)\n","        axes[i, 1].set_xlim(0, 1)\n","        axes[i, 1].set_title(f\"Predicción: {'Violencia' if prediction else 'No Violencia'} \" +\n","                          f\"(Real: {'Violencia' if label else 'No Violencia'})\")\n","        axes[i, 1].axvline(x=CONFIG[\"threshold\"], color='black', linestyle='--',\n","                      label=f'Umbral: {CONFIG[\"threshold\"]:.2f}')\n","        axes[i, 1].legend()\n","\n","    except Exception as e:\n","        logger.error(f\"Error al procesar muestra {idx}: {str(e)}\")\n","        # En caso de error, dejar la posición vacía\n","        axes[i, 0].axis('off')\n","        axes[i, 1].axis('off')\n","        continue\n","\n","plt.tight_layout()\n","plt.savefig(os.path.join(CONFIG[\"output_dir\"], \"inference_samples.png\"))\n","plt.close()\n","\n","# Guardar resultados\n","if results:\n","    results_df = pd.DataFrame(results)\n","    results_df.to_csv(os.path.join(CONFIG[\"output_dir\"], \"inference_samples_results.csv\"), index=False)\n","\n","    # Mostrar resumen\n","    correct_count = sum(1 for r in results if r['correct'])\n","    logger.info(f\"Precisión en muestras de prueba: {correct_count}/{len(results)} ({100 * correct_count / len(results):.1f}%)\")\n","else:\n","    logger.warning(\"No se pudieron procesar muestras para pruebas de inferencia\")"],"metadata":{"id":"4LaslpW1oc-x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== MEDICIÓN DE RENDIMIENTO ==============================\n","\n","logger.info(\"Realizando benchmark de velocidad de inferencia\")\n","\n","# Asegurarse de que el modelo está cargado\n","try:\n","    # Intentar acceder al modelo\n","    model\n","    # Configurar para evaluación\n","    model.eval()\n","except NameError:\n","    # Si el modelo no está definido, cargar el mejor de fine-tuning\n","    logger.info(\"Cargando modelo desde checkpoint\")\n","    model = TimesformerForVideoClassification.from_pretrained(\n","        CONFIG[\"pretrained_model\"],\n","        num_frames=CONFIG[\"num_frames\"],\n","        image_size=CONFIG[\"image_size\"],\n","    )\n","\n","    # Adaptarlo a nuestra tarea\n","    if model.classifier.out_features != CONFIG[\"num_classes\"]:\n","        model.classifier = nn.Sequential(\n","            nn.Dropout(CONFIG[\"tl_dropout\"]),\n","            nn.Linear(model.classifier.in_features, CONFIG[\"num_classes\"])\n","        )\n","\n","    best_ft_model_path = os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_best_ft.pt\")\n","    checkpoint = torch.load(best_ft_model_path)\n","    model.load_state_dict(checkpoint['model_state_dict'])\n","    model.eval()\n","\n","# Asegurar que el modelo está en el dispositivo correcto\n","model.to(device)\n","\n","# Crear datos de ejemplo\n","dummy_input = torch.randn(1, 3, CONFIG[\"num_frames\"], CONFIG[\"image_size\"], CONFIG[\"image_size\"]).to(device)\n","\n","# Calentar la GPU\n","logger.info(\"Calentando GPU...\")\n","with torch.no_grad():\n","    for _ in range(10):\n","        _ = model(pixel_values=dummy_input)\n","\n","# Medir tiempo de inferencia\n","num_runs = 100\n","logger.info(f\"Midiendo tiempo para {num_runs} ejecuciones...\")\n","\n","start_time = time.time()\n","with torch.no_grad():\n","    for _ in range(num_runs):\n","        _ = model(pixel_values=dummy_input)\n","end_time = time.time()\n","\n","# Calcular métricas\n","total_time = end_time - start_time\n","avg_time_per_inference = total_time / num_runs\n","fps = num_runs / total_time\n","\n","logger.info(f\"Tiempo total para {num_runs} ejecuciones: {total_time:.4f} segundos\")\n","logger.info(f\"Tiempo promedio por inferencia: {avg_time_per_inference*1000:.2f} ms\")\n","logger.info(f\"FPS (frames por segundo): {fps:.2f}\")\n","\n","# Guardar resultados\n","benchmark_results = {\n","    'total_time': total_time,\n","    'num_runs': num_runs,\n","    'avg_time_per_inference_ms': avg_time_per_inference * 1000,\n","    'fps': fps,\n","    'batch_size': 1,\n","    'num_frames': CONFIG[\"num_frames\"],\n","    'image_size': CONFIG[\"image_size\"],\n","    'device': str(device)\n","}\n","\n","with open(os.path.join(CONFIG[\"output_dir\"], \"benchmark_results.json\"), 'w') as f:\n","    json.dump(benchmark_results, f, indent=4)\n","\n","logger.info(\"Benchmark completado y resultados guardados.\")"],"metadata":{"id":"8YYbU2Fdofjp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ============================== RESUMEN FINAL DE MÉTRICAS ==============================\n","\n","logger.info(\"=== RESUMEN FINAL DE MÉTRICAS ===\")\n","\n","# Intentar cargar informes de evaluación\n","try:\n","    # Transfer Learning\n","    tl_report_path = os.path.join(CONFIG[\"output_dir\"], \"evaluation_report_transfer_learning.json\")\n","    with open(tl_report_path, 'r') as f:\n","        tl_report = json.load(f)\n","\n","    # Fine-Tuning\n","    ft_report_path = os.path.join(CONFIG[\"output_dir\"], \"evaluation_report_fine_tuning.json\")\n","    with open(ft_report_path, 'r') as f:\n","        ft_report = json.load(f)\n","\n","    # Test\n","    test_report_path = os.path.join(CONFIG[\"output_dir\"], \"evaluation_report_test.json\")\n","    with open(test_report_path, 'r') as f:\n","        test_report = json.load(f)\n","\n","    # Mostrar métricas\n","    logger.info(\"Métricas en Transfer Learning (validación):\")\n","    logger.info(f\"  - Accuracy: {tl_report['metrics']['accuracy']:.4f}\")\n","    logger.info(f\"  - Precision: {tl_report['metrics']['precision']:.4f}\")\n","    logger.info(f\"  - Recall (Sensibilidad): {tl_report['metrics']['recall']:.4f}\")\n","    logger.info(f\"  - Specificity: {tl_report['metrics']['specificity']:.4f}\")\n","    logger.info(f\"  - F1-Score: {tl_report['metrics']['f1_score']:.4f}\")\n","    logger.info(f\"  - ROC AUC: {tl_report['metrics']['roc_auc']:.4f}\")\n","\n","    logger.info(\"Métricas en Fine-Tuning (validación):\")\n","    logger.info(f\"  - Accuracy: {ft_report['metrics']['accuracy']:.4f}\")\n","    logger.info(f\"  - Precision: {ft_report['metrics']['precision']:.4f}\")\n","    logger.info(f\"  - Recall (Sensibilidad): {ft_report['metrics']['recall']:.4f}\")\n","    logger.info(f\"  - Specificity: {ft_report['metrics']['specificity']:.4f}\")\n","    logger.info(f\"  - F1-Score: {ft_report['metrics']['f1_score']:.4f}\")\n","    logger.info(f\"  - ROC AUC: {ft_report['metrics']['roc_auc']:.4f}\")\n","\n","    logger.info(\"Métricas en Test (final):\")\n","    logger.info(f\"  - Accuracy: {test_report['metrics']['accuracy']:.4f}\")\n","    logger.info(f\"  - Precision: {test_report['metrics']['precision']:.4f}\")\n","    logger.info(f\"  - Recall (Sensibilidad): {test_report['metrics']['recall']:.4f}\")\n","    logger.info(f\"  - Specificity: {test_report['metrics']['specificity']:.4f}\")\n","    logger.info(f\"  - F1-Score: {test_report['metrics']['f1_score']:.4f}\")\n","    logger.info(f\"  - ROC AUC: {test_report['metrics']['roc_auc']:.4f}\")\n","\n","    # Crear tabla comparativa\n","    metrics = ['accuracy', 'precision', 'recall', 'specificity', 'f1_score', 'roc_auc']\n","    data = {\n","        'Métrica': metrics,\n","        'Transfer Learning': [tl_report['metrics'][m] for m in metrics],\n","        'Fine-Tuning': [ft_report['metrics'][m] for m in metrics],\n","        'Test': [test_report['metrics'][m] for m in metrics]\n","    }\n","\n","    df = pd.DataFrame(data)\n","\n","    # Formatear para mostrar resultados\n","    pd.set_option('display.max_columns', None)\n","    pd.set_option('display.width', 120)\n","    pd.set_option('display.precision', 4)\n","\n","    print(\"\\n=== TABLA COMPARATIVA DE MÉTRICAS ===\")\n","    print(df)\n","\n","    # Guardar tabla\n","    df.to_csv(os.path.join(CONFIG[\"output_dir\"], \"metrics_comparison.csv\"), index=False)\n","\n","except Exception as e:\n","    logger.error(f\"Error al cargar informes de evaluación: {str(e)}\")\n","    logger.info(\"Asegúrate de que las fases de Transfer Learning, Fine-Tuning y Test ya se han ejecutado.\")\n","\n","# Mostrar información sobre el modelo final\n","try:\n","    # Cargar información del benchmark\n","    benchmark_path = os.path.join(CONFIG[\"output_dir\"], \"benchmark_results.json\")\n","    with open(benchmark_path, 'r') as f:\n","        benchmark = json.load(f)\n","\n","    logger.info(\"\\nRendimiento del modelo:\")\n","    logger.info(f\"  - Tiempo por inferencia: {benchmark['avg_time_per_inference_ms']:.2f} ms\")\n","    logger.info(f\"  - Frames por segundo: {benchmark['fps']:.2f} FPS\")\n","\n","    # Cargar umbrales óptimos\n","    thresholds_path = os.path.join(CONFIG[\"output_dir\"], \"optimal_thresholds.json\")\n","    with open(thresholds_path, 'r') as f:\n","        thresholds = json.load(f)\n","\n","    logger.info(\"\\nUmbrales óptimos:\")\n","    logger.info(f\"  - Umbral óptimo según F1: {thresholds['f1_optimal']:.4f}\")\n","    logger.info(f\"  - Umbral óptimo según ROC: {thresholds['roc_optimal']:.4f}\")\n","\n","except Exception as e:\n","    logger.error(f\"Error al cargar información de rendimiento: {str(e)}\")\n","\n","# Mostrar rutas de los modelos exportados\n","try:\n","    model_paths = {\n","        'Modelo PyTorch': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_final.pt\"),\n","        'Modelo de Inferencia': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_inference.pt\"),\n","        'Modelo Hugging Face': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}_hf\"),\n","        'Modelo ONNX': os.path.join(CONFIG[\"output_dir\"], f\"{CONFIG['model_name']}.onnx\"),\n","        'Script de Inferencia': os.path.join(CONFIG[\"output_dir\"], \"inference_example.py\")\n","    }\n","\n","    logger.info(\"\\nModelos exportados:\")\n","    for name, path in model_paths.items():\n","        exists = \"✓\" if os.path.exists(path) else \"✗\"\n","        logger.info(f\"  - {name}: {path} {exists}\")\n","\n","except Exception as e:\n","    logger.error(f\"Error al verificar rutas de modelos: {str(e)}\")\n","\n","logger.info(\"\\n¡Entrenamiento y evaluación del modelo TimeSformer para detección de violencia completados!\")"],"metadata":{"id":"GcqFvvp4ohU5"},"execution_count":null,"outputs":[]}]}