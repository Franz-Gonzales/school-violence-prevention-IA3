{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPzZsb5m3IvpbL/vTtPAmA8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wMWkDTqgXy6w"},"outputs":[],"source":["# coding=utf-8\n","# Copyright 2022 Meta and The HuggingFace Inc. team. All rights reserved.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\"PyTorch TimeSformer model.\"\"\"\n","\n","import collections\n","from typing import Optional, Tuple, Union\n","\n","import torch\n","import torch.nn.functional\n","import torch.utils.checkpoint\n","from torch import nn\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","\n","from ...activations import ACT2FN\n","from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n","from ...modeling_utils import PreTrainedModel\n","from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n","from .configuration_timesformer import TimesformerConfig\n","\n","\n","logger = logging.get_logger(__name__)\n","\n","_CONFIG_FOR_DOC = \"TimesformerConfig\"\n","_CHECKPOINT_FOR_DOC = \"facebook/timesformer\"\n","\n","\n","# Adapted from https://github.com/facebookresearch/TimeSformer/blob/a5ef29a7b7264baff199a30b3306ac27de901133/timesformer/models/vit.py#L155\n","class TimesformerPatchEmbeddings(nn.Module):\n","    \"\"\"Image to Patch Embedding\"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","\n","        image_size = config.image_size\n","        patch_size = config.patch_size\n","\n","        image_size = image_size if isinstance(image_size, collections.abc.Iterable) else (image_size, image_size)\n","        patch_size = patch_size if isinstance(patch_size, collections.abc.Iterable) else (patch_size, patch_size)\n","\n","        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n","        self.image_size = image_size\n","        self.patch_size = patch_size\n","        self.num_patches = num_patches\n","\n","        self.projection = nn.Conv2d(config.num_channels, config.hidden_size, kernel_size=patch_size, stride=patch_size)\n","\n","    def forward(self, pixel_values):\n","        batch_size, num_frames, num_channels, height, width = pixel_values.shape\n","        pixel_values = pixel_values.reshape(batch_size * num_frames, num_channels, height, width)\n","\n","        embeddings = self.projection(pixel_values)\n","        patch_width = embeddings.size(-1)\n","        embeddings = embeddings.flatten(2).transpose(1, 2)\n","        return embeddings, num_frames, patch_width\n","\n","\n","class TimesformerEmbeddings(nn.Module):\n","    \"\"\"\n","    Construct the patch and position embeddings.\n","    \"\"\"\n","\n","    def __init__(self, config):\n","        super().__init__()\n","\n","        embed_dim = config.hidden_size\n","        num_frames = config.num_frames\n","        drop_rate = config.hidden_dropout_prob\n","        attention_type = config.attention_type\n","\n","        self.attention_type = attention_type\n","        self.patch_embeddings = TimesformerPatchEmbeddings(config)\n","        self.num_patches = self.patch_embeddings.num_patches\n","\n","        # Positional Embeddings\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n","        self.position_embeddings = nn.Parameter(torch.zeros(1, self.num_patches + 1, embed_dim))\n","        self.pos_drop = nn.Dropout(p=drop_rate)\n","        if attention_type != \"space_only\":\n","            self.time_embeddings = nn.Parameter(torch.zeros(1, num_frames, embed_dim))\n","            self.time_drop = nn.Dropout(p=drop_rate)\n","\n","    def forward(self, pixel_values):\n","        batch_size = pixel_values.shape[0]\n","\n","        # create patch embeddings\n","        embeddings, num_frames, patch_width = self.patch_embeddings(pixel_values)\n","\n","        cls_tokens = self.cls_token.expand(embeddings.size(0), -1, -1)\n","        embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n","\n","        # resizing the positional embeddings in case they don't match the input at inference\n","        if embeddings.size(1) != self.position_embeddings.size(1):\n","            position_embeddings = self.position_embeddings\n","            cls_pos_embed = position_embeddings[0, 0, :].unsqueeze(0).unsqueeze(1)\n","            other_pos_embed = position_embeddings[0, 1:, :].unsqueeze(0).transpose(1, 2)\n","            patch_num = int(other_pos_embed.size(2) ** 0.5)\n","            patch_height = embeddings.size(1) // patch_width\n","            other_pos_embed = other_pos_embed.reshape(1, embeddings.size(2), patch_num, patch_num)\n","            new_pos_embed = nn.functional.interpolate(\n","                other_pos_embed, size=(patch_height, patch_width), mode=\"nearest\"\n","            )\n","            new_pos_embed = new_pos_embed.flatten(2)\n","            new_pos_embed = new_pos_embed.transpose(1, 2)\n","            new_pos_embed = torch.cat((cls_pos_embed, new_pos_embed), 1)\n","            embeddings = embeddings + new_pos_embed\n","        else:\n","            embeddings = embeddings + self.position_embeddings\n","        embeddings = self.pos_drop(embeddings)\n","\n","        # Time Embeddings\n","        if self.attention_type != \"space_only\":\n","            cls_tokens = embeddings[:batch_size, 0, :].unsqueeze(1)\n","            embeddings = embeddings[:, 1:]\n","            _, patch_height, patch_width = embeddings.shape\n","            embeddings = (\n","                embeddings.reshape(batch_size, num_frames, patch_height, patch_width)\n","                .permute(0, 2, 1, 3)\n","                .reshape(batch_size * patch_height, num_frames, patch_width)\n","            )\n","            # Resizing time embeddings in case they don't match\n","            if num_frames != self.time_embeddings.size(1):\n","                time_embeddings = self.time_embeddings.transpose(1, 2)\n","                new_time_embeddings = nn.functional.interpolate(time_embeddings, size=(num_frames), mode=\"nearest\")\n","                new_time_embeddings = new_time_embeddings.transpose(1, 2)\n","                embeddings = embeddings + new_time_embeddings\n","            else:\n","                embeddings = embeddings + self.time_embeddings\n","            embeddings = self.time_drop(embeddings)\n","            embeddings = embeddings.view(batch_size, patch_height, num_frames, patch_width).reshape(\n","                batch_size, patch_height * num_frames, patch_width\n","            )\n","            embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n","\n","        return embeddings\n","\n","\n","# Copied from transformers.models.beit.modeling_beit.drop_path\n","def drop_path(input: torch.Tensor, drop_prob: float = 0.0, training: bool = False) -> torch.Tensor:\n","    \"\"\"\n","    Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\n","\n","    Comment by Ross Wightman: This is the same as the DropConnect impl I created for EfficientNet, etc networks,\n","    however, the original name is misleading as 'Drop Connect' is a different form of dropout in a separate paper...\n","    See discussion: https://github.com/tensorflow/tpu/issues/494#issuecomment-532968956 ... I've opted for changing the\n","    layer and argument names to 'drop path' rather than mix DropConnect as a layer name and use 'survival rate' as the\n","    argument.\n","    \"\"\"\n","    if drop_prob == 0.0 or not training:\n","        return input\n","    keep_prob = 1 - drop_prob\n","    shape = (input.shape[0],) + (1,) * (input.ndim - 1)  # work with diff dim tensors, not just 2D ConvNets\n","    random_tensor = keep_prob + torch.rand(shape, dtype=input.dtype, device=input.device)\n","    random_tensor.floor_()  # binarize\n","    output = input.div(keep_prob) * random_tensor\n","    return output\n","\n","\n","# Copied from transformers.models.beit.modeling_beit.BeitDropPath with Beit->TimeSformer\n","class TimeSformerDropPath(nn.Module):\n","    \"\"\"Drop paths (Stochastic Depth) per sample (when applied in main path of residual blocks).\"\"\"\n","\n","    def __init__(self, drop_prob: Optional[float] = None) -> None:\n","        super().__init__()\n","        self.drop_prob = drop_prob\n","\n","    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n","        return drop_path(hidden_states, self.drop_prob, self.training)\n","\n","    def extra_repr(self) -> str:\n","        return \"p={}\".format(self.drop_prob)\n","\n","\n","# Adapted from https://github.com/facebookresearch/TimeSformer/blob/a5ef29a7b7264baff199a30b3306ac27de901133/timesformer/models/vit.py#L57\n","class TimesformerSelfAttention(nn.Module):\n","    def __init__(self, config: TimesformerConfig):\n","        super().__init__()\n","\n","        num_heads = config.num_attention_heads\n","        qkv_bias = config.qkv_bias\n","        attention_dropout_prob = config.attention_probs_dropout_prob\n","\n","        self.num_heads = num_heads\n","        head_dim = config.hidden_size // num_heads\n","        self.scale = head_dim**-0.5\n","        self.qkv = nn.Linear(config.hidden_size, config.hidden_size * 3, bias=qkv_bias)\n","        self.attn_drop = nn.Dropout(attention_dropout_prob)\n","\n","    def forward(self, hidden_states, output_attentions: bool = False):\n","        batch_size, hidden_size, num_channels = hidden_states.shape\n","        qkv = (\n","            self.qkv(hidden_states)\n","            .reshape(batch_size, hidden_size, 3, self.num_heads, num_channels // self.num_heads)\n","            .permute(2, 0, 3, 1, 4)\n","        )\n","        query, key, value = qkv[0], qkv[1], qkv[2]\n","\n","        attention_probs = (query @ key.transpose(-2, -1)) * self.scale\n","        attention_probs = attention_probs.softmax(dim=-1)\n","        attention_probs = self.attn_drop(attention_probs)\n","\n","        context_layer = (attention_probs @ value).transpose(1, 2).reshape(batch_size, hidden_size, num_channels)\n","\n","        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n","\n","        return outputs\n","\n","\n","class TimesformerSelfOutput(nn.Module):\n","    \"\"\"\n","    The residual connection is defined in TimesformerLayer instead of here (as is the case with other models), due to\n","    the layernorm applied before each block.\n","    \"\"\"\n","\n","    def __init__(self, config: TimesformerConfig) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","\n","        return hidden_states\n","\n","\n","class TimeSformerAttention(nn.Module):\n","    def __init__(self, config: TimesformerConfig) -> None:\n","        super().__init__()\n","        self.attention = TimesformerSelfAttention(config)\n","        self.output = TimesformerSelfOutput(config)\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        output_attentions: bool = False,\n","    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n","        self_outputs = self.attention(hidden_states, output_attentions)\n","\n","        attention_output = self.output(self_outputs[0])\n","\n","        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n","        return outputs\n","\n","\n","# Adapted from https://github.com/facebookresearch/TimeSformer/blob/a5ef29a7b7264baff199a30b3306ac27de901133/timesformer/models/vit.py#L39\n","class TimesformerIntermediate(nn.Module):\n","    def __init__(self, config: TimesformerConfig) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","        if isinstance(config.hidden_act, str):\n","            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n","        else:\n","            self.intermediate_act_fn = config.hidden_act\n","\n","    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.intermediate_act_fn(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","\n","        return hidden_states\n","\n","\n","class TimesformerOutput(nn.Module):\n","    def __init__(self, config: TimesformerConfig) -> None:\n","        super().__init__()\n","        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","\n","    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n","        hidden_states = self.dense(hidden_states)\n","        hidden_states = self.dropout(hidden_states)\n","\n","        return hidden_states\n","\n","\n","# Adapted from https://github.com/facebookresearch/TimeSformer/blob/a5ef29a7b7264baff199a30b3306ac27de901133/timesformer/models/vit.py#L89\n","class TimesformerLayer(nn.Module):\n","    def __init__(self, config: TimesformerConfig, layer_index: int) -> None:\n","        super().__init__()\n","\n","        attention_type = config.attention_type\n","\n","        drop_path_rates = [\n","            x.item() for x in torch.linspace(0, config.drop_path_rate, config.num_hidden_layers)\n","        ]  # stochastic depth decay rule\n","        drop_path_rate = drop_path_rates[layer_index]\n","\n","        self.drop_path = TimeSformerDropPath(drop_path_rate) if drop_path_rate > 0.0 else nn.Identity()\n","        self.attention = TimeSformerAttention(config)\n","        self.intermediate = TimesformerIntermediate(config)\n","        self.output = TimesformerOutput(config)\n","        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","\n","        self.config = config\n","        self.attention_type = attention_type\n","        if attention_type not in [\"divided_space_time\", \"space_only\", \"joint_space_time\"]:\n","            raise ValueError(\"Unknown attention type: {}\".format(attention_type))\n","\n","        # Temporal Attention Parameters\n","        if self.attention_type == \"divided_space_time\":\n","            self.temporal_layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","            self.temporal_attention = TimeSformerAttention(config)\n","            self.temporal_dense = nn.Linear(config.hidden_size, config.hidden_size)\n","\n","    def forward(self, hidden_states: torch.Tensor, output_attentions: bool = False):\n","        num_frames = self.config.num_frames\n","        num_patch_width = self.config.image_size // self.config.patch_size\n","        batch_size = hidden_states.shape[0]\n","        num_spatial_tokens = (hidden_states.size(1) - 1) // num_frames\n","        num_patch_height = num_spatial_tokens // num_patch_width\n","\n","        if self.attention_type in [\"space_only\", \"joint_space_time\"]:\n","            self_attention_outputs = self.attention(\n","                self.layernorm_before(hidden_states), output_attentions=output_attentions\n","            )\n","            attention_output = self_attention_outputs[0]\n","            outputs = self_attention_outputs[1:]  # add self attentions if we output attention weights\n","\n","            hidden_states = hidden_states + self.drop_path(attention_output)\n","\n","            layer_output = self.layernorm_after(hidden_states)\n","            layer_output = self.intermediate(layer_output)\n","            layer_output = self.output(layer_output)\n","            layer_output = hidden_states + self.drop_path(layer_output)\n","\n","            outputs = (layer_output,) + outputs\n","\n","            return outputs\n","\n","        elif self.attention_type == \"divided_space_time\":\n","            # Temporal\n","            temporal_embedding = hidden_states[:, 1:, :]\n","            temporal_embedding = temporal_embedding.reshape(\n","                batch_size, num_patch_height, num_patch_width, num_frames, temporal_embedding.shape[2]\n","            ).reshape(batch_size * num_patch_height * num_patch_width, num_frames, temporal_embedding.shape[2])\n","\n","            temporal_attention_outputs = self.temporal_attention(\n","                self.temporal_layernorm(temporal_embedding),\n","            )\n","            attention_output = temporal_attention_outputs[0]\n","\n","            residual_temporal = self.drop_path(attention_output)\n","\n","            residual_temporal = residual_temporal.reshape(\n","                batch_size, num_patch_height, num_patch_width, num_frames, residual_temporal.shape[2]\n","            ).reshape(batch_size, num_patch_height * num_patch_width * num_frames, residual_temporal.shape[2])\n","            residual_temporal = self.temporal_dense(residual_temporal)\n","            temporal_embedding = hidden_states[:, 1:, :] + residual_temporal\n","\n","            # Spatial\n","            init_cls_token = hidden_states[:, 0, :].unsqueeze(1)\n","            cls_token = init_cls_token.repeat(1, num_frames, 1)\n","            cls_token = cls_token.reshape(batch_size * num_frames, 1, cls_token.shape[2])\n","            spatial_embedding = temporal_embedding\n","            spatial_embedding = (\n","                spatial_embedding.reshape(\n","                    batch_size, num_patch_height, num_patch_width, num_frames, spatial_embedding.shape[2]\n","                )\n","                .permute(0, 3, 1, 2, 4)\n","                .reshape(batch_size * num_frames, num_patch_height * num_patch_width, spatial_embedding.shape[2])\n","            )\n","            spatial_embedding = torch.cat((cls_token, spatial_embedding), 1)\n","\n","            spatial_attention_outputs = self.attention(\n","                self.layernorm_before(spatial_embedding), output_attentions=output_attentions\n","            )\n","            attention_output = spatial_attention_outputs[0]\n","            outputs = spatial_attention_outputs[1:]  # add self attentions if we output attention weights\n","\n","            residual_spatial = self.drop_path(attention_output)\n","\n","            # Taking care of CLS token\n","            cls_token = residual_spatial[:, 0, :]\n","            cls_token = cls_token.reshape(batch_size, num_frames, cls_token.shape[1])\n","            cls_token = torch.mean(cls_token, 1, True)  # averaging for every frame\n","            residual_spatial = residual_spatial[:, 1:, :]\n","            residual_spatial = (\n","                residual_spatial.reshape(\n","                    batch_size, num_frames, num_patch_height, num_patch_width, residual_spatial.shape[2]\n","                )\n","                .permute(0, 2, 3, 1, 4)\n","                .reshape(batch_size, num_patch_height * num_patch_width * num_frames, residual_spatial.shape[2])\n","            )\n","            residual = residual_spatial\n","            hidden_states = temporal_embedding\n","\n","            # Mlp\n","            hidden_states = torch.cat((init_cls_token, hidden_states), 1) + torch.cat((cls_token, residual), 1)\n","            layer_output = self.layernorm_after(hidden_states)\n","            layer_output = self.intermediate(layer_output)\n","            layer_output = self.output(layer_output)\n","            layer_output = hidden_states + self.drop_path(layer_output)\n","\n","            outputs = (layer_output,) + outputs\n","\n","            return outputs\n","\n","\n","class TimesformerEncoder(nn.Module):\n","    def __init__(self, config: TimesformerConfig) -> None:\n","        super().__init__()\n","        self.config = config\n","        self.layer = nn.ModuleList([TimesformerLayer(config, ind) for ind in range(config.num_hidden_layers)])\n","        self.gradient_checkpointing = False\n","\n","    def forward(\n","        self,\n","        hidden_states: torch.Tensor,\n","        output_attentions: bool = False,\n","        output_hidden_states: bool = False,\n","        return_dict: bool = True,\n","    ) -> Union[tuple, BaseModelOutput]:\n","        all_hidden_states = () if output_hidden_states else None\n","        all_self_attentions = () if output_attentions else None\n","\n","        for i, layer_module in enumerate(self.layer):\n","            if output_hidden_states:\n","                all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","            if self.gradient_checkpointing and self.training:\n","                layer_outputs = self._gradient_checkpointing_func(\n","                    layer_module.__call__,\n","                    hidden_states,\n","                    output_attentions,\n","                )\n","            else:\n","                layer_outputs = layer_module(hidden_states, output_attentions)\n","\n","            hidden_states = layer_outputs[0]\n","\n","            if output_attentions:\n","                all_self_attentions = all_self_attentions + (layer_outputs[1],)\n","\n","        if output_hidden_states:\n","            all_hidden_states = all_hidden_states + (hidden_states,)\n","\n","        if not return_dict:\n","            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n","        return BaseModelOutput(\n","            last_hidden_state=hidden_states,\n","            hidden_states=all_hidden_states,\n","            attentions=all_self_attentions,\n","        )\n","\n","\n","class TimesformerPreTrainedModel(PreTrainedModel):\n","    \"\"\"\n","    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n","    models.\n","    \"\"\"\n","\n","    config_class = TimesformerConfig\n","    base_model_prefix = \"timesformer\"\n","    main_input_name = \"pixel_values\"\n","    supports_gradient_checkpointing = True\n","    _no_split_modules = [\"TimesformerLayer\"]\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, (nn.Linear, nn.Conv2d)):\n","            nn.init.trunc_normal_(module.weight, std=self.config.initializer_range)\n","            if module.bias is not None:\n","                nn.init.constant_(module.bias, 0)\n","        elif isinstance(module, nn.LayerNorm):\n","            nn.init.constant_(module.bias, 0)\n","            nn.init.constant_(module.weight, 1.0)\n","        elif isinstance(module, TimesformerEmbeddings):\n","            nn.init.trunc_normal_(module.cls_token, std=self.config.initializer_range)\n","            nn.init.trunc_normal_(module.position_embeddings, std=self.config.initializer_range)\n","            module.patch_embeddings.apply(self._init_weights)\n","\n","\n","TIMESFORMER_START_DOCSTRING = r\"\"\"\n","    This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n","    as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and\n","    behavior.\n","\n","    Parameters:\n","        config ([`TimesformerConfig`]): Model configuration class with all the parameters of the model.\n","            Initializing with a config file does not load the weights associated with the model, only the\n","            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n","\"\"\"\n","\n","TIMESFORMER_INPUTS_DOCSTRING = r\"\"\"\n","    Args:\n","        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_frames, num_channels, height, width)`):\n","            Pixel values. Pixel values can be obtained using [`AutoImageProcessor`]. See\n","            [`VideoMAEImageProcessor.preprocess`] for details.\n","\n","        output_attentions (`bool`, *optional*):\n","            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n","            tensors for more detail.\n","        output_hidden_states (`bool`, *optional*):\n","            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n","            more detail.\n","        return_dict (`bool`, *optional*):\n","            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n","\"\"\"\n","\n","\n","@add_start_docstrings(\n","    \"The bare TimeSformer Model transformer outputting raw hidden-states without any specific head on top.\",\n","    TIMESFORMER_START_DOCSTRING,\n",")\n","class TimesformerModel(TimesformerPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.config = config\n","\n","        self.embeddings = TimesformerEmbeddings(config)\n","        self.encoder = TimesformerEncoder(config)\n","\n","        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def get_input_embeddings(self):\n","        return self.embeddings.patch_embeddings\n","\n","    def _prune_heads(self, heads_to_prune):\n","        \"\"\"\n","        Prunes heads of the model. heads_to_prune: dict of {layer_num: list of heads to prune in this layer} See base\n","        class PreTrainedModel\n","        \"\"\"\n","        for layer, heads in heads_to_prune.items():\n","            self.encoder.layer[layer].attention.prune_heads(heads)\n","\n","    @add_start_docstrings_to_model_forward(TIMESFORMER_INPUTS_DOCSTRING)\n","    @replace_return_docstrings(output_type=BaseModelOutput, config_class=_CONFIG_FOR_DOC)\n","    def forward(\n","        self,\n","        pixel_values: torch.FloatTensor,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple[torch.FloatTensor], BaseModelOutput]:\n","        r\"\"\"\n","        Returns:\n","\n","        Examples:\n","\n","        ```python\n","        >>> import av\n","        >>> import numpy as np\n","\n","        >>> from transformers import AutoImageProcessor, TimesformerModel\n","        >>> from huggingface_hub import hf_hub_download\n","\n","        >>> np.random.seed(0)\n","\n","\n","        >>> def read_video_pyav(container, indices):\n","        ...     '''\n","        ...     Decode the video with PyAV decoder.\n","        ...     Args:\n","        ...         container (`av.container.input.InputContainer`): PyAV container.\n","        ...         indices (`List[int]`): List of frame indices to decode.\n","        ...     Returns:\n","        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n","        ...     '''\n","        ...     frames = []\n","        ...     container.seek(0)\n","        ...     start_index = indices[0]\n","        ...     end_index = indices[-1]\n","        ...     for i, frame in enumerate(container.decode(video=0)):\n","        ...         if i > end_index:\n","        ...             break\n","        ...         if i >= start_index and i in indices:\n","        ...             frames.append(frame)\n","        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n","\n","\n","        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n","        ...     '''\n","        ...     Sample a given number of frame indices from the video.\n","        ...     Args:\n","        ...         clip_len (`int`): Total number of frames to sample.\n","        ...         frame_sample_rate (`int`): Sample every n-th frame.\n","        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n","        ...     Returns:\n","        ...         indices (`List[int]`): List of sampled frame indices\n","        ...     '''\n","        ...     converted_len = int(clip_len * frame_sample_rate)\n","        ...     end_idx = np.random.randint(converted_len, seg_len)\n","        ...     start_idx = end_idx - converted_len\n","        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n","        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n","        ...     return indices\n","\n","\n","        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n","        >>> file_path = hf_hub_download(\n","        ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n","        ... )\n","        >>> container = av.open(file_path)\n","\n","        >>> # sample 8 frames\n","        >>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=4, seg_len=container.streams.video[0].frames)\n","        >>> video = read_video_pyav(container, indices)\n","\n","        >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base\")\n","        >>> model = TimesformerModel.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n","\n","        >>> # prepare video for the model\n","        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n","\n","        >>> # forward pass\n","        >>> outputs = model(**inputs)\n","        >>> last_hidden_states = outputs.last_hidden_state\n","        >>> list(last_hidden_states.shape)\n","        [1, 1569, 768]\n","        ```\"\"\"\n","        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n","        output_hidden_states = (\n","            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n","        )\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        embedding_output = self.embeddings(pixel_values)\n","\n","        encoder_outputs = self.encoder(\n","            embedding_output,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        sequence_output = encoder_outputs[0]\n","        if self.layernorm is not None:\n","            sequence_output = self.layernorm(sequence_output)\n","\n","        if not return_dict:\n","            return (sequence_output,) + encoder_outputs[1:]\n","\n","        return BaseModelOutput(\n","            last_hidden_state=sequence_output,\n","            hidden_states=encoder_outputs.hidden_states,\n","            attentions=encoder_outputs.attentions,\n","        )\n","\n","\n","@add_start_docstrings(\n","    \"\"\"TimeSformer Model transformer with a video classification head on top (a linear layer on top of the final hidden state\n","of the [CLS] token) e.g. for ImageNet.\"\"\",\n","    TIMESFORMER_START_DOCSTRING,\n",")\n","class TimesformerForVideoClassification(TimesformerPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","\n","        self.num_labels = config.num_labels\n","        self.timesformer = TimesformerModel(config)\n","\n","        # Classifier head\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    @add_start_docstrings_to_model_forward(TIMESFORMER_INPUTS_DOCSTRING)\n","    @replace_return_docstrings(output_type=ImageClassifierOutput, config_class=_CONFIG_FOR_DOC)\n","    def forward(\n","        self,\n","        pixel_values: Optional[torch.Tensor] = None,\n","        labels: Optional[torch.Tensor] = None,\n","        output_attentions: Optional[bool] = None,\n","        output_hidden_states: Optional[bool] = None,\n","        return_dict: Optional[bool] = None,\n","    ) -> Union[Tuple, ImageClassifierOutput]:\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n","            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n","            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","\n","        Returns:\n","\n","        Examples:\n","\n","        ```python\n","        >>> import av\n","        >>> import torch\n","        >>> import numpy as np\n","\n","        >>> from transformers import AutoImageProcessor, TimesformerForVideoClassification\n","        >>> from huggingface_hub import hf_hub_download\n","\n","        >>> np.random.seed(0)\n","\n","\n","        >>> def read_video_pyav(container, indices):\n","        ...     '''\n","        ...     Decode the video with PyAV decoder.\n","        ...     Args:\n","        ...         container (`av.container.input.InputContainer`): PyAV container.\n","        ...         indices (`List[int]`): List of frame indices to decode.\n","        ...     Returns:\n","        ...         result (np.ndarray): np array of decoded frames of shape (num_frames, height, width, 3).\n","        ...     '''\n","        ...     frames = []\n","        ...     container.seek(0)\n","        ...     start_index = indices[0]\n","        ...     end_index = indices[-1]\n","        ...     for i, frame in enumerate(container.decode(video=0)):\n","        ...         if i > end_index:\n","        ...             break\n","        ...         if i >= start_index and i in indices:\n","        ...             frames.append(frame)\n","        ...     return np.stack([x.to_ndarray(format=\"rgb24\") for x in frames])\n","\n","\n","        >>> def sample_frame_indices(clip_len, frame_sample_rate, seg_len):\n","        ...     '''\n","        ...     Sample a given number of frame indices from the video.\n","        ...     Args:\n","        ...         clip_len (`int`): Total number of frames to sample.\n","        ...         frame_sample_rate (`int`): Sample every n-th frame.\n","        ...         seg_len (`int`): Maximum allowed index of sample's last frame.\n","        ...     Returns:\n","        ...         indices (`List[int]`): List of sampled frame indices\n","        ...     '''\n","        ...     converted_len = int(clip_len * frame_sample_rate)\n","        ...     end_idx = np.random.randint(converted_len, seg_len)\n","        ...     start_idx = end_idx - converted_len\n","        ...     indices = np.linspace(start_idx, end_idx, num=clip_len)\n","        ...     indices = np.clip(indices, start_idx, end_idx - 1).astype(np.int64)\n","        ...     return indices\n","\n","\n","        >>> # video clip consists of 300 frames (10 seconds at 30 FPS)\n","        >>> file_path = hf_hub_download(\n","        ...     repo_id=\"nielsr/video-demo\", filename=\"eating_spaghetti.mp4\", repo_type=\"dataset\"\n","        ... )\n","        >>> container = av.open(file_path)\n","\n","        >>> # sample 8 frames\n","        >>> indices = sample_frame_indices(clip_len=8, frame_sample_rate=1, seg_len=container.streams.video[0].frames)\n","        >>> video = read_video_pyav(container, indices)\n","\n","        >>> image_processor = AutoImageProcessor.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\")\n","        >>> model = TimesformerForVideoClassification.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n","\n","        >>> inputs = image_processor(list(video), return_tensors=\"pt\")\n","\n","        >>> with torch.no_grad():\n","        ...     outputs = model(**inputs)\n","        ...     logits = outputs.logits\n","\n","        >>> # model predicts one of the 400 Kinetics-400 classes\n","        >>> predicted_label = logits.argmax(-1).item()\n","        >>> print(model.config.id2label[predicted_label])\n","        eating spaghetti\n","        ```\"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.timesformer(\n","            pixel_values,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0][:, 0]\n","\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                if self.num_labels == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[1:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return ImageClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","\n","__all__ = [\"TimesformerModel\", \"TimesformerForVideoClassification\", \"TimesformerPreTrainedModel\"]\n"]},{"cell_type":"code","source":["# coding=utf-8\n","# Copyright 2022 The HuggingFace Inc. team.\n","#\n","# Licensed under the Apache License, Version 2.0 (the \"License\");\n","# you may not use this file except in compliance with the License.\n","# You may obtain a copy of the License at\n","#\n","#     http://www.apache.org/licenses/LICENSE-2.0\n","#\n","# Unless required by applicable law or agreed to in writing, software\n","# distributed under the License is distributed on an \"AS IS\" BASIS,\n","# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","# See the License for the specific language governing permissions and\n","# limitations under the License.\n","\"\"\"AutoImageProcessor class.\"\"\"\n","\n","import importlib\n","import json\n","import os\n","import warnings\n","from collections import OrderedDict\n","from typing import TYPE_CHECKING, Dict, Optional, Tuple, Union\n","\n","# Build the list of all image processors\n","from ...configuration_utils import PretrainedConfig\n","from ...dynamic_module_utils import get_class_from_dynamic_module, resolve_trust_remote_code\n","from ...image_processing_utils import ImageProcessingMixin\n","from ...image_processing_utils_fast import BaseImageProcessorFast\n","from ...utils import (\n","    CONFIG_NAME,\n","    IMAGE_PROCESSOR_NAME,\n","    cached_file,\n","    is_timm_config_dict,\n","    is_timm_local_checkpoint,\n","    is_torchvision_available,\n","    is_vision_available,\n","    logging,\n",")\n","from .auto_factory import _LazyAutoMapping\n","from .configuration_auto import (\n","    CONFIG_MAPPING_NAMES,\n","    AutoConfig,\n","    model_type_to_module_name,\n","    replace_list_option_in_docstrings,\n",")\n","\n","\n","logger = logging.get_logger(__name__)\n","\n","\n","if TYPE_CHECKING:\n","    # This significantly improves completion suggestion performance when\n","    # the transformers package is used with Microsoft's Pylance language server.\n","    IMAGE_PROCESSOR_MAPPING_NAMES: OrderedDict[str, Tuple[Optional[str], Optional[str]]] = OrderedDict()\n","else:\n","    IMAGE_PROCESSOR_MAPPING_NAMES = OrderedDict(\n","        [\n","            (\"align\", (\"EfficientNetImageProcessor\",)),\n","            (\"aria\", (\"AriaImageProcessor\",)),\n","            (\"beit\", (\"BeitImageProcessor\",)),\n","            (\"bit\", (\"BitImageProcessor\",)),\n","            (\"blip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n","            (\"blip-2\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n","            (\"bridgetower\", (\"BridgeTowerImageProcessor\",)),\n","            (\"chameleon\", (\"ChameleonImageProcessor\",)),\n","            (\"chinese_clip\", (\"ChineseCLIPImageProcessor\",)),\n","            (\"clip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n","            (\"clipseg\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"conditional_detr\", (\"ConditionalDetrImageProcessor\",)),\n","            (\"convnext\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n","            (\"convnextv2\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n","            (\"cvt\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n","            (\"data2vec-vision\", (\"BeitImageProcessor\",)),\n","            (\"deformable_detr\", (\"DeformableDetrImageProcessor\", \"DeformableDetrImageProcessorFast\")),\n","            (\"deit\", (\"DeiTImageProcessor\", \"DeiTImageProcessorFast\")),\n","            (\"depth_anything\", (\"DPTImageProcessor\",)),\n","            (\"depth_pro\", (\"DepthProImageProcessor\", \"DepthProImageProcessorFast\")),\n","            (\"deta\", (\"DetaImageProcessor\",)),\n","            (\"detr\", (\"DetrImageProcessor\", \"DetrImageProcessorFast\")),\n","            (\"dinat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"dinov2\", (\"BitImageProcessor\",)),\n","            (\"donut-swin\", (\"DonutImageProcessor\",)),\n","            (\"dpt\", (\"DPTImageProcessor\",)),\n","            (\"efficientformer\", (\"EfficientFormerImageProcessor\",)),\n","            (\"efficientnet\", (\"EfficientNetImageProcessor\",)),\n","            (\"flava\", (\"FlavaImageProcessor\",)),\n","            (\"focalnet\", (\"BitImageProcessor\",)),\n","            (\"fuyu\", (\"FuyuImageProcessor\",)),\n","            (\"gemma3\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n","            (\"git\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n","            (\"glpn\", (\"GLPNImageProcessor\",)),\n","            (\"got_ocr2\", (\"GotOcr2ImageProcessor\", \"GotOcr2ImageProcessorFast\")),\n","            (\"grounding-dino\", (\"GroundingDinoImageProcessor\",)),\n","            (\"groupvit\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n","            (\"hiera\", (\"BitImageProcessor\",)),\n","            (\"idefics\", (\"IdeficsImageProcessor\",)),\n","            (\"idefics2\", (\"Idefics2ImageProcessor\",)),\n","            (\"idefics3\", (\"Idefics3ImageProcessor\",)),\n","            (\"ijepa\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"imagegpt\", (\"ImageGPTImageProcessor\",)),\n","            (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n","            (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\",)),\n","            (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n","            (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\",)),\n","            (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\",)),\n","            (\"levit\", (\"LevitImageProcessor\",)),\n","            (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),\n","            (\"llava\", (\"LlavaImageProcessor\", \"LlavaImageProcessorFast\")),\n","            (\"llava_next\", (\"LlavaNextImageProcessor\", \"LlavaNextImageProcessorFast\")),\n","            (\"llava_next_video\", (\"LlavaNextVideoImageProcessor\",)),\n","            (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\", \"LlavaOnevisionImageProcessorFast\")),\n","            (\"mask2former\", (\"Mask2FormerImageProcessor\",)),\n","            (\"maskformer\", (\"MaskFormerImageProcessor\",)),\n","            (\"mgp-str\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"mistral3\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n","            (\"mllama\", (\"MllamaImageProcessor\",)),\n","            (\"mobilenet_v1\", (\"MobileNetV1ImageProcessor\",)),\n","            (\"mobilenet_v2\", (\"MobileNetV2ImageProcessor\",)),\n","            (\"mobilevit\", (\"MobileViTImageProcessor\",)),\n","            (\"mobilevitv2\", (\"MobileViTImageProcessor\",)),\n","            (\"nat\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"nougat\", (\"NougatImageProcessor\",)),\n","            (\"oneformer\", (\"OneFormerImageProcessor\",)),\n","            (\"owlv2\", (\"Owlv2ImageProcessor\",)),\n","            (\"owlvit\", (\"OwlViTImageProcessor\",)),\n","            (\"paligemma\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n","            (\"perceiver\", (\"PerceiverImageProcessor\",)),\n","            (\"phi4_multimodal\", \"Phi4MultimodalImageProcessorFast\"),\n","            (\"pix2struct\", (\"Pix2StructImageProcessor\",)),\n","            (\"pixtral\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n","            (\"poolformer\", (\"PoolFormerImageProcessor\",)),\n","            (\"prompt_depth_anything\", (\"PromptDepthAnythingImageProcessor\",)),\n","            (\"pvt\", (\"PvtImageProcessor\",)),\n","            (\"pvt_v2\", (\"PvtImageProcessor\",)),\n","            (\"qwen2_5_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n","            (\"qwen2_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n","            (\"regnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n","            (\"resnet\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n","            (\"rt_detr\", (\"RTDetrImageProcessor\", \"RTDetrImageProcessorFast\")),\n","            (\"sam\", (\"SamImageProcessor\",)),\n","            (\"segformer\", (\"SegformerImageProcessor\",)),\n","            (\"seggpt\", (\"SegGptImageProcessor\",)),\n","            (\"shieldgemma2\", (\"Gemma3ImageProcessor\", \"Gemma3ImageProcessorFast\")),\n","            (\"siglip\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n","            (\"siglip2\", (\"Siglip2ImageProcessor\", \"Siglip2ImageProcessorFast\")),\n","            (\"superglue\", (\"SuperGlueImageProcessor\",)),\n","            (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"swin2sr\", (\"Swin2SRImageProcessor\",)),\n","            (\"swinv2\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"table-transformer\", (\"DetrImageProcessor\",)),\n","            (\"timesformer\", (\"VideoMAEImageProcessor\",)),\n","            (\"timm_wrapper\", (\"TimmWrapperImageProcessor\",)),\n","            (\"tvlt\", (\"TvltImageProcessor\",)),\n","            (\"tvp\", (\"TvpImageProcessor\",)),\n","            (\"udop\", (\"LayoutLMv3ImageProcessor\",)),\n","            (\"upernet\", (\"SegformerImageProcessor\",)),\n","            (\"van\", (\"ConvNextImageProcessor\", \"ConvNextImageProcessorFast\")),\n","            (\"videomae\", (\"VideoMAEImageProcessor\",)),\n","            (\"vilt\", (\"ViltImageProcessor\",)),\n","            (\"vipllava\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n","            (\"vit\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"vit_hybrid\", (\"ViTHybridImageProcessor\",)),\n","            (\"vit_mae\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"vit_msn\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n","            (\"vitmatte\", (\"VitMatteImageProcessor\",)),\n","            (\"xclip\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n","            (\"yolos\", (\"YolosImageProcessor\",)),\n","            (\"zoedepth\", (\"ZoeDepthImageProcessor\",)),\n","        ]\n","    )\n","\n","for model_type, image_processors in IMAGE_PROCESSOR_MAPPING_NAMES.items():\n","    slow_image_processor_class, *fast_image_processor_class = image_processors\n","    if not is_vision_available():\n","        slow_image_processor_class = None\n","\n","    # If the fast image processor is not defined, or torchvision is not available, we set it to None\n","    if not fast_image_processor_class or fast_image_processor_class[0] is None or not is_torchvision_available():\n","        fast_image_processor_class = None\n","    else:\n","        fast_image_processor_class = fast_image_processor_class[0]\n","\n","    IMAGE_PROCESSOR_MAPPING_NAMES[model_type] = (slow_image_processor_class, fast_image_processor_class)\n","\n","IMAGE_PROCESSOR_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, IMAGE_PROCESSOR_MAPPING_NAMES)\n","\n","\n","def get_image_processor_class_from_name(class_name: str):\n","    if class_name == \"BaseImageProcessorFast\":\n","        return BaseImageProcessorFast\n","\n","    for module_name, extractors in IMAGE_PROCESSOR_MAPPING_NAMES.items():\n","        if class_name in extractors:\n","            module_name = model_type_to_module_name(module_name)\n","\n","            module = importlib.import_module(f\".{module_name}\", \"transformers.models\")\n","            try:\n","                return getattr(module, class_name)\n","            except AttributeError:\n","                continue\n","\n","    for _, extractors in IMAGE_PROCESSOR_MAPPING._extra_content.items():\n","        for extractor in extractors:\n","            if getattr(extractor, \"__name__\", None) == class_name:\n","                return extractor\n","\n","    # We did not find the class, but maybe it's because a dep is missing. In that case, the class will be in the main\n","    # init and we return the proper dummy to get an appropriate error message.\n","    main_module = importlib.import_module(\"transformers\")\n","    if hasattr(main_module, class_name):\n","        return getattr(main_module, class_name)\n","\n","    return None\n","\n","\n","def get_image_processor_config(\n","    pretrained_model_name_or_path: Union[str, os.PathLike],\n","    cache_dir: Optional[Union[str, os.PathLike]] = None,\n","    force_download: bool = False,\n","    resume_download: Optional[bool] = None,\n","    proxies: Optional[Dict[str, str]] = None,\n","    token: Optional[Union[bool, str]] = None,\n","    revision: Optional[str] = None,\n","    local_files_only: bool = False,\n","    **kwargs,\n","):\n","    \"\"\"\n","    Loads the image processor configuration from a pretrained model image processor configuration.\n","\n","    Args:\n","        pretrained_model_name_or_path (`str` or `os.PathLike`):\n","            This can be either:\n","\n","            - a string, the *model id* of a pretrained model configuration hosted inside a model repo on\n","              huggingface.co.\n","            - a path to a *directory* containing a configuration file saved using the\n","              [`~PreTrainedTokenizer.save_pretrained`] method, e.g., `./my_model_directory/`.\n","\n","        cache_dir (`str` or `os.PathLike`, *optional*):\n","            Path to a directory in which a downloaded pretrained model configuration should be cached if the standard\n","            cache should not be used.\n","        force_download (`bool`, *optional*, defaults to `False`):\n","            Whether or not to force to (re-)download the configuration files and override the cached versions if they\n","            exist.\n","        resume_download:\n","            Deprecated and ignored. All downloads are now resumed by default when possible.\n","            Will be removed in v5 of Transformers.\n","        proxies (`Dict[str, str]`, *optional*):\n","            A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n","            'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n","        token (`str` or *bool*, *optional*):\n","            The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n","            when running `huggingface-cli login` (stored in `~/.huggingface`).\n","        revision (`str`, *optional*, defaults to `\"main\"`):\n","            The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n","            git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n","            identifier allowed by git.\n","        local_files_only (`bool`, *optional*, defaults to `False`):\n","            If `True`, will only try to load the image processor configuration from local files.\n","\n","    <Tip>\n","\n","    Passing `token=True` is required when you want to use a private model.\n","\n","    </Tip>\n","\n","    Returns:\n","        `Dict`: The configuration of the image processor.\n","\n","    Examples:\n","\n","    ```python\n","    # Download configuration from huggingface.co and cache.\n","    image_processor_config = get_image_processor_config(\"google-bert/bert-base-uncased\")\n","    # This model does not have a image processor config so the result will be an empty dict.\n","    image_processor_config = get_image_processor_config(\"FacebookAI/xlm-roberta-base\")\n","\n","    # Save a pretrained image processor locally and you can reload its config\n","    from transformers import AutoTokenizer\n","\n","    image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","    image_processor.save_pretrained(\"image-processor-test\")\n","    image_processor_config = get_image_processor_config(\"image-processor-test\")\n","    ```\"\"\"\n","    use_auth_token = kwargs.pop(\"use_auth_token\", None)\n","    if use_auth_token is not None:\n","        warnings.warn(\n","            \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n","            FutureWarning,\n","        )\n","        if token is not None:\n","            raise ValueError(\"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\")\n","        token = use_auth_token\n","\n","    resolved_config_file = cached_file(\n","        pretrained_model_name_or_path,\n","        IMAGE_PROCESSOR_NAME,\n","        cache_dir=cache_dir,\n","        force_download=force_download,\n","        resume_download=resume_download,\n","        proxies=proxies,\n","        token=token,\n","        revision=revision,\n","        local_files_only=local_files_only,\n","        _raise_exceptions_for_gated_repo=False,\n","        _raise_exceptions_for_missing_entries=False,\n","        _raise_exceptions_for_connection_errors=False,\n","    )\n","    if resolved_config_file is None:\n","        logger.info(\n","            \"Could not locate the image processor configuration file, will try to use the model config instead.\"\n","        )\n","        return {}\n","\n","    with open(resolved_config_file, encoding=\"utf-8\") as reader:\n","        return json.load(reader)\n","\n","\n","def _warning_fast_image_processor_available(fast_class):\n","    logger.warning(\n","        f\"Fast image processor class {fast_class} is available for this model. \"\n","        \"Using slow image processor class. To use the fast image processor class set `use_fast=True`.\"\n","    )\n","\n","\n","class AutoImageProcessor:\n","    r\"\"\"\n","    This is a generic image processor class that will be instantiated as one of the image processor classes of the\n","    library when created with the [`AutoImageProcessor.from_pretrained`] class method.\n","\n","    This class cannot be instantiated directly using `__init__()` (throws an error).\n","    \"\"\"\n","\n","    def __init__(self):\n","        raise EnvironmentError(\n","            \"AutoImageProcessor is designed to be instantiated \"\n","            \"using the `AutoImageProcessor.from_pretrained(pretrained_model_name_or_path)` method.\"\n","        )\n","\n","    @classmethod\n","    @replace_list_option_in_docstrings(IMAGE_PROCESSOR_MAPPING_NAMES)\n","    def from_pretrained(cls, pretrained_model_name_or_path, *inputs, **kwargs):\n","        r\"\"\"\n","        Instantiate one of the image processor classes of the library from a pretrained model vocabulary.\n","\n","        The image processor class to instantiate is selected based on the `model_type` property of the config object\n","        (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible), or when it's\n","        missing, by falling back to using pattern matching on `pretrained_model_name_or_path`:\n","\n","        List options\n","\n","        Params:\n","            pretrained_model_name_or_path (`str` or `os.PathLike`):\n","                This can be either:\n","\n","                - a string, the *model id* of a pretrained image_processor hosted inside a model repo on\n","                  huggingface.co.\n","                - a path to a *directory* containing a image processor file saved using the\n","                  [`~image_processing_utils.ImageProcessingMixin.save_pretrained`] method, e.g.,\n","                  `./my_model_directory/`.\n","                - a path or url to a saved image processor JSON *file*, e.g.,\n","                  `./my_model_directory/preprocessor_config.json`.\n","            cache_dir (`str` or `os.PathLike`, *optional*):\n","                Path to a directory in which a downloaded pretrained model image processor should be cached if the\n","                standard cache should not be used.\n","            force_download (`bool`, *optional*, defaults to `False`):\n","                Whether or not to force to (re-)download the image processor files and override the cached versions if\n","                they exist.\n","            resume_download:\n","                Deprecated and ignored. All downloads are now resumed by default when possible.\n","                Will be removed in v5 of Transformers.\n","            proxies (`Dict[str, str]`, *optional*):\n","                A dictionary of proxy servers to use by protocol or endpoint, e.g., `{'http': 'foo.bar:3128',\n","                'http://hostname': 'foo.bar:4012'}.` The proxies are used on each request.\n","            token (`str` or *bool*, *optional*):\n","                The token to use as HTTP bearer authorization for remote files. If `True`, will use the token generated\n","                when running `huggingface-cli login` (stored in `~/.huggingface`).\n","            revision (`str`, *optional*, defaults to `\"main\"`):\n","                The specific model version to use. It can be a branch name, a tag name, or a commit id, since we use a\n","                git-based system for storing models and other artifacts on huggingface.co, so `revision` can be any\n","                identifier allowed by git.\n","            use_fast (`bool`, *optional*, defaults to `False`):\n","                Use a fast torchvision-base image processor if it is supported for a given model.\n","                If a fast image processor is not available for a given model, a normal numpy-based image processor\n","                is returned instead.\n","            return_unused_kwargs (`bool`, *optional*, defaults to `False`):\n","                If `False`, then this function returns just the final image processor object. If `True`, then this\n","                functions returns a `Tuple(image_processor, unused_kwargs)` where *unused_kwargs* is a dictionary\n","                consisting of the key/value pairs whose keys are not image processor attributes: i.e., the part of\n","                `kwargs` which has not been used to update `image_processor` and is otherwise ignored.\n","            trust_remote_code (`bool`, *optional*, defaults to `False`):\n","                Whether or not to allow for custom models defined on the Hub in their own modeling files. This option\n","                should only be set to `True` for repositories you trust and in which you have read the code, as it will\n","                execute code present on the Hub on your local machine.\n","            image_processor_filename (`str`, *optional*, defaults to `\"config.json\"`):\n","                The name of the file in the model directory to use for the image processor config.\n","            kwargs (`Dict[str, Any]`, *optional*):\n","                The values in kwargs of any keys which are image processor attributes will be used to override the\n","                loaded values. Behavior concerning key/value pairs whose keys are *not* image processor attributes is\n","                controlled by the `return_unused_kwargs` keyword parameter.\n","\n","        <Tip>\n","\n","        Passing `token=True` is required when you want to use a private model.\n","\n","        </Tip>\n","\n","        Examples:\n","\n","        ```python\n","        >>> from transformers import AutoImageProcessor\n","\n","        >>> # Download image processor from huggingface.co and cache.\n","        >>> image_processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n","\n","        >>> # If image processor files are in a directory (e.g. image processor was saved using *save_pretrained('./test/saved_model/')*)\n","        >>> # image_processor = AutoImageProcessor.from_pretrained(\"./test/saved_model/\")\n","        ```\"\"\"\n","        use_auth_token = kwargs.pop(\"use_auth_token\", None)\n","        if use_auth_token is not None:\n","            warnings.warn(\n","                \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\",\n","                FutureWarning,\n","            )\n","            if kwargs.get(\"token\", None) is not None:\n","                raise ValueError(\n","                    \"`token` and `use_auth_token` are both specified. Please set only the argument `token`.\"\n","                )\n","            kwargs[\"token\"] = use_auth_token\n","\n","        config = kwargs.pop(\"config\", None)\n","        # TODO: @yoni, change in v4.48 (use_fast set to True by default)\n","        use_fast = kwargs.pop(\"use_fast\", None)\n","        trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n","        kwargs[\"_from_auto\"] = True\n","\n","        # Resolve the image processor config filename\n","        if \"image_processor_filename\" in kwargs:\n","            image_processor_filename = kwargs.pop(\"image_processor_filename\")\n","        elif is_timm_local_checkpoint(pretrained_model_name_or_path):\n","            image_processor_filename = CONFIG_NAME\n","        else:\n","            image_processor_filename = IMAGE_PROCESSOR_NAME\n","\n","        # Load the image processor config\n","        try:\n","            # Main path for all transformers models and local TimmWrapper checkpoints\n","            config_dict, _ = ImageProcessingMixin.get_image_processor_dict(\n","                pretrained_model_name_or_path, image_processor_filename=image_processor_filename, **kwargs\n","            )\n","        except Exception as initial_exception:\n","            # Fallback path for Hub TimmWrapper checkpoints. Timm models' image processing is saved in `config.json`\n","            # instead of `preprocessor_config.json`. Because this is an Auto class and we don't have any information\n","            # except the model name, the only way to check if a remote checkpoint is a timm model is to try to\n","            # load `config.json` and if it fails with some error, we raise the initial exception.\n","            try:\n","                config_dict, _ = ImageProcessingMixin.get_image_processor_dict(\n","                    pretrained_model_name_or_path, image_processor_filename=CONFIG_NAME, **kwargs\n","                )\n","            except Exception:\n","                raise initial_exception\n","\n","            # In case we have a config_dict, but it's not a timm config dict, we raise the initial exception,\n","            # because only timm models have image processing in `config.json`.\n","            if not is_timm_config_dict(config_dict):\n","                raise initial_exception\n","\n","        image_processor_type = config_dict.get(\"image_processor_type\", None)\n","        image_processor_auto_map = None\n","        if \"AutoImageProcessor\" in config_dict.get(\"auto_map\", {}):\n","            image_processor_auto_map = config_dict[\"auto_map\"][\"AutoImageProcessor\"]\n","\n","        # If we still don't have the image processor class, check if we're loading from a previous feature extractor config\n","        # and if so, infer the image processor class from there.\n","        if image_processor_type is None and image_processor_auto_map is None:\n","            feature_extractor_class = config_dict.pop(\"feature_extractor_type\", None)\n","            if feature_extractor_class is not None:\n","                image_processor_type = feature_extractor_class.replace(\"FeatureExtractor\", \"ImageProcessor\")\n","            if \"AutoFeatureExtractor\" in config_dict.get(\"auto_map\", {}):\n","                feature_extractor_auto_map = config_dict[\"auto_map\"][\"AutoFeatureExtractor\"]\n","                image_processor_auto_map = feature_extractor_auto_map.replace(\"FeatureExtractor\", \"ImageProcessor\")\n","\n","        # If we don't find the image processor class in the image processor config, let's try the model config.\n","        if image_processor_type is None and image_processor_auto_map is None:\n","            if not isinstance(config, PretrainedConfig):\n","                config = AutoConfig.from_pretrained(\n","                    pretrained_model_name_or_path,\n","                    trust_remote_code=trust_remote_code,\n","                    **kwargs,\n","                )\n","            # It could be in `config.image_processor_type``\n","            image_processor_type = getattr(config, \"image_processor_type\", None)\n","            if hasattr(config, \"auto_map\") and \"AutoImageProcessor\" in config.auto_map:\n","                image_processor_auto_map = config.auto_map[\"AutoImageProcessor\"]\n","\n","        image_processor_class = None\n","        # TODO: @yoni, change logic in v4.52 (when use_fast set to True by default)\n","        if image_processor_type is not None:\n","            # if use_fast is not set and the processor was saved with a fast processor, we use it, otherwise we use the slow processor.\n","            if use_fast is None:\n","                use_fast = image_processor_type.endswith(\"Fast\")\n","                if not use_fast:\n","                    logger.warning_once(\n","                        \"Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. \"\n","                        \"`use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. \"\n","                        \"This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\"\n","                    )\n","            # Update class name to reflect the use_fast option. If class is not found, we fall back to the slow version.\n","            if use_fast and not is_torchvision_available():\n","                logger.warning_once(\n","                    \"Using `use_fast=True` but `torchvision` is not available. Falling back to the slow image processor.\"\n","                )\n","                use_fast = False\n","            if use_fast:\n","                if not image_processor_type.endswith(\"Fast\"):\n","                    image_processor_type += \"Fast\"\n","                for _, image_processors in IMAGE_PROCESSOR_MAPPING_NAMES.items():\n","                    if image_processor_type in image_processors:\n","                        break\n","                else:\n","                    image_processor_type = image_processor_type[:-4]\n","                    use_fast = False\n","                    logger.warning_once(\n","                        \"`use_fast` is set to `True` but the image processor class does not have a fast version. \"\n","                        \" Falling back to the slow version.\"\n","                    )\n","                image_processor_class = get_image_processor_class_from_name(image_processor_type)\n","            else:\n","                image_processor_type = (\n","                    image_processor_type[:-4] if image_processor_type.endswith(\"Fast\") else image_processor_type\n","                )\n","                image_processor_class = get_image_processor_class_from_name(image_processor_type)\n","\n","        has_remote_code = image_processor_auto_map is not None\n","        has_local_code = image_processor_class is not None or type(config) in IMAGE_PROCESSOR_MAPPING\n","        trust_remote_code = resolve_trust_remote_code(\n","            trust_remote_code, pretrained_model_name_or_path, has_local_code, has_remote_code\n","        )\n","\n","        if image_processor_auto_map is not None and not isinstance(image_processor_auto_map, tuple):\n","            # In some configs, only the slow image processor class is stored\n","            image_processor_auto_map = (image_processor_auto_map, None)\n","\n","        if has_remote_code and trust_remote_code:\n","            if not use_fast and image_processor_auto_map[1] is not None:\n","                _warning_fast_image_processor_available(image_processor_auto_map[1])\n","\n","            if use_fast and image_processor_auto_map[1] is not None:\n","                class_ref = image_processor_auto_map[1]\n","            else:\n","                class_ref = image_processor_auto_map[0]\n","            image_processor_class = get_class_from_dynamic_module(class_ref, pretrained_model_name_or_path, **kwargs)\n","            _ = kwargs.pop(\"code_revision\", None)\n","            if os.path.isdir(pretrained_model_name_or_path):\n","                image_processor_class.register_for_auto_class()\n","            return image_processor_class.from_dict(config_dict, **kwargs)\n","        elif image_processor_class is not None:\n","            return image_processor_class.from_dict(config_dict, **kwargs)\n","        # Last try: we use the IMAGE_PROCESSOR_MAPPING.\n","        elif type(config) in IMAGE_PROCESSOR_MAPPING:\n","            image_processor_tuple = IMAGE_PROCESSOR_MAPPING[type(config)]\n","\n","            image_processor_class_py, image_processor_class_fast = image_processor_tuple\n","\n","            if not use_fast and image_processor_class_fast is not None:\n","                _warning_fast_image_processor_available(image_processor_class_fast)\n","\n","            if image_processor_class_fast and (use_fast or image_processor_class_py is None):\n","                return image_processor_class_fast.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n","            else:\n","                if image_processor_class_py is not None:\n","                    return image_processor_class_py.from_pretrained(pretrained_model_name_or_path, *inputs, **kwargs)\n","                else:\n","                    raise ValueError(\n","                        \"This image processor cannot be instantiated. Please make sure you have `Pillow` installed.\"\n","                    )\n","\n","        raise ValueError(\n","            f\"Unrecognized image processor in {pretrained_model_name_or_path}. Should have a \"\n","            f\"`image_processor_type` key in its {IMAGE_PROCESSOR_NAME} of {CONFIG_NAME}, or one of the following \"\n","            f\"`model_type` keys in its {CONFIG_NAME}: {', '.join(c for c in IMAGE_PROCESSOR_MAPPING_NAMES.keys())}\"\n","        )\n","\n","    @staticmethod\n","    def register(\n","        config_class,\n","        image_processor_class=None,\n","        slow_image_processor_class=None,\n","        fast_image_processor_class=None,\n","        exist_ok=False,\n","    ):\n","        \"\"\"\n","        Register a new image processor for this class.\n","\n","        Args:\n","            config_class ([`PretrainedConfig`]):\n","                The configuration corresponding to the model to register.\n","            image_processor_class ([`ImageProcessingMixin`]): The image processor to register.\n","        \"\"\"\n","        if image_processor_class is not None:\n","            if slow_image_processor_class is not None:\n","                raise ValueError(\"Cannot specify both image_processor_class and slow_image_processor_class\")\n","            warnings.warn(\n","                \"The image_processor_class argument is deprecated and will be removed in v4.42. Please use `slow_image_processor_class`, or `fast_image_processor_class` instead\",\n","                FutureWarning,\n","            )\n","            slow_image_processor_class = image_processor_class\n","\n","        if slow_image_processor_class is None and fast_image_processor_class is None:\n","            raise ValueError(\"You need to specify either slow_image_processor_class or fast_image_processor_class\")\n","        if slow_image_processor_class is not None and issubclass(slow_image_processor_class, BaseImageProcessorFast):\n","            raise ValueError(\"You passed a fast image processor in as the `slow_image_processor_class`.\")\n","        if fast_image_processor_class is not None and not issubclass(\n","            fast_image_processor_class, BaseImageProcessorFast\n","        ):\n","            raise ValueError(\"The `fast_image_processor_class` should inherit from `BaseImageProcessorFast`.\")\n","\n","        if (\n","            slow_image_processor_class is not None\n","            and fast_image_processor_class is not None\n","            and issubclass(fast_image_processor_class, BaseImageProcessorFast)\n","            and fast_image_processor_class.slow_image_processor_class != slow_image_processor_class\n","        ):\n","            raise ValueError(\n","                \"The fast processor class you are passing has a `slow_image_processor_class` attribute that is not \"\n","                \"consistent with the slow processor class you passed (fast tokenizer has \"\n","                f\"{fast_image_processor_class.slow_image_processor_class} and you passed {slow_image_processor_class}. Fix one of those \"\n","                \"so they match!\"\n","            )\n","\n","        # Avoid resetting a set slow/fast image processor if we are passing just the other ones.\n","        if config_class in IMAGE_PROCESSOR_MAPPING._extra_content:\n","            existing_slow, existing_fast = IMAGE_PROCESSOR_MAPPING[config_class]\n","            if slow_image_processor_class is None:\n","                slow_image_processor_class = existing_slow\n","            if fast_image_processor_class is None:\n","                fast_image_processor_class = existing_fast\n","\n","        IMAGE_PROCESSOR_MAPPING.register(\n","            config_class, (slow_image_processor_class, fast_image_processor_class), exist_ok=exist_ok\n","        )\n"],"metadata":{"id":"plW8U44sYKpP"},"execution_count":null,"outputs":[]}]}