{"cells":[{"cell_type":"markdown","metadata":{"id":"Hz_-DBvnjHy_"},"source":["# **PROTOTIPO FUNCIONAL DE DETECCIÓN DE VIOLENCIA FÍSICAS**"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":121174,"status":"ok","timestamp":1744164453627,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"},"user_tz":240},"id":"XxrHBHxTmpmW","outputId":"0a0e90a0-9ba6-4dae-9ecf-c0c035cd41a3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.105-py3-none-any.whl.metadata (37 kB)\n","Collecting deep-sort-realtime\n","  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Downloading ultralytics-8.3.105-py3-none-any.whl (994 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, deep-sort-realtime, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed deep-sort-realtime-1.3.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.105 ultralytics-thop-2.0.14\n"]}],"source":["!pip install ultralytics deep-sort-realtime transformers torch opencv-python numpy"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5967,"status":"ok","timestamp":1744168519960,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"},"user_tz":240},"id":"fZS8BXQ1GS5O","outputId":"d9b82d45-a498-4e7d-80fa-66ea76696ef5"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"829xodAIS3U5"},"source":["## PROCESAMIENTO DE VIDEOS A 1280x720"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3323,"status":"ok","timestamp":1744147951233,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"},"user_tz":240},"id":"YA0ldpkg9VIH","outputId":"d720feed-84f7-4b3e-d15f-cd696525cbd6"},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n","  if event.key is 'enter':\n","\n"]},{"name":"stdout","output_type":"stream","text":["==================================================\n","PROCESADOR DE VIDEO: RECORTE Y REDIMENSIONAMIENTO\n","==================================================\n","Video de entrada: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba3.mp4\n","Carpeta de salida: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados\n","Rango de recorte: 9s - 14s\n","Resolución objetivo: 1280x720\n","==================================================\n","Cargando video: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba3.mp4\n","Información del video original:\n","- Duración: 5.00 segundos\n","- Resolución: 768x432\n","Error al procesar el video: Tiempos de recorte inválidos: 9s - 14s. El video tiene una duración de 5.00s\n","\n","==================================================\n","ERROR: Tiempos de recorte inválidos: 9s - 14s. El video tiene una duración de 5.00s\n","==================================================\n"]}],"source":["import os\n","from moviepy.editor import VideoFileClip\n","\n","# PARÁMETROS PREDEFINIDOS - MODIFICA ESTOS VALORES\n","INPUT_PATH = \"/content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba3.mp4\"  # Ruta al video de entrada\n","OUTPUT_FOLDER = \"/content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados\"  # Carpeta donde se guardará el video procesado\n","START_TIME = 9  # Tiempo de inicio para el recorte (en segundos)\n","END_TIME = 14  # Tiempo de fin para el recorte (en segundos)\n","TARGET_RESOLUTION = (1280, 720)  # Resolución objetivo (ancho, alto)\n","\n","def process_video(input_path, output_folder, start_time, end_time, target_resolution=(1280, 720)):\n","    \"\"\"\n","    Procesa un video recortándolo por un rango de tiempo específico y ajustando su resolución.\n","    Mantiene el nombre original del archivo de entrada.\n","\n","    Args:\n","        input_path (str): Ruta al video de entrada\n","        output_folder (str): Carpeta donde se guardará el video procesado\n","        start_time (float): Tiempo de inicio para el recorte (en segundos)\n","        end_time (float): Tiempo de fin para el recorte (en segundos)\n","        target_resolution (tuple): Resolución objetivo en formato (ancho, alto)\n","\n","    Returns:\n","        str: Ruta del archivo de salida procesado\n","    \"\"\"\n","    # Verificar si el archivo de entrada existe\n","    if not os.path.exists(input_path):\n","        raise FileNotFoundError(f\"El archivo de video no existe: {input_path}\")\n","\n","    # Crear la carpeta de salida si no existe\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","        print(f\"Carpeta de salida creada: {output_folder}\")\n","\n","    # Obtener el nombre base del archivo de entrada sin extensión\n","    base_name = os.path.basename(input_path)\n","    file_name, file_ext = os.path.splitext(base_name)\n","\n","    # Mantener el nombre original del archivo\n","    output_name = f\"{file_name}{file_ext}\"\n","    output_path = os.path.join(output_folder, output_name)\n","\n","    try:\n","        # Cargar el video\n","        print(f\"Cargando video: {input_path}\")\n","        video = VideoFileClip(input_path)\n","\n","        # Obtener información del video original\n","        original_duration = video.duration\n","        original_size = video.size\n","        print(f\"Información del video original:\")\n","        print(f\"- Duración: {original_duration:.2f} segundos\")\n","        print(f\"- Resolución: {original_size[0]}x{original_size[1]}\")\n","\n","        # Validar tiempos de recorte\n","        if start_time < 0 or end_time > original_duration or start_time >= end_time:\n","            raise ValueError(f\"Tiempos de recorte inválidos: {start_time}s - {end_time}s. \" +\n","                            f\"El video tiene una duración de {original_duration:.2f}s\")\n","\n","        # Recortar el video dentro del rango especificado\n","        print(f\"Recortando video del segundo {start_time} al {end_time}\")\n","        video_trimmed = video.subclip(start_time, end_time)\n","\n","        # Redimensionar el video si es necesario\n","        if original_size[0] != target_resolution[0] or original_size[1] != target_resolution[1]:\n","            print(f\"Redimensionando video de {original_size[0]}x{original_size[1]} a {target_resolution[0]}x{target_resolution[1]}\")\n","            # Usar redimensionamiento compatible con moviepy 1.0.3 y Pillow 6.2.2\n","            video_processed = video_trimmed.resize(target_resolution)\n","        else:\n","            print(\"El video ya tiene la resolución objetivo. No es necesario redimensionar.\")\n","            video_processed = video_trimmed\n","\n","        # Guardar el video procesado con alta calidad\n","        print(f\"Guardando video procesado: {output_path}\")\n","\n","        # Configurar parámetros de codificación para mantener buena calidad\n","        video_processed.write_videofile(\n","            output_path,\n","            codec=\"libx264\",\n","            audio_codec=\"aac\",\n","            bitrate=\"8000k\",\n","            preset=\"slow\",  # Mejor calidad\n","            threads=4,\n","            fps=video.fps,  # Mantener la misma tasa de fotogramas\n","            temp_audiofile=\"temp-audio.m4a\",\n","            remove_temp=True\n","        )\n","\n","        # Obtener información del video procesado\n","        print(f\"Video procesado guardado exitosamente:\")\n","        print(f\"- Duración: {end_time - start_time:.2f} segundos\")\n","        print(f\"- Resolución: {target_resolution[0]}x{target_resolution[1]}\")\n","\n","        # Limpiar\n","        video.close()\n","        video_processed.close()\n","\n","        return output_path\n","\n","    except Exception as e:\n","        print(f\"Error al procesar el video: {str(e)}\")\n","        # Asegurar que los recursos se liberen en caso de error\n","        try:\n","            video.close()\n","        except:\n","            pass\n","        try:\n","            video_processed.close()\n","        except:\n","            pass\n","        raise\n","\n","# Punto de entrada principal del script\n","if __name__ == \"__main__\":\n","    print(\"=\" * 50)\n","    print(\"PROCESADOR DE VIDEO: RECORTE Y REDIMENSIONAMIENTO\")\n","    print(\"=\" * 50)\n","    print(f\"Video de entrada: {INPUT_PATH}\")\n","    print(f\"Carpeta de salida: {OUTPUT_FOLDER}\")\n","    print(f\"Rango de recorte: {START_TIME}s - {END_TIME}s\")\n","    print(f\"Resolución objetivo: {TARGET_RESOLUTION[0]}x{TARGET_RESOLUTION[1]}\")\n","    print(\"=\" * 50)\n","\n","    try:\n","        # Ejecutar procesamiento con los parámetros predefinidos\n","        result = process_video(INPUT_PATH, OUTPUT_FOLDER, START_TIME, END_TIME, TARGET_RESOLUTION)\n","        print(\"\\n\" + \"=\" * 50)\n","        print(f\"PROCESO COMPLETADO EXITOSAMENTE\")\n","        print(f\"Video procesado guardado en: {result}\")\n","        print(\"=\" * 50)\n","    except Exception as e:\n","        print(\"\\n\" + \"=\" * 50)\n","        print(f\"ERROR: {str(e)}\")\n","        print(\"=\" * 50)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30205,"status":"ok","timestamp":1744168595789,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"},"user_tz":240},"id":"pJaJevhEVXXw","outputId":"91400ecf-3c5c-447a-d259-3cb37cf38389"},"outputs":[{"name":"stdout","output_type":"stream","text":["==================================================\n","PROCESADOR DE VIDEO: SOLO REDIMENSIONAMIENTO\n","==================================================\n","Video de entrada: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba10.mp4\n","Carpeta de salida: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados\n","Resolución objetivo: 1280x720\n","==================================================\n","Cargando video: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba10.mp4\n","Información del video original:\n","- Duración: 5.00 segundos\n","- Resolución: 1280x720\n","El video ya tiene la resolución objetivo. No es necesario redimensionar.\n","Guardando video procesado: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4\n","Moviepy - Building video /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4.\n","MoviePy - Writing audio in temp-audio.m4a\n"]},{"name":"stderr","output_type":"stream","text":[]},{"name":"stdout","output_type":"stream","text":["MoviePy - Done.\n","Moviepy - Writing video /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4\n","\n"]},{"name":"stderr","output_type":"stream","text":["t:  99%|█████████▉| 150/151 [00:15<00:00,  5.43it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba10.mp4, 2764800 bytes wanted but 0 bytes read,at frame 150/151, at time 5.00/5.00 sec. Using the last valid frame instead.\n","  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n","\n"]},{"name":"stdout","output_type":"stream","text":["Moviepy - Done !\n","Moviepy - video ready /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4\n","Video procesado guardado exitosamente.\n","- Duración: 5.00 segundos\n","- Resolución: 1280x720\n","\n","==================================================\n","PROCESO COMPLETADO EXITOSAMENTE\n","Video procesado guardado en: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4\n","==================================================\n"]}],"source":["import os\n","from moviepy.editor import VideoFileClip\n","\n","# PARÁMETROS PREDEFINIDOS - MODIFICA ESTOS VALORES\n","INPUT_PATH = \"/content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba10.mp4\"  # Ruta al video de entrada\n","OUTPUT_FOLDER = \"/content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados\"  # Carpeta donde se guardará el video procesado\n","TARGET_RESOLUTION = (1280, 720)  # Resolución objetivo (ancho, alto)\n","\n","def process_video(input_path, output_folder, target_resolution=(1280, 720)):\n","    \"\"\"\n","    Procesa un video ajustando su resolución.\n","    Mantiene el nombre original del archivo de entrada.\n","\n","    Args:\n","        input_path (str): Ruta al video de entrada\n","        output_folder (str): Carpeta donde se guardará el video procesado\n","        target_resolution (tuple): Resolución objetivo en formato (ancho, alto)\n","\n","    Returns:\n","        str: Ruta del archivo de salida procesado\n","    \"\"\"\n","    if not os.path.exists(input_path):\n","        raise FileNotFoundError(f\"El archivo de video no existe: {input_path}\")\n","\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","        print(f\"Carpeta de salida creada: {output_folder}\")\n","\n","    base_name = os.path.basename(input_path)\n","    file_name, file_ext = os.path.splitext(base_name)\n","    output_name = f\"{file_name}{file_ext}\"\n","    output_path = os.path.join(output_folder, output_name)\n","\n","    try:\n","        print(f\"Cargando video: {input_path}\")\n","        video = VideoFileClip(input_path)\n","\n","        original_size = video.size\n","        original_duration = video.duration\n","        print(f\"Información del video original:\")\n","        print(f\"- Duración: {original_duration:.2f} segundos\")\n","        print(f\"- Resolución: {original_size[0]}x{original_size[1]}\")\n","\n","        if original_size != list(target_resolution):\n","            print(f\"Redimensionando video a {target_resolution[0]}x{target_resolution[1]}\")\n","            video_processed = video.resize(target_resolution)\n","        else:\n","            print(\"El video ya tiene la resolución objetivo. No es necesario redimensionar.\")\n","            video_processed = video\n","\n","        print(f\"Guardando video procesado: {output_path}\")\n","        video_processed.write_videofile(\n","            output_path,\n","            codec=\"libx264\",\n","            audio_codec=\"aac\",\n","            bitrate=\"8000k\",\n","            preset=\"slow\",\n","            threads=4,\n","            fps=video.fps,\n","            temp_audiofile=\"temp-audio.m4a\",\n","            remove_temp=True\n","        )\n","\n","        print(f\"Video procesado guardado exitosamente.\")\n","        print(f\"- Duración: {original_duration:.2f} segundos\")\n","        print(f\"- Resolución: {target_resolution[0]}x{target_resolution[1]}\")\n","\n","        video.close()\n","        video_processed.close()\n","\n","        return output_path\n","\n","    except Exception as e:\n","        print(f\"Error al procesar el video: {str(e)}\")\n","        try:\n","            video.close()\n","        except:\n","            pass\n","        try:\n","            video_processed.close()\n","        except:\n","            pass\n","        raise\n","\n","# Punto de entrada principal del script\n","if __name__ == \"__main__\":\n","    print(\"=\" * 50)\n","    print(\"PROCESADOR DE VIDEO: SOLO REDIMENSIONAMIENTO\")\n","    print(\"=\" * 50)\n","    print(f\"Video de entrada: {INPUT_PATH}\")\n","    print(f\"Carpeta de salida: {OUTPUT_FOLDER}\")\n","    print(f\"Resolución objetivo: {TARGET_RESOLUTION[0]}x{TARGET_RESOLUTION[1]}\")\n","    print(\"=\" * 50)\n","\n","    try:\n","        result = process_video(INPUT_PATH, OUTPUT_FOLDER, TARGET_RESOLUTION)\n","        print(\"\\n\" + \"=\" * 50)\n","        print(\"PROCESO COMPLETADO EXITOSAMENTE\")\n","        print(f\"Video procesado guardado en: {result}\")\n","        print(\"=\" * 50)\n","    except Exception as e:\n","        print(\"\\n\" + \"=\" * 50)\n","        print(f\"ERROR: {str(e)}\")\n","        print(\"=\" * 50)\n"]},{"cell_type":"markdown","metadata":{"id":"FbLHPVEQIoWX"},"source":[]},{"cell_type":"markdown","metadata":{"id":"MdhC38zblwMr"},"source":["# PROTORIPO DEL MODELO"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"f_OTfSUOT0Qv","outputId":"8869f962-85ce-49aa-d3c7-a8b91aa0ff83","executionInfo":{"status":"ok","timestamp":1746475106997,"user_tz":240,"elapsed":14934,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.127-py3-none-any.whl.metadata (37 kB)\n","Collecting deep-sort-realtime\n","  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.2)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Downloading ultralytics-8.3.127-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m25.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m123.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m840.6 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, deep-sort-realtime, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed deep-sort-realtime-1.3.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.127 ultralytics-thop-2.0.14\n"]}],"source":["!pip install ultralytics deep-sort-realtime transformers torch opencv-python numpy"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20129,"status":"ok","timestamp":1746475131005,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"},"user_tz":240},"id":"dkI7LYU0jWJO","outputId":"3d3092fc-225c-4f0f-eb6b-d56cc1704b4c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}],"source":["import cv2\n","import numpy as np\n","import torch\n","from ultralytics import YOLO\n","from deep_sort_realtime.deepsort_tracker import DeepSort\n","from transformers import TimesformerForVideoClassification, AutoImageProcessor\n","import logging\n","import os\n","from collections import deque, defaultdict\n","from datetime import datetime"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6420,"status":"ok","timestamp":1746475524833,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"},"user_tz":240},"id":"ZEIt0raQXj3w","outputId":"428a0562-14e6-484c-c27d-ad38c720abfa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"BX1j-XZHjeye","executionInfo":{"status":"ok","timestamp":1746475528036,"user_tz":240,"elapsed":465,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}}},"outputs":[],"source":["# Configurar logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n","    handlers=[\n","        logging.FileHandler(\"violence_detection_prototype.log\"),\n","        logging.StreamHandler()\n","    ]\n",")\n","\n","# Definir rutas\n","BASE_PATH = \"/content/drive/MyDrive/Proyecto IA-3/violence_school_project/\"\n","YOLO_MODEL_PATH = os.path.join(BASE_PATH, \"modelosV2/best.pt\")  # Modelo YOLO entrenado\n","TIMESFORMER_MODEL_PATH = os.path.join(BASE_PATH, \"models/timesformer/run_finetune_20250408_003512/best_timesformer_finetune.pt\")\n","# TIMESFORMER_MODEL_PATH = os.path.join(BASE_PATH, \"models/timesformer/run_20250407_115035/best_timesformer_transfer.pt\")\n","VIDEO_PATH = os.path.join(BASE_PATH, \"videos/fight_0620_004.mp4\")  # Video de prueba\n","OUTPUT_PATH = os.path.join(BASE_PATH, \"output_videos/video_prueba4.2.mp4\")\n","\n","# Verificar que los modelos existan\n","if not os.path.exists(YOLO_MODEL_PATH):\n","    raise FileNotFoundError(f\"No se encontró el modelo YOLO en: {YOLO_MODEL_PATH}\")\n","if not os.path.exists(TIMESFORMER_MODEL_PATH):\n","    raise FileNotFoundError(f\"No se encontró el modelo TimeSformer en: {TIMESFORMER_MODEL_PATH}\")\n","\n","# Crear la carpeta de salida si no existe\n","output_dir = os.path.dirname(OUTPUT_PATH)\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","    logging.info(f\"Carpeta de salida creada: {output_dir}\")"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269,"referenced_widgets":["b0a3a945159c4c8fb44990cd0439b5cd","4c3fe7b0969b4a6da5689723af2e9421","c94ab07fdffe419bbf04ebe2d678caeb","7b28a84925cb4ed08c6ff2ccc0bac0be","ccac8a5c33374d99a52bc6a074658d02","8108fc9bcce24936b51e7e4ce2ffa4c2","63ee978502dc4488a6ba1fcdacf5edd5","7a65755a18d949b5b6d7f94628d2c7df","758c31d6524a42d5901800f0253929da","05dd7d9a1ef3410f96c14f2e04503ade","b9b13b826a0d41db99fe8989632055ce"]},"executionInfo":{"elapsed":21302,"status":"ok","timestamp":1746475563657,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"},"user_tz":240},"id":"OFAd66h5jjw3","outputId":"8709fef6-2ddd-4f32-9bb1-6ba10fa31621"},"outputs":[{"output_type":"stream","name":"stdout","text":["Usando dispositivo: cuda\n","Modelo YOLO cargado desde: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/modelosV2/best.pt\n","DeepSORT inicializado.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0a3a945159c4c8fb44990cd0439b5cd"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"]},{"output_type":"stream","name":"stdout","text":["Modelo TimeSformer cargado desde: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/models/timesformer/run_finetune_20250408_003512/best_timesformer_finetune.pt\n"]}],"source":["# Configurar dispositivo\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logging.info(f\"Usando dispositivo: {device}\")\n","print(f\"Usando dispositivo: {device}\")\n","\n","# Cargar modelo YOLO entrenado\n","yolo_model = YOLO(YOLO_MODEL_PATH)\n","logging.info(f\"Modelo YOLO cargado desde: {YOLO_MODEL_PATH}\")\n","print(f\"Modelo YOLO cargado desde: {YOLO_MODEL_PATH}\")\n","\n","# Cargar DeepSORT\n","deepsort = DeepSort(max_age=30, n_init=3, nn_budget=100)\n","logging.info(\"DeepSORT inicializado.\")\n","print(\"DeepSORT inicializado.\")\n","\n","# Cargar modelo TimeSformer\n","timesformer_model = TimesformerForVideoClassification.from_pretrained(TIMESFORMER_MODEL_PATH)\n","timesformer_model.to(device)\n","timesformer_model.eval()\n","processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n","logging.info(f\"Modelo TimeSformer cargado desde: {TIMESFORMER_MODEL_PATH}\")\n","print(f\"Modelo TimeSformer cargado desde: {TIMESFORMER_MODEL_PATH}\")"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"jzX701s7ivU0","executionInfo":{"status":"ok","timestamp":1746475568733,"user_tz":240,"elapsed":393,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}}},"outputs":[],"source":["# Parámetros clave para el procesamiento del video y los modelos\n","CLIP_DURATION_SECONDS = 10  # Duración de cada clip de video a analizar (10 segundos)\n","FPS = 30  # FPS inicial esperado del video (se ajustará según el video real)\n","CLIP_FRAMES = CLIP_DURATION_SECONDS * FPS  # Total de frames por clip (10 seg * 30 FPS = 300 frames)\n","STRIDE_FRAMES = CLIP_FRAMES // 5  # Solapamiento para procesar clips cada 2 segundos (300 // 5 = 60 frames)\n","TIMESFORMER_FPS = 15  # FPS objetivo para TimeSformer (reduce la carga computacional)\n","TIMESFORMER_FRAMES = CLIP_DURATION_SECONDS * TIMESFORMER_FPS  # Frames totales para TimeSformer (10 seg * 15 FPS = 150 frames)\n","NUM_FRAMES_TIMESFORMER = 8  # Número de frames que TimeSformer usará por clip (muestreo uniforme)\n","THRESHOLD_VIOLENCE = 0.6  # Umbral de probabilidad para clasificar un clip como violento (60%)\n","YOLO_CONF_THRESHOLD = 0.65  # Umbral de confianza para las detecciones de YOLO (reduce falsos positivos)\n","\n","# Cola para almacenar frames en un buffer circular (máximo CLIP_FRAMES)\n","frame_buffer = deque(maxlen=CLIP_FRAMES)\n","\n","# Diccionario para almacenar las trayectorias de las personas detectadas por DeepSORT\n","trajectories = defaultdict(list)\n","\n","# Función para preprocesar frames para TimeSformer\n","def preprocess_frames_for_timesformer(frames, num_frames=NUM_FRAMES_TIMESFORMER, target_size=(224, 224), target_fps=TIMESFORMER_FPS):\n","    # Reducir FPS a 15 FPS para TimeSformer\n","    total_frames = len(frames)  # Total de frames en el clip (300 a 30 FPS)\n","    target_frame_count = int((total_frames / FPS) * target_fps)  # Frames a 15 FPS (150 frames)\n","    frame_indices = np.linspace(0, total_frames - 1, target_frame_count, dtype=int)  # Seleccionar índices uniformemente\n","    selected_frames = [frames[i] for i in frame_indices]  # Frames seleccionados a 15 FPS\n","\n","    # Muestrear 8 frames uniformemente de los 150 frames\n","    if len(selected_frames) < num_frames:\n","        raise ValueError(f\"No hay suficientes frames después de reducir FPS: {len(selected_frames)} < {num_frames}\")\n","\n","    sample_indices = np.linspace(0, len(selected_frames) - 1, num_frames, dtype=int)  # Índices para 8 frames\n","    final_frames = [selected_frames[i] for i in sample_indices]  # Frames finales para TimeSformer\n","\n","    # Redimensionar y convertir a RGB\n","    processed_frames = []\n","    for frame in final_frames:\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)  # Convertir de BGR (OpenCV) a RGB (TimeSformer)\n","        h, w = frame.shape[:2]  # Dimensiones originales del frame\n","        ratio = min(target_size[0] / w, target_size[1] / h)  # Calcular ratio de redimensionamiento\n","        new_w, new_h = int(w * ratio), int(h * ratio)  # Nuevas dimensiones manteniendo proporción\n","        resized_frame = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)  # Redimensionar\n","        padded_frame = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)  # Crear frame vacío de 224x224\n","        pad_top = (target_size[1] - new_h) // 2  # Calcular padding superior\n","        pad_left = (target_size[0] - new_w) // 2  # Calcular padding izquierdo\n","        padded_frame[pad_top:pad_top + new_h, pad_left:pad_left + new_w] = resized_frame  # Colocar frame redimensionado\n","        processed_frames.append(padded_frame)\n","\n","    # Preprocesar con AutoImageProcessor para TimeSformer\n","    inputs = processor(processed_frames, return_tensors=\"pt\")  # Convertir a tensores\n","    pixel_values = inputs[\"pixel_values\"].to(device)  # Mover a GPU/CPU según dispositivo\n","    return pixel_values\n","\n","# Función para predecir violencia con TimeSformer\n","def predict_violence(frames):\n","    try:\n","        pixel_values = preprocess_frames_for_timesformer(frames)  # Preprocesar frames\n","        with torch.no_grad():  # Desactivar gradientes para inferencia\n","            outputs = timesformer_model(pixel_values=pixel_values)  # Pasar frames a TimeSformer\n","            logits = outputs.logits  # Obtener logits (salida cruda del modelo)\n","            probs = torch.softmax(logits, dim=1)  # Convertir logits a probabilidades\n","            prob_violence = probs[0, 1].item()  # Probabilidad de la clase \"Violencia\"\n","            pred = 1 if prob_violence > THRESHOLD_VIOLENCE else 0  # Clasificar según umbral\n","        return pred, prob_violence  # Devolver predicción (0 o 1) y probabilidad\n","    except Exception as e:\n","        logging.error(f\"Error al predecir violencia: {str(e)}\")  # Registrar error\n","        return 0, 0.0  # Devolver valores por defecto en caso de error\n","\n","# Función para obtener los IDs de personas presentes en un intervalo de frames\n","def get_ids_in_interval(start_frame, end_frame):\n","    ids_in_interval = set()  # Conjunto para almacenar IDs únicos\n","    for frame_num in range(start_frame, end_frame + 1):  # Iterar sobre el intervalo\n","        if frame_num in trajectories:  # Si el frame tiene trayectorias\n","            for track_id, _ in trajectories[frame_num]:  # Obtener IDs\n","                ids_in_interval.add(track_id)  # Añadir ID al conjunto\n","    return sorted(list(ids_in_interval))  # Devolver lista ordenada de IDs"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9560,"status":"ok","timestamp":1746475792166,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"},"user_tz":240},"id":"gy-h6Mfqj52P","outputId":"5e288af9-fa9b-4199-b0ae-45ffe0d5c08c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Procesando video: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/fight_0620_004.mp4\n","\n","0: 640x640 3 items, 14.0ms\n","Speed: 3.0ms preprocess, 14.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.7ms\n","Speed: 2.8ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 11.0ms\n","Speed: 3.0ms preprocess, 11.0ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 11.1ms\n","Speed: 3.0ms preprocess, 11.1ms inference, 3.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 items, 10.1ms\n","Speed: 2.5ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 11.2ms\n","Speed: 2.3ms preprocess, 11.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 11.1ms\n","Speed: 2.4ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.8ms\n","Speed: 2.5ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.3ms\n","Speed: 3.5ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.2ms\n","Speed: 3.0ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 9.0ms\n","Speed: 2.9ms preprocess, 9.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.2ms\n","Speed: 2.9ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.3ms\n","Speed: 3.1ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 22.3ms\n","Speed: 3.4ms preprocess, 22.3ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 10.5ms\n","Speed: 2.3ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.2ms\n","Speed: 2.3ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.2ms\n","Speed: 2.6ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 10.0ms\n","Speed: 2.5ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.8ms\n","Speed: 2.2ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.5ms\n","Speed: 2.3ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.8ms\n","Speed: 2.4ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.1ms\n","Speed: 2.9ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.5ms\n","Speed: 3.8ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.0ms\n","Speed: 3.2ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.9ms\n","Speed: 3.0ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.0ms\n","Speed: 2.2ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 9.2ms\n","Speed: 2.7ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.4ms\n","Speed: 3.5ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.3ms\n","Speed: 2.5ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.6ms\n","Speed: 2.6ms preprocess, 10.6ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.4ms\n","Speed: 2.9ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.7ms\n","Speed: 2.7ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 15.3ms\n","Speed: 2.7ms preprocess, 15.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.3ms\n","Speed: 2.4ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.1ms\n","Speed: 2.8ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.4ms\n","Speed: 2.9ms preprocess, 9.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.5ms\n","Speed: 2.5ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.7ms\n","Speed: 3.3ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.9ms\n","Speed: 2.5ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.3ms\n","Speed: 3.2ms preprocess, 10.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.5ms\n","Speed: 2.3ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.2ms\n","Speed: 2.8ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.6ms\n","Speed: 2.7ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.7ms\n","Speed: 2.5ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.2ms\n","Speed: 2.7ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.2ms\n","Speed: 2.6ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.5ms\n","Speed: 2.4ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.9ms\n","Speed: 3.3ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.6ms\n","Speed: 2.7ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.1ms\n","Speed: 2.6ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 10.0ms\n","Speed: 2.4ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 12.3ms\n","Speed: 2.5ms preprocess, 12.3ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 13.2ms\n","Speed: 2.9ms preprocess, 13.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.1ms\n","Speed: 2.4ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.9ms\n","Speed: 2.9ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.3ms\n","Speed: 2.5ms preprocess, 9.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.2ms\n","Speed: 2.6ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 10.5ms\n","Speed: 3.2ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.2ms\n","Speed: 2.8ms preprocess, 9.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.8ms\n","Speed: 2.2ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","Predicción en frames 1-60: Violencia=Sí, Probabilidad=0.9999\n","Violencia detectada en frames 1-60, Probabilidad: 0.9999, IDs: ['14', '18', '19', '2', '5'], Timestamp: 2025-05-05 20:09:45\n","\n","0: 640x640 2 items, 10.0ms\n","Speed: 3.0ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.4ms\n","Speed: 2.4ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.8ms\n","Speed: 2.3ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.7ms\n","Speed: 2.6ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.3ms\n","Speed: 2.4ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 23.2ms\n","Speed: 2.3ms preprocess, 23.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.5ms\n","Speed: 2.9ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.9ms\n","Speed: 2.3ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.2ms\n","Speed: 2.3ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.0ms\n","Speed: 2.8ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.8ms\n","Speed: 2.7ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.3ms\n","Speed: 2.8ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.4ms\n","Speed: 2.4ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 11.1ms\n","Speed: 2.2ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.3ms\n","Speed: 2.6ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.6ms\n","Speed: 3.7ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.1ms\n","Speed: 2.8ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.0ms\n","Speed: 2.7ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 11.2ms\n","Speed: 2.6ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.7ms\n","Speed: 2.6ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 12.2ms\n","Speed: 3.1ms preprocess, 12.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 9.0ms\n","Speed: 2.7ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 9.3ms\n","Speed: 2.6ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 8.9ms\n","Speed: 2.7ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 13.6ms\n","Speed: 2.7ms preprocess, 13.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.7ms\n","Speed: 2.7ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.6ms\n","Speed: 2.9ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.7ms\n","Speed: 3.0ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 items, 9.7ms\n","Speed: 2.6ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 10.2ms\n","Speed: 2.6ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 11.3ms\n","Speed: 2.5ms preprocess, 11.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.1ms\n","Speed: 2.4ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 9.1ms\n","Speed: 2.9ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.4ms\n","Speed: 3.2ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 11.4ms\n","Speed: 3.0ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.6ms\n","Speed: 4.3ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.8ms\n","Speed: 3.2ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.0ms\n","Speed: 3.5ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.0ms\n","Speed: 3.1ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.4ms\n","Speed: 3.0ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","Procesados 100/150 frames\n","\n","0: 640x640 1 item, 15.5ms\n","Speed: 2.8ms preprocess, 15.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.5ms\n","Speed: 2.4ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.3ms\n","Speed: 2.1ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.0ms\n","Speed: 2.2ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.6ms\n","Speed: 2.5ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.9ms\n","Speed: 3.5ms preprocess, 10.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.4ms\n","Speed: 2.3ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.0ms\n","Speed: 2.3ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 11.4ms\n","Speed: 2.3ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.4ms\n","Speed: 2.3ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 10.5ms\n","Speed: 3.1ms preprocess, 10.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.5ms\n","Speed: 2.7ms preprocess, 9.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 9.1ms\n","Speed: 2.6ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 16.0ms\n","Speed: 2.4ms preprocess, 16.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 16.5ms\n","Speed: 2.7ms preprocess, 16.5ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 11.7ms\n","Speed: 3.7ms preprocess, 11.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 11.9ms\n","Speed: 2.6ms preprocess, 11.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 14.9ms\n","Speed: 2.5ms preprocess, 14.9ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 12.4ms\n","Speed: 2.6ms preprocess, 12.4ms inference, 2.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 13.1ms\n","Speed: 2.3ms preprocess, 13.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","Predicción en frames 1-120: Violencia=Sí, Probabilidad=1.0000\n","Violencia detectada en frames 1-120, Probabilidad: 1.0000, IDs: ['14', '18', '19', '2', '22', '5'], Timestamp: 2025-05-05 20:09:49\n","\n","0: 640x640 1 item, 13.8ms\n","Speed: 2.6ms preprocess, 13.8ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 12.9ms\n","Speed: 2.4ms preprocess, 12.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 18.5ms\n","Speed: 2.4ms preprocess, 18.5ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 11.4ms\n","Speed: 2.3ms preprocess, 11.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 12.4ms\n","Speed: 2.2ms preprocess, 12.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 11.2ms\n","Speed: 3.4ms preprocess, 11.2ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 12.4ms\n","Speed: 2.4ms preprocess, 12.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 12.4ms\n","Speed: 2.6ms preprocess, 12.4ms inference, 6.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 13.7ms\n","Speed: 2.6ms preprocess, 13.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 11.9ms\n","Speed: 2.6ms preprocess, 11.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 13.0ms\n","Speed: 2.5ms preprocess, 13.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 20.7ms\n","Speed: 2.7ms preprocess, 20.7ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 12.4ms\n","Speed: 2.7ms preprocess, 12.4ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 12.3ms\n","Speed: 3.8ms preprocess, 12.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 14.9ms\n","Speed: 4.1ms preprocess, 14.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 13.2ms\n","Speed: 3.0ms preprocess, 13.2ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 12.7ms\n","Speed: 5.7ms preprocess, 12.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 14.5ms\n","Speed: 2.5ms preprocess, 14.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 12.1ms\n","Speed: 4.6ms preprocess, 12.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 12.0ms\n","Speed: 2.3ms preprocess, 12.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 item, 11.8ms\n","Speed: 2.4ms preprocess, 11.8ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 12.1ms\n","Speed: 2.6ms preprocess, 12.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 22.5ms\n","Speed: 3.6ms preprocess, 22.5ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 18.6ms\n","Speed: 2.4ms preprocess, 18.6ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 11.9ms\n","Speed: 2.5ms preprocess, 11.9ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 17.0ms\n","Speed: 2.8ms preprocess, 17.0ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 18.2ms\n","Speed: 2.5ms preprocess, 18.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 22.4ms\n","Speed: 2.5ms preprocess, 22.4ms inference, 2.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 16.6ms\n","Speed: 3.6ms preprocess, 16.6ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 items, 19.5ms\n","Speed: 2.4ms preprocess, 19.5ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n"]}],"source":["# Cargar video de entrada\n","cap = cv2.VideoCapture(VIDEO_PATH)  # Abrir video desde la ruta especificada\n","if not cap.isOpened():  # Verificar si se pudo abrir\n","    raise ValueError(f\"No se pudo abrir el video: {VIDEO_PATH}\")\n","\n","# Obtener propiedades del video\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))  # Ancho del video\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))  # Alto del video\n","fps = int(cap.get(cv2.CAP_PROP_FPS))  # FPS real del video\n","total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))  # Total de frames\n","\n","# Ajustar FPS si es necesario\n","if fps != FPS:  # Si el FPS real no coincide con el esperado\n","    logging.warning(f\"El FPS del video ({fps}) no coincide con el esperado ({FPS}). Ajustando...\")\n","    print(f\"El FPS del video ({fps}) no coincide con el esperado ({FPS}). Ajustando...\")\n","    FPS = fps  # Actualizar FPS\n","    CLIP_FRAMES = CLIP_DURATION_SECONDS * FPS  # Recalcular frames por clip\n","    STRIDE_FRAMES = CLIP_FRAMES // 5  # Recalcular solapamiento\n","    TIMESFORMER_FRAMES = CLIP_DURATION_SECONDS * TIMESFORMER_FPS  # Recalcular frames para TimeSformer\n","    frame_buffer = deque(maxlen=CLIP_FRAMES)  # Reiniciar buffer con nuevo tamaño\n","\n","# Configurar video de salida\n","fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")  # Códec para el video de salida\n","out = cv2.VideoWriter(OUTPUT_PATH, fourcc, FPS, (width, height))  # Crear video de salida\n","\n","# Verificar que VideoWriter se inicializó correctamente\n","if not out.isOpened():\n","    raise ValueError(f\"No se pudo inicializar el VideoWriter para: {OUTPUT_PATH}. Verifica permisos y códec.\")\n","\n","\n","\n","\n","# Inicializar contadores y variables de estado\n","frame_count = 0  # Contador de frames procesados\n","written_frames = 0  # Contador de frames escritos en el video de salida\n","violence_detected = False  # Estado de detección de violencia\n","last_prob_violence = 0.0  # Última probabilidad de violencia\n","last_violence_ids = []  # IDs de personas involucradas en el último evento de violencia\n","\n","logging.info(f\"Procesando video: {VIDEO_PATH}\")\n","print(f\"Procesando video: {VIDEO_PATH}\")\n","\n","# Bucle principal para procesar cada frame del video\n","while cap.isOpened():\n","    ret, frame = cap.read()  # Leer el siguiente frame\n","    if not ret:  # Si no hay más frames, salir del bucle\n","        break\n","\n","    frame_count += 1  # Incrementar contador de frames\n","\n","    # Redimensionar frame para YOLOv8n\n","    yolo_frame = cv2.resize(frame, (640, 640), interpolation=cv2.INTER_AREA)  # Redimensionar a 640x640\n","    frame_buffer.append(frame.copy())  # Añadir frame al buffer\n","\n","    # Detección de personas con YOLOv8n\n","    results = yolo_model(yolo_frame, conf=YOLO_CONF_THRESHOLD, classes=0, iou=0.5)  # Detectar personas\n","    detections = []  # Lista para almacenar detecciones\n","    for result in results:\n","        boxes = result.boxes.xyxy.cpu().numpy()  # Coordenadas de los bounding boxes\n","        scores = result.boxes.conf.cpu().numpy()  # Confianzas de las detecciones\n","        scale_x = width / 640  # Factor de escala para ancho\n","        scale_y = height / 640  # Factor de escala para alto\n","        for box, score in zip(boxes, scores):\n","            x1, y1, x2, y2 = box  # Coordenadas en 640x640\n","            x1, x2 = x1 * scale_x, x2 * scale_x  # Escalar a dimensiones originales\n","            y1, y2 = y1 * scale_y, y2 * scale_y  # Escalar a dimensiones originales\n","            detections.append(([x1, y1, x2 - x1, y2 - y1], score, 0))  # Formato para DeepSORT\n","\n","    # Seguimiento de personas con DeepSORT\n","    tracks = deepsort.update_tracks(detections, frame=frame)  # Actualizar trayectorias\n","\n","    # Almacenar trayectorias del frame actual\n","    current_trajectories = []\n","    for track in tracks:\n","        if not track.is_confirmed():  # Ignorar trayectorias no confirmadas\n","            continue\n","        track_id = track.track_id  # Obtener ID de la persona\n","        ltrb = track.to_ltrb()  # Coordenadas del bounding box (left, top, right, bottom)\n","        current_trajectories.append((track_id, ltrb))  # Guardar ID y coordenadas\n","    trajectories[frame_count] = current_trajectories  # Almacenar en el diccionario\n","\n","    # Predecir violencia cada STRIDE_FRAMES (cada 60 frames o 2 segundos)\n","    if frame_count % STRIDE_FRAMES == 0 and len(frame_buffer) == CLIP_FRAMES:\n","        start_frame = max(1, frame_count - CLIP_FRAMES + 1)  # Frame inicial del clip\n","        end_frame = frame_count  # Frame final del clip\n","\n","        pred, prob_violence = predict_violence(list(frame_buffer))  # Predecir violencia\n","        # Post-procesamiento: si la probabilidad está cerca del umbral, no clasificar como violencia\n","        if 0.4 < prob_violence < 0.6:\n","            pred = 0  # Considerar como \"incierto\"\n","        violence_detected = (pred == 1)  # Actualizar estado de violencia\n","        last_prob_violence = prob_violence  # Guardar probabilidad\n","\n","        logging.info(f\"Predicción en frames {start_frame}-{end_frame}: Violencia={'Sí' if violence_detected else 'No'}, Probabilidad={prob_violence:.4f}\")\n","        print(f\"Predicción en frames {start_frame}-{end_frame}: Violencia={'Sí' if violence_detected else 'No'}, Probabilidad={prob_violence:.4f}\")\n","\n","        ids_in_interval = get_ids_in_interval(start_frame, end_frame)  # Obtener IDs en el intervalo\n","        last_violence_ids = ids_in_interval if violence_detected else []  # Guardar IDs si hay violencia\n","\n","        if violence_detected:\n","            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")  # Timestamp del evento\n","            event = f\"Violencia detectada en frames {start_frame}-{end_frame}, Probabilidad: {prob_violence:.4f}, IDs: {ids_in_interval}, Timestamp: {timestamp}\"\n","            logging.info(event)  # Registrar evento\n","            print(event)  # Mostrar evento\n","\n","    # Visualización: dibujar bounding boxes y IDs\n","    for track in tracks:\n","        if not track.is_confirmed():  # Ignorar trayectorias no confirmadas\n","            continue\n","        track_id = track.track_id  # Obtener ID\n","        ltrb = track.to_ltrb()  # Obtener coordenadas\n","        x1, y1, x2, y2 = map(int, ltrb)  # Convertir a enteros\n","        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)  # Dibujar bounding box\n","        cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)  # Mostrar ID\n","\n","    # Mostrar predicción de violencia en el frame\n","    status_text = f\"Violencia: {'Sí' if violence_detected else 'No'} (Prob: {last_prob_violence:.4f})\"\n","    status_color = (0, 0, 255) if violence_detected else (0, 255, 0)  # Rojo si hay violencia, verde si no\n","    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2)  # Mostrar estado\n","\n","    # Mostrar IDs involucrados en violencia\n","    if violence_detected and last_violence_ids:\n","        ids_text = f\"IDs Involucrados: {last_violence_ids}\"\n","        cv2.putText(frame, ids_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)  # Mostrar IDs\n","\n","    # Escribir frame en el video de salida\n","    out.write(frame)  # Guardar frame procesado\n","    written_frames += 1  # Incrementar contador\n","\n","    # Mostrar progreso cada 100 frames\n","    if frame_count % 100 == 0:\n","        logging.info(f\"Procesados {frame_count}/{total_frames} frames\")\n","        print(f\"Procesados {frame_count}/{total_frames} frames\")\n","\n","\n","# Liberar recursos al finalizar\n","cap.release()  # Cerrar el video de entrada\n","out.release()  # Cerrar el video de salida\n","# cv2.destroyAllWindows()  # Cerrar ventanas de OpenCV (si las hubiera)"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1746475812145,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"},"user_tz":240},"id":"0ej2ZbkDGHHQ","outputId":"cee1931e-8d77-492e-b843-d547720e2aa8"},"outputs":[{"output_type":"stream","name":"stdout","text":["Video procesado guardado en: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/output_videos/video_prueba4.2.mp4\n","Total de frames escritos: 150\n"]}],"source":["# Verificar que el video de salida existe y reportar frames escritos\n","if os.path.exists(OUTPUT_PATH):\n","    logging.info(f\"Video procesado guardado en: {OUTPUT_PATH}\")\n","    print(f\"Video procesado guardado en: {OUTPUT_PATH}\")\n","    logging.info(f\"Total de frames escritos: {written_frames}\")\n","    print(f\"Total de frames escritos: {written_frames}\")\n","else:\n","    logging.error(f\"No se encontró el video de salida en: {OUTPUT_PATH}. Verifica permisos y códec.\")\n","    print(f\"No se encontró el video de salida en: {OUTPUT_PATH}. Verifica permisos y códec.\")"]},{"cell_type":"markdown","source":["# *EXPORTAR EL MODELO EN ONNX*"],"metadata":{"id":"-WDd1XXmqPnl"}},{"cell_type":"code","source":["from transformers import TimesformerForVideoClassification\n","\n","# Cargar el modelo desde el directorio que contiene model.safetensors\n","model_dir = \"path/to/your/model/directory\"  # Reemplaza con la ruta al directorio que contiene model.safetensors\n","model = TimesformerForVideoClassification.from_pretrained(model_dir)\n","\n","# Mover el modelo a evaluación y al dispositivo adecuado\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.eval()\n","model.to(device)"],"metadata":{"id":"n7tEhXdeqVDT"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[{"file_id":"1L_wGZgQ2cCWwHyI2obEkH1v2Hz_vaK15","timestamp":1744228224334}],"authorship_tag":"ABX9TyMYuovFOofh8cuU52mp70hF"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"b0a3a945159c4c8fb44990cd0439b5cd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4c3fe7b0969b4a6da5689723af2e9421","IPY_MODEL_c94ab07fdffe419bbf04ebe2d678caeb","IPY_MODEL_7b28a84925cb4ed08c6ff2ccc0bac0be"],"layout":"IPY_MODEL_ccac8a5c33374d99a52bc6a074658d02"}},"4c3fe7b0969b4a6da5689723af2e9421":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8108fc9bcce24936b51e7e4ce2ffa4c2","placeholder":"​","style":"IPY_MODEL_63ee978502dc4488a6ba1fcdacf5edd5","value":"preprocessor_config.json: 100%"}},"c94ab07fdffe419bbf04ebe2d678caeb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7a65755a18d949b5b6d7f94628d2c7df","max":412,"min":0,"orientation":"horizontal","style":"IPY_MODEL_758c31d6524a42d5901800f0253929da","value":412}},"7b28a84925cb4ed08c6ff2ccc0bac0be":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05dd7d9a1ef3410f96c14f2e04503ade","placeholder":"​","style":"IPY_MODEL_b9b13b826a0d41db99fe8989632055ce","value":" 412/412 [00:00&lt;00:00, 36.4kB/s]"}},"ccac8a5c33374d99a52bc6a074658d02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8108fc9bcce24936b51e7e4ce2ffa4c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"63ee978502dc4488a6ba1fcdacf5edd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7a65755a18d949b5b6d7f94628d2c7df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"758c31d6524a42d5901800f0253929da":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"05dd7d9a1ef3410f96c14f2e04503ade":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9b13b826a0d41db99fe8989632055ce":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}