{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMf03FL7k2p9auq2WQRNa/U"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"805dbabf96834438b07c7e901ce0972a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c1c0205720fe497d99a7de285a1841cd","IPY_MODEL_d95cba15d29540bd9c9ef8ed349405c3","IPY_MODEL_2e337e0fc5fe497b924e476a3d086f01"],"layout":"IPY_MODEL_15a4c279bbf34e60a9de150c2c85ed13"}},"c1c0205720fe497d99a7de285a1841cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23843a91e4ea4295b275299f75fb52ea","placeholder":"​","style":"IPY_MODEL_6b4a5a759c014b909f317cc9d18a8c79","value":"preprocessor_config.json: 100%"}},"d95cba15d29540bd9c9ef8ed349405c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_56c29cc38cad4351bfcab083aa3cfe08","max":412,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09cb7af628d6444cbae9c7e9d893087e","value":412}},"2e337e0fc5fe497b924e476a3d086f01":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_71b48a7909f946e2b6a97ddd15ae56a5","placeholder":"​","style":"IPY_MODEL_dac43e4d101b410a8cf5531339af0cc1","value":" 412/412 [00:00&lt;00:00, 33.8kB/s]"}},"15a4c279bbf34e60a9de150c2c85ed13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23843a91e4ea4295b275299f75fb52ea":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b4a5a759c014b909f317cc9d18a8c79":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"56c29cc38cad4351bfcab083aa3cfe08":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09cb7af628d6444cbae9c7e9d893087e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"71b48a7909f946e2b6a97ddd15ae56a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dac43e4d101b410a8cf5531339af0cc1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# **PROTOTIPO FUNCIONAL DE DETECCIÓN DE VIOLENCIA FÍSICAS**"],"metadata":{"id":"Hz_-DBvnjHy_"}},{"cell_type":"code","source":["!pip install ultralytics deep-sort-realtime transformers torch opencv-python numpy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxrHBHxTmpmW","executionInfo":{"status":"ok","timestamp":1744164453627,"user_tz":240,"elapsed":121174,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}},"outputId":"0a0e90a0-9ba6-4dae-9ecf-c0c035cd41a3"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting ultralytics\n","  Downloading ultralytics-8.3.105-py3-none-any.whl.metadata (37 kB)\n","Collecting deep-sort-realtime\n","  Downloading deep_sort_realtime-1.3.2-py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n","Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n","Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n","Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n","Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n","Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n","Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n","Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n","Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n","Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n","  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.57.0)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n","Downloading ultralytics-8.3.105-py3-none-any.whl (994 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m994.0/994.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading deep_sort_realtime-1.3.2-py3-none-any.whl (8.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m34.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m33.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n","Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, deep-sort-realtime, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed deep-sort-realtime-1.3.2 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.105 ultralytics-thop-2.0.14\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fZS8BXQ1GS5O","executionInfo":{"status":"ok","timestamp":1744168519960,"user_tz":240,"elapsed":5967,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}},"outputId":"d9b82d45-a498-4e7d-80fa-66ea76696ef5"},"execution_count":53,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## PROCESAMIENTO DE VIDEOS A 1280x720"],"metadata":{"id":"829xodAIS3U5"}},{"cell_type":"code","source":["import os\n","from moviepy.editor import VideoFileClip\n","\n","# PARÁMETROS PREDEFINIDOS - MODIFICA ESTOS VALORES\n","INPUT_PATH = \"/content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba3.mp4\"  # Ruta al video de entrada\n","OUTPUT_FOLDER = \"/content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados\"  # Carpeta donde se guardará el video procesado\n","START_TIME = 9  # Tiempo de inicio para el recorte (en segundos)\n","END_TIME = 14  # Tiempo de fin para el recorte (en segundos)\n","TARGET_RESOLUTION = (1280, 720)  # Resolución objetivo (ancho, alto)\n","\n","def process_video(input_path, output_folder, start_time, end_time, target_resolution=(1280, 720)):\n","    \"\"\"\n","    Procesa un video recortándolo por un rango de tiempo específico y ajustando su resolución.\n","    Mantiene el nombre original del archivo de entrada.\n","\n","    Args:\n","        input_path (str): Ruta al video de entrada\n","        output_folder (str): Carpeta donde se guardará el video procesado\n","        start_time (float): Tiempo de inicio para el recorte (en segundos)\n","        end_time (float): Tiempo de fin para el recorte (en segundos)\n","        target_resolution (tuple): Resolución objetivo en formato (ancho, alto)\n","\n","    Returns:\n","        str: Ruta del archivo de salida procesado\n","    \"\"\"\n","    # Verificar si el archivo de entrada existe\n","    if not os.path.exists(input_path):\n","        raise FileNotFoundError(f\"El archivo de video no existe: {input_path}\")\n","\n","    # Crear la carpeta de salida si no existe\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","        print(f\"Carpeta de salida creada: {output_folder}\")\n","\n","    # Obtener el nombre base del archivo de entrada sin extensión\n","    base_name = os.path.basename(input_path)\n","    file_name, file_ext = os.path.splitext(base_name)\n","\n","    # Mantener el nombre original del archivo\n","    output_name = f\"{file_name}{file_ext}\"\n","    output_path = os.path.join(output_folder, output_name)\n","\n","    try:\n","        # Cargar el video\n","        print(f\"Cargando video: {input_path}\")\n","        video = VideoFileClip(input_path)\n","\n","        # Obtener información del video original\n","        original_duration = video.duration\n","        original_size = video.size\n","        print(f\"Información del video original:\")\n","        print(f\"- Duración: {original_duration:.2f} segundos\")\n","        print(f\"- Resolución: {original_size[0]}x{original_size[1]}\")\n","\n","        # Validar tiempos de recorte\n","        if start_time < 0 or end_time > original_duration or start_time >= end_time:\n","            raise ValueError(f\"Tiempos de recorte inválidos: {start_time}s - {end_time}s. \" +\n","                            f\"El video tiene una duración de {original_duration:.2f}s\")\n","\n","        # Recortar el video dentro del rango especificado\n","        print(f\"Recortando video del segundo {start_time} al {end_time}\")\n","        video_trimmed = video.subclip(start_time, end_time)\n","\n","        # Redimensionar el video si es necesario\n","        if original_size[0] != target_resolution[0] or original_size[1] != target_resolution[1]:\n","            print(f\"Redimensionando video de {original_size[0]}x{original_size[1]} a {target_resolution[0]}x{target_resolution[1]}\")\n","            # Usar redimensionamiento compatible con moviepy 1.0.3 y Pillow 6.2.2\n","            video_processed = video_trimmed.resize(target_resolution)\n","        else:\n","            print(\"El video ya tiene la resolución objetivo. No es necesario redimensionar.\")\n","            video_processed = video_trimmed\n","\n","        # Guardar el video procesado con alta calidad\n","        print(f\"Guardando video procesado: {output_path}\")\n","\n","        # Configurar parámetros de codificación para mantener buena calidad\n","        video_processed.write_videofile(\n","            output_path,\n","            codec=\"libx264\",\n","            audio_codec=\"aac\",\n","            bitrate=\"8000k\",\n","            preset=\"slow\",  # Mejor calidad\n","            threads=4,\n","            fps=video.fps,  # Mantener la misma tasa de fotogramas\n","            temp_audiofile=\"temp-audio.m4a\",\n","            remove_temp=True\n","        )\n","\n","        # Obtener información del video procesado\n","        print(f\"Video procesado guardado exitosamente:\")\n","        print(f\"- Duración: {end_time - start_time:.2f} segundos\")\n","        print(f\"- Resolución: {target_resolution[0]}x{target_resolution[1]}\")\n","\n","        # Limpiar\n","        video.close()\n","        video_processed.close()\n","\n","        return output_path\n","\n","    except Exception as e:\n","        print(f\"Error al procesar el video: {str(e)}\")\n","        # Asegurar que los recursos se liberen en caso de error\n","        try:\n","            video.close()\n","        except:\n","            pass\n","        try:\n","            video_processed.close()\n","        except:\n","            pass\n","        raise\n","\n","# Punto de entrada principal del script\n","if __name__ == \"__main__\":\n","    print(\"=\" * 50)\n","    print(\"PROCESADOR DE VIDEO: RECORTE Y REDIMENSIONAMIENTO\")\n","    print(\"=\" * 50)\n","    print(f\"Video de entrada: {INPUT_PATH}\")\n","    print(f\"Carpeta de salida: {OUTPUT_FOLDER}\")\n","    print(f\"Rango de recorte: {START_TIME}s - {END_TIME}s\")\n","    print(f\"Resolución objetivo: {TARGET_RESOLUTION[0]}x{TARGET_RESOLUTION[1]}\")\n","    print(\"=\" * 50)\n","\n","    try:\n","        # Ejecutar procesamiento con los parámetros predefinidos\n","        result = process_video(INPUT_PATH, OUTPUT_FOLDER, START_TIME, END_TIME, TARGET_RESOLUTION)\n","        print(\"\\n\" + \"=\" * 50)\n","        print(f\"PROCESO COMPLETADO EXITOSAMENTE\")\n","        print(f\"Video procesado guardado en: {result}\")\n","        print(\"=\" * 50)\n","    except Exception as e:\n","        print(\"\\n\" + \"=\" * 50)\n","        print(f\"ERROR: {str(e)}\")\n","        print(\"=\" * 50)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YA0ldpkg9VIH","executionInfo":{"status":"ok","timestamp":1744147951233,"user_tz":240,"elapsed":3323,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}},"outputId":"d720feed-84f7-4b3e-d15f-cd696525cbd6"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n","  if event.key is 'enter':\n","\n"]},{"output_type":"stream","name":"stdout","text":["==================================================\n","PROCESADOR DE VIDEO: RECORTE Y REDIMENSIONAMIENTO\n","==================================================\n","Video de entrada: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba3.mp4\n","Carpeta de salida: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados\n","Rango de recorte: 9s - 14s\n","Resolución objetivo: 1280x720\n","==================================================\n","Cargando video: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba3.mp4\n","Información del video original:\n","- Duración: 5.00 segundos\n","- Resolución: 768x432\n","Error al procesar el video: Tiempos de recorte inválidos: 9s - 14s. El video tiene una duración de 5.00s\n","\n","==================================================\n","ERROR: Tiempos de recorte inválidos: 9s - 14s. El video tiene una duración de 5.00s\n","==================================================\n"]}]},{"cell_type":"code","source":["import os\n","from moviepy.editor import VideoFileClip\n","\n","# PARÁMETROS PREDEFINIDOS - MODIFICA ESTOS VALORES\n","INPUT_PATH = \"/content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba10.mp4\"  # Ruta al video de entrada\n","OUTPUT_FOLDER = \"/content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados\"  # Carpeta donde se guardará el video procesado\n","TARGET_RESOLUTION = (1280, 720)  # Resolución objetivo (ancho, alto)\n","\n","def process_video(input_path, output_folder, target_resolution=(1280, 720)):\n","    \"\"\"\n","    Procesa un video ajustando su resolución.\n","    Mantiene el nombre original del archivo de entrada.\n","\n","    Args:\n","        input_path (str): Ruta al video de entrada\n","        output_folder (str): Carpeta donde se guardará el video procesado\n","        target_resolution (tuple): Resolución objetivo en formato (ancho, alto)\n","\n","    Returns:\n","        str: Ruta del archivo de salida procesado\n","    \"\"\"\n","    if not os.path.exists(input_path):\n","        raise FileNotFoundError(f\"El archivo de video no existe: {input_path}\")\n","\n","    if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","        print(f\"Carpeta de salida creada: {output_folder}\")\n","\n","    base_name = os.path.basename(input_path)\n","    file_name, file_ext = os.path.splitext(base_name)\n","    output_name = f\"{file_name}{file_ext}\"\n","    output_path = os.path.join(output_folder, output_name)\n","\n","    try:\n","        print(f\"Cargando video: {input_path}\")\n","        video = VideoFileClip(input_path)\n","\n","        original_size = video.size\n","        original_duration = video.duration\n","        print(f\"Información del video original:\")\n","        print(f\"- Duración: {original_duration:.2f} segundos\")\n","        print(f\"- Resolución: {original_size[0]}x{original_size[1]}\")\n","\n","        if original_size != list(target_resolution):\n","            print(f\"Redimensionando video a {target_resolution[0]}x{target_resolution[1]}\")\n","            video_processed = video.resize(target_resolution)\n","        else:\n","            print(\"El video ya tiene la resolución objetivo. No es necesario redimensionar.\")\n","            video_processed = video\n","\n","        print(f\"Guardando video procesado: {output_path}\")\n","        video_processed.write_videofile(\n","            output_path,\n","            codec=\"libx264\",\n","            audio_codec=\"aac\",\n","            bitrate=\"8000k\",\n","            preset=\"slow\",\n","            threads=4,\n","            fps=video.fps,\n","            temp_audiofile=\"temp-audio.m4a\",\n","            remove_temp=True\n","        )\n","\n","        print(f\"Video procesado guardado exitosamente.\")\n","        print(f\"- Duración: {original_duration:.2f} segundos\")\n","        print(f\"- Resolución: {target_resolution[0]}x{target_resolution[1]}\")\n","\n","        video.close()\n","        video_processed.close()\n","\n","        return output_path\n","\n","    except Exception as e:\n","        print(f\"Error al procesar el video: {str(e)}\")\n","        try:\n","            video.close()\n","        except:\n","            pass\n","        try:\n","            video_processed.close()\n","        except:\n","            pass\n","        raise\n","\n","# Punto de entrada principal del script\n","if __name__ == \"__main__\":\n","    print(\"=\" * 50)\n","    print(\"PROCESADOR DE VIDEO: SOLO REDIMENSIONAMIENTO\")\n","    print(\"=\" * 50)\n","    print(f\"Video de entrada: {INPUT_PATH}\")\n","    print(f\"Carpeta de salida: {OUTPUT_FOLDER}\")\n","    print(f\"Resolución objetivo: {TARGET_RESOLUTION[0]}x{TARGET_RESOLUTION[1]}\")\n","    print(\"=\" * 50)\n","\n","    try:\n","        result = process_video(INPUT_PATH, OUTPUT_FOLDER, TARGET_RESOLUTION)\n","        print(\"\\n\" + \"=\" * 50)\n","        print(\"PROCESO COMPLETADO EXITOSAMENTE\")\n","        print(f\"Video procesado guardado en: {result}\")\n","        print(\"=\" * 50)\n","    except Exception as e:\n","        print(\"\\n\" + \"=\" * 50)\n","        print(f\"ERROR: {str(e)}\")\n","        print(\"=\" * 50)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pJaJevhEVXXw","executionInfo":{"status":"ok","timestamp":1744168595789,"user_tz":240,"elapsed":30205,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}},"outputId":"91400ecf-3c5c-447a-d259-3cb37cf38389"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["==================================================\n","PROCESADOR DE VIDEO: SOLO REDIMENSIONAMIENTO\n","==================================================\n","Video de entrada: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba10.mp4\n","Carpeta de salida: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados\n","Resolución objetivo: 1280x720\n","==================================================\n","Cargando video: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba10.mp4\n","Información del video original:\n","- Duración: 5.00 segundos\n","- Resolución: 1280x720\n","El video ya tiene la resolución objetivo. No es necesario redimensionar.\n","Guardando video procesado: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4\n","Moviepy - Building video /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4.\n","MoviePy - Writing audio in temp-audio.m4a\n"]},{"output_type":"stream","name":"stderr","text":[]},{"output_type":"stream","name":"stdout","text":["MoviePy - Done.\n","Moviepy - Writing video /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4\n","\n"]},{"output_type":"stream","name":"stderr","text":["t:  99%|█████████▉| 150/151 [00:15<00:00,  5.43it/s, now=None]WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:123: UserWarning: Warning: in file /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_nuevos/video_prueba10.mp4, 2764800 bytes wanted but 0 bytes read,at frame 150/151, at time 5.00/5.00 sec. Using the last valid frame instead.\n","  warnings.warn(\"Warning: in file %s, \"%(self.filename)+\n","\n"]},{"output_type":"stream","name":"stdout","text":["Moviepy - Done !\n","Moviepy - video ready /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4\n","Video procesado guardado exitosamente.\n","- Duración: 5.00 segundos\n","- Resolución: 1280x720\n","\n","==================================================\n","PROCESO COMPLETADO EXITOSAMENTE\n","Video procesado guardado en: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4\n","==================================================\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"FbLHPVEQIoWX"}},{"cell_type":"code","source":["!pip install ultralytics deep-sort-realtime transformers torch opencv-python numpy"],"metadata":{"id":"f_OTfSUOT0Qv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import cv2\n","import numpy as np\n","import torch\n","from ultralytics import YOLO\n","from deep_sort_realtime.deepsort_tracker import DeepSort\n","from transformers import TimesformerForVideoClassification, AutoImageProcessor\n","import logging\n","import os\n","from collections import deque, defaultdict\n","from datetime import datetime"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dkI7LYU0jWJO","executionInfo":{"status":"ok","timestamp":1744164571945,"user_tz":240,"elapsed":24488,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}},"outputId":"8c31b516-3813-4cf3-f13d-ce623eecb21e"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Creating new Ultralytics Settings v0.0.6 file ✅ \n","View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n","Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.flush_and_unmount()\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZEIt0raQXj3w","executionInfo":{"status":"ok","timestamp":1744148784844,"user_tz":240,"elapsed":7359,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}},"outputId":"43035386-a2a8-478c-f7ec-0e158a813ca4"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Configurar logging\n","logging.basicConfig(\n","    level=logging.INFO,\n","    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n","    handlers=[\n","        logging.FileHandler(\"violence_detection_prototype.log\"),\n","        logging.StreamHandler()\n","    ]\n",")\n","\n","# Definir rutas\n","BASE_PATH = \"/content/drive/MyDrive/Proyecto IA-3/violence_school_project/\"\n","YOLO_MODEL_PATH = os.path.join(BASE_PATH, \"models/yolo_V2/save_models/finetune/weights/best.pt\")  # Modelo YOLO entrenado\n","TIMESFORMER_MODEL_PATH = os.path.join(BASE_PATH, \"models/timesformer/run_finetune_20250408_003512/best_timesformer_finetune.pt\")\n","# TIMESFORMER_MODEL_PATH = os.path.join(BASE_PATH, \"models/timesformer/run_20250407_115035/best_timesformer_transfer.pt\")\n","VIDEO_PATH = os.path.join(BASE_PATH, \"videos/videos_procesados/video_prueba10.mp4\")  # Video de prueba\n","OUTPUT_PATH = os.path.join(BASE_PATH, \"output_videos/video_prueba10.mp4\")\n","\n","# Verificar que los modelos existan\n","if not os.path.exists(YOLO_MODEL_PATH):\n","    raise FileNotFoundError(f\"No se encontró el modelo YOLO en: {YOLO_MODEL_PATH}\")\n","if not os.path.exists(TIMESFORMER_MODEL_PATH):\n","    raise FileNotFoundError(f\"No se encontró el modelo TimeSformer en: {TIMESFORMER_MODEL_PATH}\")\n","\n","# Crear la carpeta de salida si no existe\n","output_dir = os.path.dirname(OUTPUT_PATH)\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","    logging.info(f\"Carpeta de salida creada: {output_dir}\")"],"metadata":{"id":"BX1j-XZHjeye","executionInfo":{"status":"ok","timestamp":1744168668198,"user_tz":240,"elapsed":18,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}}},"execution_count":60,"outputs":[]},{"cell_type":"code","source":["# Configurar dispositivo\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","logging.info(f\"Usando dispositivo: {device}\")\n","print(f\"Usando dispositivo: {device}\")\n","\n","# Cargar modelo YOLO entrenado\n","yolo_model = YOLO(YOLO_MODEL_PATH)\n","logging.info(f\"Modelo YOLO cargado desde: {YOLO_MODEL_PATH}\")\n","print(f\"Modelo YOLO cargado desde: {YOLO_MODEL_PATH}\")\n","\n","# Cargar DeepSORT\n","deepsort = DeepSort(max_age=30, n_init=3, nn_budget=100)\n","logging.info(\"DeepSORT inicializado.\")\n","print(\"DeepSORT inicializado.\")\n","\n","# Cargar modelo TimeSformer\n","timesformer_model = TimesformerForVideoClassification.from_pretrained(TIMESFORMER_MODEL_PATH)\n","timesformer_model.to(device)\n","timesformer_model.eval()\n","processor = AutoImageProcessor.from_pretrained(\"facebook/timesformer-base-finetuned-k400\")\n","logging.info(f\"Modelo TimeSformer cargado desde: {TIMESFORMER_MODEL_PATH}\")\n","print(f\"Modelo TimeSformer cargado desde: {TIMESFORMER_MODEL_PATH}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":269,"referenced_widgets":["805dbabf96834438b07c7e901ce0972a","c1c0205720fe497d99a7de285a1841cd","d95cba15d29540bd9c9ef8ed349405c3","2e337e0fc5fe497b924e476a3d086f01","15a4c279bbf34e60a9de150c2c85ed13","23843a91e4ea4295b275299f75fb52ea","6b4a5a759c014b909f317cc9d18a8c79","56c29cc38cad4351bfcab083aa3cfe08","09cb7af628d6444cbae9c7e9d893087e","71b48a7909f946e2b6a97ddd15ae56a5","dac43e4d101b410a8cf5531339af0cc1"]},"id":"OFAd66h5jjw3","executionInfo":{"status":"ok","timestamp":1744165005332,"user_tz":240,"elapsed":15467,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}},"outputId":"ef6d053d-46ef-49d8-af11-34aa4aaf42bd"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Usando dispositivo: cuda\n","Modelo YOLO cargado desde: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/models/yolo_V2/save_models/finetune/weights/best.pt\n","DeepSORT inicializado.\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["preprocessor_config.json:   0%|          | 0.00/412 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"805dbabf96834438b07c7e901ce0972a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"]},{"output_type":"stream","name":"stdout","text":["Modelo TimeSformer cargado desde: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/models/timesformer/run_finetune_20250408_003512/best_timesformer_finetune.pt\n"]}]},{"cell_type":"code","execution_count":61,"metadata":{"id":"jzX701s7ivU0","executionInfo":{"status":"ok","timestamp":1744168671849,"user_tz":240,"elapsed":5,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}}},"outputs":[],"source":["# Parámetros\n","CLIP_DURATION_SECONDS = 10  # Aumentar a 10 segundos para más contexto\n","FPS = 30  # FPS del video de entrada (valor inicial, se ajustará)\n","CLIP_FRAMES = CLIP_DURATION_SECONDS * FPS\n","STRIDE_FRAMES = CLIP_FRAMES // 5  # Procesar cada 2 segundos (más solapamiento)\n","TIMESFORMER_FPS = 15\n","TIMESFORMER_FRAMES = CLIP_DURATION_SECONDS * TIMESFORMER_FPS\n","NUM_FRAMES_TIMESFORMER = 8\n","THRESHOLD_VIOLENCE = 0.6  # Aumentar el umbral para reducir falsos positivos\n","YOLO_CONF_THRESHOLD = 0.65  # Aumentar para reducir detecciones duplicadas\n","\n","# Cola para almacenar frames para TimeSformer\n","frame_buffer = deque(maxlen=CLIP_FRAMES)\n","\n","# Estructura para almacenar trayectorias de DeepSORT por frame\n","trajectories = defaultdict(list)\n","\n","# Función para preprocesar frames para TimeSformer\n","def preprocess_frames_for_timesformer(frames, num_frames=NUM_FRAMES_TIMESFORMER, target_size=(224, 224), target_fps=TIMESFORMER_FPS):\n","    # Reducir FPS a 15 FPS\n","    total_frames = len(frames)\n","    target_frame_count = int((total_frames / FPS) * target_fps)\n","    frame_indices = np.linspace(0, total_frames - 1, target_frame_count, dtype=int)\n","    selected_frames = [frames[i] for i in frame_indices]\n","\n","    # Muestrear 8 frames uniformemente\n","    if len(selected_frames) < num_frames:\n","        raise ValueError(f\"No hay suficientes frames después de reducir FPS: {len(selected_frames)} < {num_frames}\")\n","\n","    sample_indices = np.linspace(0, len(selected_frames) - 1, num_frames, dtype=int)\n","    final_frames = [selected_frames[i] for i in sample_indices]\n","\n","    # Redimensionar y convertir a RGB\n","    processed_frames = []\n","    for frame in final_frames:\n","        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n","        h, w = frame.shape[:2]\n","        ratio = min(target_size[0] / w, target_size[1] / h)\n","        new_w, new_h = int(w * ratio), int(h * ratio)\n","        resized_frame = cv2.resize(frame, (new_w, new_h), interpolation=cv2.INTER_AREA)\n","        padded_frame = np.zeros((target_size[1], target_size[0], 3), dtype=np.uint8)\n","        pad_top = (target_size[1] - new_h) // 2\n","        pad_left = (target_size[0] - new_w) // 2\n","        padded_frame[pad_top:pad_top + new_h, pad_left:pad_left + new_w] = resized_frame\n","        processed_frames.append(padded_frame)\n","\n","    # Preprocesar con AutoImageProcessor\n","    inputs = processor(processed_frames, return_tensors=\"pt\")\n","    pixel_values = inputs[\"pixel_values\"].to(device)\n","    return pixel_values\n","\n","# Función para predecir violencia con TimeSformer\n","def predict_violence(frames):\n","    try:\n","        pixel_values = preprocess_frames_for_timesformer(frames)\n","        with torch.no_grad():\n","            outputs = timesformer_model(pixel_values=pixel_values)\n","            logits = outputs.logits\n","            probs = torch.softmax(logits, dim=1)\n","            prob_violence = probs[0, 1].item()\n","            pred = 1 if prob_violence > THRESHOLD_VIOLENCE else 0\n","        return pred, prob_violence\n","    except Exception as e:\n","        logging.error(f\"Error al predecir violencia: {str(e)}\")\n","        return 0, 0.0\n","\n","# Función para obtener los IDs presentes en un intervalo de frames\n","def get_ids_in_interval(start_frame, end_frame):\n","    ids_in_interval = set()\n","    for frame_num in range(start_frame, end_frame + 1):\n","        if frame_num in trajectories:\n","            for track_id, _ in trajectories[frame_num]:\n","                ids_in_interval.add(track_id)\n","    return sorted(list(ids_in_interval))\n"]},{"cell_type":"code","source":["# Cargar video\n","cap = cv2.VideoCapture(VIDEO_PATH)\n","if not cap.isOpened():\n","    raise ValueError(f\"No se pudo abrir el video: {VIDEO_PATH}\")\n","\n","# Obtener propiedades del video\n","width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n","height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","fps = int(cap.get(cv2.CAP_PROP_FPS))\n","total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n","\n","# Ajustar FPS si es necesario\n","if fps != FPS:\n","    logging.warning(f\"El FPS del video ({fps}) no coincide con el esperado ({FPS}). Ajustando...\")\n","    print(f\"El FPS del video ({fps}) no coincide con el esperado ({FPS}). Ajustando...\")\n","    FPS = fps\n","    CLIP_FRAMES = CLIP_DURATION_SECONDS * FPS\n","    STRIDE_FRAMES = CLIP_FRAMES // 5\n","    TIMESFORMER_FRAMES = CLIP_DURATION_SECONDS * TIMESFORMER_FPS\n","    frame_buffer = deque(maxlen=CLIP_FRAMES)\n","\n","# Configurar video de salida\n","fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n","out = cv2.VideoWriter(OUTPUT_PATH, fourcc, FPS, (width, height))\n","\n","# Verificar que VideoWriter se inicializó correctamente\n","if not out.isOpened():\n","    raise ValueError(f\"No se pudo inicializar el VideoWriter para: {OUTPUT_PATH}. Verifica permisos y códec.\")\n","\n","frame_count = 0\n","written_frames = 0\n","violence_detected = False\n","last_prob_violence = 0.0\n","last_violence_ids = []\n","\n","logging.info(f\"Procesando video: {VIDEO_PATH}\")\n","print(f\"Procesando video: {VIDEO_PATH}\")\n","\n","while cap.isOpened():\n","    ret, frame = cap.read()\n","    if not ret:\n","        break\n","\n","    frame_count += 1\n","\n","    # Redimensionar frame para YOLO\n","    yolo_frame = cv2.resize(frame, (640, 640), interpolation=cv2.INTER_AREA)\n","    frame_buffer.append(frame.copy())\n","\n","    # Detección con YOLO\n","    results = yolo_model(yolo_frame, conf=YOLO_CONF_THRESHOLD, classes=0, iou=0.5)\n","    detections = []\n","    for result in results:\n","        boxes = result.boxes.xyxy.cpu().numpy()\n","        scores = result.boxes.conf.cpu().numpy()\n","        scale_x = width / 640\n","        scale_y = height / 640\n","        for box, score in zip(boxes, scores):\n","            x1, y1, x2, y2 = box\n","            x1, x2 = x1 * scale_x, x2 * scale_x\n","            y1, y2 = y1 * scale_y, y2 * scale_y\n","            detections.append(([x1, y1, x2 - x1, y2 - y1], score, 0))\n","\n","    # Seguimiento con DeepSORT\n","    tracks = deepsort.update_tracks(detections, frame=frame)\n","\n","    # Almacenar trayectorias\n","    current_trajectories = []\n","    for track in tracks:\n","        if not track.is_confirmed():\n","            continue\n","        track_id = track.track_id\n","        ltrb = track.to_ltrb()\n","        current_trajectories.append((track_id, ltrb))\n","    trajectories[frame_count] = current_trajectories\n","\n","    # Predecir violencia cada STRIDE_FRAMES\n","    if frame_count % STRIDE_FRAMES == 0 and len(frame_buffer) == CLIP_FRAMES:\n","        start_frame = max(1, frame_count - CLIP_FRAMES + 1)\n","        end_frame = frame_count\n","\n","        pred, prob_violence = predict_violence(list(frame_buffer))\n","        # Post-procesamiento: si la probabilidad está cerca del umbral, no clasificar como violencia\n","        if 0.4 < prob_violence < 0.6:\n","            pred = 0  # Considerar como \"incierto\"\n","        violence_detected = (pred == 1)\n","        last_prob_violence = prob_violence\n","\n","        logging.info(f\"Predicción en frames {start_frame}-{end_frame}: Violencia={'Sí' if violence_detected else 'No'}, Probabilidad={prob_violence:.4f}\")\n","        print(f\"Predicción en frames {start_frame}-{end_frame}: Violencia={'Sí' if violence_detected else 'No'}, Probabilidad={prob_violence:.4f}\")\n","\n","        ids_in_interval = get_ids_in_interval(start_frame, end_frame)\n","        last_violence_ids = ids_in_interval if violence_detected else []\n","\n","        if violence_detected:\n","            timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n","            event = f\"Violencia detectada en frames {start_frame}-{end_frame}, Probabilidad: {prob_violence:.4f}, IDs: {ids_in_interval}, Timestamp: {timestamp}\"\n","            logging.info(event)\n","            print(event)\n","\n","    # Visualización\n","    for track in tracks:\n","        if not track.is_confirmed():\n","            continue\n","        track_id = track.track_id\n","        ltrb = track.to_ltrb()\n","        x1, y1, x2, y2 = map(int, ltrb)\n","        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n","        cv2.putText(frame, f\"ID: {track_id}\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n","\n","    # Mostrar predicción de violencia\n","    status_text = f\"Violencia: {'Sí' if violence_detected else 'No'} (Prob: {last_prob_violence:.4f})\"\n","    status_color = (0, 0, 255) if violence_detected else (0, 255, 0)\n","    cv2.putText(frame, status_text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, status_color, 2)\n","\n","    # Mostrar IDs involucrados en violencia\n","    if violence_detected and last_violence_ids:\n","        ids_text = f\"IDs Involucrados: {last_violence_ids}\"\n","        cv2.putText(frame, ids_text, (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n","\n","    # Escribir frame en el video de salida\n","    out.write(frame)\n","    written_frames += 1\n","\n","    # Mostrar progreso\n","    if frame_count % 100 == 0:\n","        logging.info(f\"Procesados {frame_count}/{total_frames} frames\")\n","        print(f\"Procesados {frame_count}/{total_frames} frames\")\n","\n","# Liberar recursos\n","cap.release()\n","out.release()\n","cv2.destroyAllWindows()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gy-h6Mfqj52P","executionInfo":{"status":"ok","timestamp":1744168685073,"user_tz":240,"elapsed":9850,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}},"outputId":"5870f2ed-e363-4c25-b5db-6aed27654e7a"},"execution_count":62,"outputs":[{"output_type":"stream","name":"stdout","text":["Procesando video: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/videos/videos_procesados/video_prueba10.mp4\n","\n","0: 640x640 2 personas, 14.7ms\n","Speed: 6.3ms preprocess, 14.7ms inference, 10.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 9.9ms\n","Speed: 4.9ms preprocess, 9.9ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 14.0ms\n","Speed: 6.2ms preprocess, 14.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 10.1ms\n","Speed: 2.9ms preprocess, 10.1ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 9.7ms\n","Speed: 2.9ms preprocess, 9.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 10.4ms\n","Speed: 3.1ms preprocess, 10.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 10.4ms\n","Speed: 3.1ms preprocess, 10.4ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 12.4ms\n","Speed: 3.1ms preprocess, 12.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 13.9ms\n","Speed: 3.1ms preprocess, 13.9ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 10.2ms\n","Speed: 4.3ms preprocess, 10.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 10.4ms\n","Speed: 5.5ms preprocess, 10.4ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 10.5ms\n","Speed: 5.9ms preprocess, 10.5ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 12.8ms\n","Speed: 3.0ms preprocess, 12.8ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 16.8ms\n","Speed: 2.8ms preprocess, 16.8ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 14.2ms\n","Speed: 3.1ms preprocess, 14.2ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 9.4ms\n","Speed: 2.7ms preprocess, 9.4ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 12.0ms\n","Speed: 3.0ms preprocess, 12.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 9.7ms\n","Speed: 2.8ms preprocess, 9.7ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 10.0ms\n","Speed: 2.8ms preprocess, 10.0ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 14.6ms\n","Speed: 2.8ms preprocess, 14.6ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 14.0ms\n","Speed: 6.5ms preprocess, 14.0ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 10.1ms\n","Speed: 3.0ms preprocess, 10.1ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 10.9ms\n","Speed: 4.6ms preprocess, 10.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 5 personas, 11.3ms\n","Speed: 3.1ms preprocess, 11.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 18.1ms\n","Speed: 3.6ms preprocess, 18.1ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 5 personas, 11.2ms\n","Speed: 3.2ms preprocess, 11.2ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 13.3ms\n","Speed: 3.1ms preprocess, 13.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 5 personas, 16.4ms\n","Speed: 3.2ms preprocess, 16.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 15.4ms\n","Speed: 2.9ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 12.9ms\n","Speed: 3.0ms preprocess, 12.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 14.2ms\n","Speed: 3.1ms preprocess, 14.2ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 12.9ms\n","Speed: 3.1ms preprocess, 12.9ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 14.0ms\n","Speed: 4.3ms preprocess, 14.0ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 8.1ms\n","Speed: 2.7ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 7.5ms\n","Speed: 2.4ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 8.1ms\n","Speed: 2.8ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 8.2ms\n","Speed: 3.0ms preprocess, 8.2ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 8.0ms\n","Speed: 3.1ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 9.5ms\n","Speed: 2.7ms preprocess, 9.5ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 7.9ms\n","Speed: 3.2ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 7.9ms\n","Speed: 3.4ms preprocess, 7.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 7.9ms\n","Speed: 3.4ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 8.6ms\n","Speed: 3.1ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 7.7ms\n","Speed: 3.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 7.9ms\n","Speed: 3.1ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 7.7ms\n","Speed: 3.1ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 8.4ms\n","Speed: 2.3ms preprocess, 8.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 8.0ms\n","Speed: 3.0ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 8.0ms\n","Speed: 3.2ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.9ms\n","Speed: 3.1ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.9ms\n","Speed: 4.2ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 10.6ms\n","Speed: 3.8ms preprocess, 10.6ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.3ms\n","Speed: 3.1ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.9ms\n","Speed: 3.1ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.6ms\n","Speed: 3.5ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 14.3ms\n","Speed: 3.0ms preprocess, 14.3ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 7.9ms\n","Speed: 3.3ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.6ms\n","Speed: 2.7ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.6ms\n","Speed: 2.2ms preprocess, 8.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.0ms\n","Speed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 9.7ms\n","Speed: 2.9ms preprocess, 9.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.1ms\n","Speed: 2.9ms preprocess, 8.1ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.8ms\n","Speed: 2.3ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 8.2ms\n","Speed: 2.7ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.7ms\n","Speed: 3.3ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 10.1ms\n","Speed: 3.2ms preprocess, 10.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.1ms\n","Speed: 2.3ms preprocess, 8.1ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.0ms\n","Speed: 2.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.7ms\n","Speed: 3.5ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.7ms\n","Speed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.0ms\n","Speed: 2.3ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.8ms\n","Speed: 2.7ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.3ms\n","Speed: 3.6ms preprocess, 8.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.9ms\n","Speed: 3.3ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 8.9ms\n","Speed: 3.2ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.9ms\n","Speed: 3.4ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 8.1ms\n","Speed: 3.2ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 7.7ms\n","Speed: 3.4ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 8.6ms\n","Speed: 2.8ms preprocess, 8.6ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.1ms\n","Speed: 3.5ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 8.1ms\n","Speed: 2.8ms preprocess, 8.1ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.3ms\n","Speed: 3.3ms preprocess, 8.3ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 8.8ms\n","Speed: 3.7ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 7.7ms\n","Speed: 3.2ms preprocess, 7.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 9.8ms\n","Speed: 3.7ms preprocess, 9.8ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.3ms\n","Speed: 3.1ms preprocess, 8.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 9.2ms\n","Speed: 3.5ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 8.6ms\n","Speed: 2.6ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.1ms\n","Speed: 3.4ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 9.9ms\n","Speed: 4.3ms preprocess, 9.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 8.4ms\n","Speed: 5.0ms preprocess, 8.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 9.1ms\n","Speed: 3.0ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 7.9ms\n","Speed: 3.1ms preprocess, 7.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.0ms\n","Speed: 3.1ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 8.4ms\n","Speed: 3.4ms preprocess, 8.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 7.7ms\n","Speed: 3.3ms preprocess, 7.7ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 8.4ms\n","Speed: 3.3ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.8ms\n","Speed: 2.9ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.9ms\n","Speed: 2.5ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 4 personas, 7.9ms\n","Speed: 3.0ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","Procesados 100/151 frames\n","\n","0: 640x640 3 personas, 8.3ms\n","Speed: 3.6ms preprocess, 8.3ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.6ms\n","Speed: 3.2ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 8.0ms\n","Speed: 2.9ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 10.5ms\n","Speed: 3.7ms preprocess, 10.5ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 8.1ms\n","Speed: 2.6ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.9ms\n","Speed: 4.1ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.5ms\n","Speed: 2.3ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 10.7ms\n","Speed: 3.2ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.0ms\n","Speed: 3.6ms preprocess, 8.0ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 10.1ms\n","Speed: 4.3ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.0ms\n","Speed: 2.4ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 7.9ms\n","Speed: 3.4ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 10.2ms\n","Speed: 2.7ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.2ms\n","Speed: 2.9ms preprocess, 8.2ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.9ms\n","Speed: 3.1ms preprocess, 7.9ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.6ms\n","Speed: 3.1ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.5ms\n","Speed: 3.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 9.0ms\n","Speed: 3.2ms preprocess, 9.0ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 10.1ms\n","Speed: 2.2ms preprocess, 10.1ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.4ms\n","Speed: 2.7ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.4ms\n","Speed: 2.7ms preprocess, 8.4ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.2ms\n","Speed: 2.5ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.8ms\n","Speed: 2.2ms preprocess, 7.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.9ms\n","Speed: 3.3ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.1ms\n","Speed: 2.6ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 8.4ms\n","Speed: 2.5ms preprocess, 8.4ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 15.4ms\n","Speed: 2.6ms preprocess, 15.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.0ms\n","Speed: 3.5ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.6ms\n","Speed: 3.3ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.8ms\n","Speed: 3.5ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 3 personas, 7.5ms\n","Speed: 2.6ms preprocess, 7.5ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 7.7ms\n","Speed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.7ms\n","Speed: 3.1ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 9.4ms\n","Speed: 3.2ms preprocess, 9.4ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 8.6ms\n","Speed: 3.2ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 9.9ms\n","Speed: 3.7ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 7.9ms\n","Speed: 3.7ms preprocess, 7.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 7.9ms\n","Speed: 3.5ms preprocess, 7.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 9.4ms\n","Speed: 3.6ms preprocess, 9.4ms inference, 0.8ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 7.9ms\n","Speed: 2.9ms preprocess, 7.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.0ms\n","Speed: 2.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 8.5ms\n","Speed: 2.7ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 7.7ms\n","Speed: 2.8ms preprocess, 7.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 7.6ms\n","Speed: 3.2ms preprocess, 7.6ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 7.5ms\n","Speed: 3.0ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 9.4ms\n","Speed: 2.9ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 8.2ms\n","Speed: 2.8ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 2 personas, 11.4ms\n","Speed: 2.7ms preprocess, 11.4ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 1 persona, 9.1ms\n","Speed: 3.4ms preprocess, 9.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 7.9ms\n","Speed: 3.0ms preprocess, 7.9ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n","\n","0: 640x640 (no detections), 9.5ms\n","Speed: 2.9ms preprocess, 9.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n"]}]},{"cell_type":"code","source":["# Verificar que el video de salida existe y reportar frames escritos\n","if os.path.exists(OUTPUT_PATH):\n","    logging.info(f\"Video procesado guardado en: {OUTPUT_PATH}\")\n","    print(f\"Video procesado guardado en: {OUTPUT_PATH}\")\n","    logging.info(f\"Total de frames escritos: {written_frames}\")\n","    print(f\"Total de frames escritos: {written_frames}\")\n","else:\n","    logging.error(f\"No se encontró el video de salida en: {OUTPUT_PATH}. Verifica permisos y códec.\")\n","    print(f\"No se encontró el video de salida en: {OUTPUT_PATH}. Verifica permisos y códec.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0ej2ZbkDGHHQ","executionInfo":{"status":"ok","timestamp":1744168687008,"user_tz":240,"elapsed":14,"user":{"displayName":"Franz Gonzales","userId":"04298735210063603073"}},"outputId":"a11d38a6-b9d4-4967-a423-63589ee01a46"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Video procesado guardado en: /content/drive/MyDrive/Proyecto IA-3/violence_school_project/output_videos/video_prueba10.mp4\n","Total de frames escritos: 151\n"]}]}]}